[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estruturação e Análise de Dados Jurídicos",
    "section": "",
    "text": "1 Prefácio",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefácio</span>"
    ]
  },
  {
    "objectID": "index.html#motivação",
    "href": "index.html#motivação",
    "title": "Estruturação e Análise de Dados Jurídicos",
    "section": "1.1 Motivação",
    "text": "1.1 Motivação\nAcreditamos que a jurimetria é fundamental para a construção de um sistema de Justiça mais eficiente, transparente e orientado por evidências. Ela permite transformar a forma como o Direito é praticado, tornando operadores mais conscientes e capazes de lidar com um mundo em que dados e tecnologia assumem papel central nas decisões.\nA jurimetria e a inteligência artificial aplicadas ao Direito contribuem decisivamente para melhorar a atuação institucional, desde a formulação de políticas públicas até a condução de processos e a gestão estratégica de casos. Elas ajudam a compreender padrões, prever resultados, priorizar recursos e tornar o trabalho jurídico mais objetivo e fundamentado.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefácio</span>"
    ]
  },
  {
    "objectID": "index.html#a-quem-se-destina",
    "href": "index.html#a-quem-se-destina",
    "title": "Estruturação e Análise de Dados Jurídicos",
    "section": "1.2 A quem se Destina",
    "text": "1.2 A quem se Destina\nEste Curso de Extensão em Jurimetria foi desenvolvido em parceria com a Escola Superior da Magistratura Tocantinense (ESMAT), com especial foco os membros e servidores do Tribunal de Justiça de Tocantins.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefácio</span>"
    ]
  },
  {
    "objectID": "index.html#objetivo",
    "href": "index.html#objetivo",
    "title": "Estruturação e Análise de Dados Jurídicos",
    "section": "1.3 Objetivo",
    "text": "1.3 Objetivo\nO curso tem como objetivo proporcionar aos membros e servidores do TJTO a formação necessária para compreender e aplicar os fundamentos e as práticas iniciais da jurimetria e da inteligência artificial aplicada ao Direito. Busca-se capacitar os participantes a percorrer o ciclo jurimétrico completo, desde a formulação da pergunta de pesquisa até a apresentação dos resultados, por meio de atividades que envolvem desenho de pesquisa, coleta e listagem de processos, estruturação, limpeza e organização de dados, além da visualização inicial das informações e aplicação de modelos estatísticos.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefácio</span>"
    ]
  },
  {
    "objectID": "index.html#abordagem",
    "href": "index.html#abordagem",
    "title": "Estruturação e Análise de Dados Jurídicos",
    "section": "1.4 Abordagem",
    "text": "1.4 Abordagem\nO curso adotará uma abordagem essencialmente prática, baseada em casos reais e na metodologia de aprendizagem baseada em casos reais. A proposta é que os participantes desenvolvam seus próprios estudos a partir de questões-problema selecionadas, estruturando e analisando dados reais sob orientação dos docentes.\nAlém da fundamentação teórica oferecida pelo professor, as atividades serão organizadas para que os participantes aprendam por meio da elaboração e resolução de um caso concreto, percorrendo o ciclo jurimétrico completo, da formulação da pergunta de pesquisa à visualização, análise e apresentação dos resultados.\nO curso enfatiza a integração entre Direito, Estatística e Computação, com o objetivo de capacitar os participantes a aplicar métodos quantitativos na análise jurídica e a desenvolver interpretações que contribuam para decisões mais eficientes e justas.\nA etapa inicial, realizada em formato on-line, será voltada à introdução das ferramentas e à formação dos grupos de trabalho. Em seguida, as aulas presenciais se concentrarão no desenvolvimento prático dos projetos, culminando na apresentação final das análises produzidas pelos alunos.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prefácio</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Sumário do Curso ESMAT",
    "section": "",
    "text": "3.1 Motivação:\nEsta é a página de apoio do Professor Alexandre Nicolella e Jose de Jesus Filho ao curso de Curso de Extensão em Jurimetria da Escola Superior da Magistratura Tocantinense (ESMAT).\nO curso tem como propósito introduzir os participantes aos conceitos fundamentais de jurimetria e de inteligência artificial aplicada ao Direito, oferecendo uma formação integrada entre teoria e prática. A proposta parte da compreensão de como métodos quantitativos e computacionais podem apoiar a análise jurídica, aprimorando a tomada de decisão e promovendo maior eficiência e transparência no sistema de Justiça.\nA motivação central é capacitar os alunos a desenvolver uma visão empírica do Direito, a partir de atividades que envolvem o desenho de pesquisa e a definição de hipóteses, o planejamento de estratégias de coleta e listagem de processos, e a estruturação, limpeza e organização de bases de dados jurídicas.\nDurante o curso, serão exploradas ferramentas e pacotes da linguagem R voltados à manipulação e à visualização de dados jurídicos, com aplicação prática em exemplos reais do Tribunal de Justiça do Tocantins (TJTO). A combinação entre conceitos teóricos, prática orientada por problemas e uso de dados reais visa proporcionar uma formação sólida e aplicada, aproximando os participantes dos desafios contemporâneos da jurimetria e da análise de dados no Direito.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#objetivo",
    "href": "summary.html#objetivo",
    "title": "3  Sumário do Curso ESMAT",
    "section": "3.2 Objetivo:",
    "text": "3.2 Objetivo:\nO objetivo é fornecer aos participantes competências necessárias para: - Formular e estruturar pesquisas empíricas no Direito. - Extrair e organizar dados processuais de forma sistemática. - Aplicar ferramentas estatísticas e computacionais básicas para análise inicial de dados jurídicos. - Criar visualizações que traduzam informações complexas em resultados claros e objetivos.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#plano-detalhado-das-aulas",
    "href": "summary.html#plano-detalhado-das-aulas",
    "title": "3  Sumário do Curso ESMAT",
    "section": "3.3 Plano detalhado das aulas:",
    "text": "3.3 Plano detalhado das aulas:\nOs professores do curso são:\n\n\n\n\n\nProfessor\nEmail\n\n\n\n\nAlexandre Chibebe Nicolella\nnicolella@usp.br\n\n\nJosé de Jesus Filho\njose.filho@mpsp.mp.br; jjfilho@pucsp.br",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#etapa-online-fundamentos-e-conceitos-15-horas-síncronas",
    "href": "summary.html#etapa-online-fundamentos-e-conceitos-15-horas-síncronas",
    "title": "3  Sumário do Curso ESMAT",
    "section": "4.1 Etapa Online: Fundamentos e Conceitos (15 horas síncronas)",
    "text": "4.1 Etapa Online: Fundamentos e Conceitos (15 horas síncronas)\nNesta fase, os participantes terão contato com os conceitos fundamentais de jurimetria e inteligência artificial aplicada ao Direito.\nO enfoque será teórico, preparando o terreno para a etapa prática.\n\n4.1.1 Aula 01 - Introdução à Jurimetria\n\nConceito de jurimetria e a interdisciplinaridade necessária (Direito, Computação e Estatística)\n\nJurimetria e inteligência artificial aplicada ao Direito\n\nExemplos práticos de jurimetria\n\n\n\n4.1.2 Aula 02 - Desenho de Pesquisa\n\nPergunta de pesquisa\n\nFormulação da hipótese\n\nOperacionalização de conceitos\n\nInferência causal\n\n\n\n4.1.3 Aula 03 - Análise de Viabilidade e Listagem de Processos\n\nAnálise de viabilidade\n\nEscopo de pesquisa: pesquisa prospectiva vs. retrospectiva (resultado, tempo e distribuição)\n\nSistemas de listagem (CJ e CPO)\n\nIntrodução ao pacote tjsp\n\n\n\n4.1.4 Aula 04 - Estruturação de Dados I\n\nIntrodução ao dplyr\n\nTratamento de datas com lubridate\n\nTratamento de texto com stringr\n\nIteração com purrr\n\n\n\n4.1.5 Aula 05 - Estruturação de Dados II\n\nIntrodução aos Large Language Models (LLMs)\n\nEngenharia de prompt\n\n\n\n4.1.6 Aula 06 - Visualização de Dados\n\nNoções básicas de estatística\n\nIntrodução ao ggplot2",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#etapa-presencial-aplicação-prática-no-tocantins-15-horas",
    "href": "summary.html#etapa-presencial-aplicação-prática-no-tocantins-15-horas",
    "title": "3  Sumário do Curso ESMAT",
    "section": "4.2 Etapa Presencial: Aplicação Prática no Tocantins (15 horas)",
    "text": "4.2 Etapa Presencial: Aplicação Prática no Tocantins (15 horas)\nRealizada no Tocantins, esta etapa será totalmente prática e orientada a resultados.\nOs participantes serão organizados em grupos de trabalho por tema, e cada grupo desenvolverá um projeto jurimétrico completo, percorrendo as seguintes etapas:\n\nDefinição de uma pergunta de pesquisa aplicada ao contexto do TJTO.\n\nColeta de dados judiciais relevantes.\n\nEstruturação, organização e limpeza das bases.\n\nAnálise exploratória e visualização de resultados.\n\nApresentação final dos achados.\n\nAo final do módulo, os participantes terão vivenciado o ciclo completo da jurimetria, desde a formulação inicial até a comunicação dos resultados, combinando competências jurídicas, estatísticas e computacionais.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#método-de-trabalho",
    "href": "summary.html#método-de-trabalho",
    "title": "3  Sumário do Curso ESMAT",
    "section": "4.3 Método de Trabalho",
    "text": "4.3 Método de Trabalho\n\n4.3.1 Divisão do Curso em Módulos e Formatos\nOnline (15h, síncrono): - Desenvolvimento dos conceitos centrais de forma teórica\n- Discussão de fundamentos, referências e boas práticas\n- Interação por meio de exercícios guiados e debates curtos\nPresencial (15h, em Tocantins): - Aplicação prática do conhecimento adquirido\n- Formação de grupos de trabalho por tema/problema\n- Cada grupo seguirá o ciclo da pergunta à análise dos resultados\n\n\n4.3.2 Ciclo de Trabalho em Grupo\n\nDefinição do problema ou pergunta de pesquisa: cada grupo escolhe um tema específico a partir dos conceitos apresentados.\n\nColeta e organização de dados/informações: orientação sobre como levantar dados relevantes para a análise.\n\nAnálise e interpretação dos resultados: uso das ferramentas e métodos aprendidos na parte online.\n\nApresentação e discussão dos resultados: compartilhamento das conclusões e lições aprendidas.\n\n\n\n4.3.3 Integração Teoria e Prática\nO método garante que os conceitos vistos online sejam aplicados de forma concreta.\nA interação presencial reforça o aprendizado ativo e colaborativo.\n\n\n4.3.4 Feedback e Aprimoramento\n\nAvaliação contínua do progresso dos grupos.\n\nDiscussões coletivas para melhorar abordagens e resultados.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#infraestrutura-e-dados",
    "href": "summary.html#infraestrutura-e-dados",
    "title": "3  Sumário do Curso ESMAT",
    "section": "4.4 Infraestrutura e Dados",
    "text": "4.4 Infraestrutura e Dados\n\n4.4.1 Dados\nOs dados utilizados no curso serão fornecidos pelo TJTO, diretamente ou via uso de API para acesso à informação.\n\n\n4.4.2 Computadores e Rede\n\nCada participante terá acesso a um computador individual.\n\nTodos os computadores estarão conectados à internet de alta velocidade.\n\n\n\n4.4.3 Softwares e Ferramentas\n\nR (versão mais recente) pré-instalado em todas as máquinas.\n\nInstalação de packages adicionais conforme necessidade dos exercícios.\n\nOutros softwares de apoio poderão ser instalados mediante solicitação.\n\n\n\n4.4.4 Ambiente de Trabalho\n\nEspaço preparado para trabalho em grupo e colaboração.\n\nProjetor e quadro branco disponíveis.\n\nSuporte técnico para instalação de pacotes e resolução de problemas.\n\n\n\n4.4.5 Segurança e Backup\n\nSistemas de backup configurados para evitar perda de dados.\n\nGarantia de que os participantes possam salvar e recuperar seus projetos com segurança.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#resultados-esperados",
    "href": "summary.html#resultados-esperados",
    "title": "3  Sumário do Curso ESMAT",
    "section": "4.5 Resultados Esperados",
    "text": "4.5 Resultados Esperados\n\n4.5.1 Aprendizado Conceitual\n\nCompreensão dos fundamentos, metodologias e boas práticas do tema.\n\nCapacidade de aplicar conceitos teóricos em situações reais.\n\n\n\n4.5.2 Habilidades Práticas\n\nDesenvolvimento da análise de dados e interpretação de resultados utilizando R.\n\nExperiência em trabalho colaborativo, da definição do problema à apresentação de soluções.\n\n\n\n4.5.3 Produção de Resultados Concretos\n\nCada grupo produzirá relatórios ou apresentações com análise detalhada de seu tema.\n\nDiscussão coletiva dos resultados, promovendo aprendizado entre todos.\n\n\n\n4.5.4 Fortalecimento da Capacidade Analítica\n\nHabilidade de formular perguntas relevantes, organizar dados, aplicar métodos e tirar conclusões consistentes.\n\nPreparação para aplicar o conhecimento em projetos profissionais ou acadêmicos.\n\n\n\n4.5.5 Feedback e Melhoria Contínua\n\nFeedback detalhado sobre os trabalhos dos grupos.\n\nIncentivo à reflexão sobre processos e resultados, promovendo aprimoramento contínuo.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "summary.html#bibliografia",
    "href": "summary.html#bibliografia",
    "title": "3  Sumário do Curso ESMAT",
    "section": "4.6 Bibliografia",
    "text": "4.6 Bibliografia\nAGRESTI, Alan; FINLAY, Barbara. Métodos Estatísticos para as Ciências Sociais. São Paulo: Tenso, 2017.\nAMINIKHANGHAHI, Samaneh; COOK, Daniel J. A Survey of Methods for Time Series Change Point Detection. Knowl Inf Syst, v. 51, p. 339–367, 2017. DOI: 10.1007/s10115-016-0987-z.\nENDERS, Craig. Applied Missing Data Analysis. New York: The Guilford Press, 2022.\nEPSTEIN, Lee; KING, Gary. As Regras de Inferência. São Paulo: Direito GV, 2013.\nGALANTER, Marc. Why the ‘Haves’ Come Out Ahead: Speculations on the Limit of Social Change. Law and Society Review, v. 9:1, 1974.\nHILBE, Joseph. Practical Guide for Logistic Regression. Chapman and Hall/CRC, 2018.\nKELLSTEDT, Paul M.; WHITTEN, Guy D. Fundamentos da Pesquisa em Ciência Política. São Paulo: Blucher, 2014.\nKING, Gary; KEOHANE, Roberto; VERBA, Sidney. Designing Social Inquiry: Scientific Inference for Qualitative Methods. 2021.\nLEE, Alex Yoon-Ho; KLERMAN, Daniel M. The Priest-Klein Hypotheses: Proofs and Generality. International Review of Law and Economics, v. 59, 2016.\nMOORE, Dirk F. Applied Survival Analysis Using R. Switzerland: Springer, 2016.\nMORETTIN, Wilton de O.; BUSSAB, Pedro A. Estatística Básica. São Paulo: Saraiva, 2021.\nPRIEST, George L.; KLEIN, Benjamin. The Selection of Disputes for Litigation. Journal of Legal Studies, v. XIII, jan. 1984.\nSILVA, Glauco. Desenho de Pesquisa. São Paulo: ENAP, 2018.\nSTOCK, James H.; WATSON, Mark W. Introduction to Econometrics. London: Pearson, 2020.\nWOOLDRIDGE, Jeffrey M. Introductory Econometrics: A Modern Approach. South Western Educational Publishing, 2012.\nKILLICK, Rebecca; ECKLEY, Idris A. changepoint: An R Package for Changepoint Analysis. Journal of Statistical Software, 58(3), 1–19, 2014. DOI: 10.18637/jss.v058.i03.\nTOOMET, Ott; HENNINGSEN, Arne. Sample Selection Models in R: Package sampleSelection. Journal of Statistical Software, 27(7), 1–23, 2008. DOI: 10.18637/jss.v027.i07.\nTRUONGA, Charles; OUDRE, Laurent; VAYATIS, Nicolas. Selective Review of Offline Change Point Detection Methods. Signal Processing, 167, 2020.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sumário do Curso ESMAT</span>"
    ]
  },
  {
    "objectID": "manipulacao.html",
    "href": "manipulacao.html",
    "title": "4  Maipulando os Dados",
    "section": "",
    "text": "4.1 O R-project e as Boas Práticas",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maipulando os Dados</span>"
    ]
  },
  {
    "objectID": "manipulacao.html#o-r-project-e-as-boas-práticas",
    "href": "manipulacao.html#o-r-project-e-as-boas-práticas",
    "title": "4  Maipulando os Dados",
    "section": "",
    "text": "4.1.1 O software R\nO R é uma linguagem e ambiente de desenvolvimento de Estatística e gráficos. É uma ferramenta poderosa, fornecendo ao seu usuário maior integração e qualidade gráfica e de análise. Alguns motivos para utilizar o R:\n\nÉ Gratuito: é um projeto open-source. Pode ser utilizado em qualquer sistema operacional e tem aberto seus códigos e pactos para poder ser inspecionado.\nR é uma Linguagem:Requer que seja escrito um script ao invés de clicar. A primeira vista uma característica negativa, entretanto, permite maior exploração, organização, memória da atividade, maior integração entre processos etc.\nGráficos e Visualizações: É sem sombra de dúvida o pacote estatístico com melhor e mais poderosa ferramenta de elaboração de gráficos e visualização.\nPacotes Estatísticos:Já possui muitas rotinas de análises já programadas nos diversos pacotes desenvolvidos, sendo muito bem documentados. Já possui muitas rotinas para regressão, regressão com séries temporais, regressão em painel, finanças, modelos de causalidade etc.\nFronteira do Conhecimento: Os principais desenvolvimentos teóricos em Econometria tem sua aplicação demonstrada e desenvolvida utilizando o R. Isso é valido para todas as subáreas do conhecimento em econometria, séries temporais, painel, finanças, etc.\nRecursos de Ajuda: Há uma comunidade muito grande disponível para solucionar dúvidas e uma vasta documentação disponível para consulta na rede.\nConexão com Outros Pacotes: O R integra com outros pacotes que automatizam o nosso trabalho cotidiano. Pode se conectar com o Python, Java, SQL, Latex etc.\n\n\n\n4.1.2 Utilizando Interface Gráfica - O Rstudio\nPode-se realizar seu script diretamente no console do R. Ele irá realizar um comando por vez. O R é uma interface leve e com poucos recursos gráficos.\nUma alternativa ao uso do R diretamente é o Rstudio, o qual é um editor de código ou um ambiente de desenvolvimento integrado. Ele possui quatro janelas, sendo a primeira a janela de script (superior esquerda) onde escrevemos os comandos em R.\nA segunda janela é o console (inferior esquerda) similar ao que temos no R e onde os resultados são apresentados. Pode-se digitar comando diretamente no console do RStudio.\nA terceira é a janela de ambiente e história (superior direita) ela armazena seus dados, valores e funções e a aba história possui a memoria dos comandos realizados.\nPor fim a quarta janela (inferior direita) apresenta os pacotes, os gráficos, os arquivos gerados e ajuda. Essa janela facilita a instalação de pacotes, carregamento de bibliotecas, visualização de gráficos e o caminho dos arquivos.\n\n\n4.1.3 Ajuda\nPara abrir a ajuda geral o seguinte abaixo pode ser utilizado e abrirá uma janela no seu navegador.\n\nhelp.start()\n\nSuponha que queiramos saber de uma função específica, assim pode-se utilizar o seguinte comando:\n\nhelp(summary)\n\nou\n\n?summary\n\nInclusive pode pedir um exemplo de como utilizar a função que está buscando\n\nexample(summary)\n\n\n\n4.1.4 Boas Práticas\nÉ fundamental que o usuário seja organizado. Forma é muito importante! Assim o usuário deve adotar padrões que auxiliem na organização do seu script ou programa.\nCase Sensitivy: O R diferencia letras minúsculas e maiúscula. Ou seja, m é diferente de M. Por exemplo, considere as três formas de escrever a palavra idade.\n\nidade ou Idade ou IDADE\n\nCada uma delas representa variáveis diferentes.\n\n\n\n\n\n\nDICA\n\n\n\nSempre utilize as suas variáveis em minúsculo. Adote isso como regra geral.\n\n\nCriando Bons Nomes: Vamos supor que queiramos criar uma variável que indique a idade que se formou na Universidade. Temos algumas opções:\n\nid: Ruim, pois não tem significado claro e pode confundir com a variável de identificação\nidade_formou_na_universidade: Ruim, pois o nome da variável é muito grande, difícil de escrever e de visualizar no banco de dados.\nidade_form: Bom nome, significativo, minúsculo e pequeno separa os dois nomes por underline\nidadeForm :Bom nome, significativo, minúsculo e pequeno separa os dois nomes por uma letra maiúscula.\n\n\n\n\n\n\n\nDICA\n\n\n\nAdote uma regra de criação para você e evite mudar. Crie nomes pequenos e significativos. Nunca inicie uma variável com número.\n\n\n\n\n4.1.5 Criando projeto no R\nPara saber em qual diretório o R está utilizando para salvar seu espaço de trabalho utilize o seguinte comando:\n\ngetwd() \n\nNo RStudio, sempre prefira a criação de um projeto para a organização de seus dados, com isso, ao mudar de máquina (ou estrutura de diretórios) seu código continuará funcionando normalmente.\n\n  File -&gt; New Project\n\n\n\n4.1.6 Identação é Importante\nIdentar é o recuo no texto em relação a margem. É importante que esse recuo exista para linhas do seu programa que são hierarquicamente conectadas. Vejamos dois exemplos com e sem identação:\nSem Identação\n\nx=c()\nx[1] = 3\nfor (i in 2:9) { \nx[i]=2*x[i-1]\n}\n\nNote que a quarta linha desse programa está hierarquicamente conectada a linha 3 do “for”, ou seja, é uma continuação do comando e portanto deve ser identado para demonstrar essa relação de dependência. Vejamos\nCom Identação\n\nx=c()\nx[1] = 3\nfor (i in 2:9) { \n  x[i]=2*x[i-1]\n}",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maipulando os Dados</span>"
    ]
  },
  {
    "objectID": "manipulacao.html#inserindo-dados-no-r",
    "href": "manipulacao.html#inserindo-dados-no-r",
    "title": "4  Maipulando os Dados",
    "section": "4.2 Inserindo Dados no R",
    "text": "4.2 Inserindo Dados no R\n\n4.2.1 Tipos de Variáveis\nO R possui diversos tipos de variáveis. Alguns desses tipos são:\nVetores: Vamos inserir os dados denúmero de homicídios de mulheres nos diversos Estados brasileiros para o ano de 2022.No primeiro elemento teremos um erro, ao invés de 22 colocaremos 2.E não colocamos o valor do Distrito Federal e nem Tocantins\n\nhomic &lt;- c(2,  73,  22,  88,  406,  264,    95,  137,  127,  101,  75,  309,  200,  86,  256,  219,  70,  283,  60,  281,  88,  33,  101,  423,  37)\nhomic\n\n [1]   2  73  22  88 406 264  95 137 127 101  75 309 200  86 256 219  70 283  60\n[20] 281  88  33 101 423  37\n\n\nPodemos inserir vetores de texto, por exemplo, iremos inserir os estados brasileiros na mesma ordem do homicídio acima.\n\n\n [1] \"Acre\"                \"Alagoas\"             \"Amapá\"              \n [4] \"Amazonas\"            \"Bahia\"               \"Ceará\"              \n [7] \"Distrito Federal \"   \"Espírito Santo\"      \"Goiás\"              \n[10] \"Maranhão\"            \"Mato Grosso\"         \"Mato Grosso do Sul\" \n[13] \"Minas Gerais\"        \"Pará\"                \"Paraíba\"            \n[16] \"Paraná\"              \"Pernambuco\"          \"Piauí\"              \n[19] \"Rio de Janeiro\"      \"Rio Grande do Norte\" \"Rio Grande do Sul\"  \n[22] \"Rondônia\"            \"Roraima\"             \"Santa Catarina\"     \n[25] \"São Paulo\"           \"Sergipe\"             \"Tocantins\"          \n\n\nAlgumas manipulações importantes que podemos fazer com os vetores. Renomeando e removendo o vetor antigo:\nTrocando o primeiro elemento do vetor e dando o print do novo resultado:\n\nhomic_abs[1]=22\nhomic_abs\n\n [1]  22  73  22  88 406 264  95 137 127 101  75 309 200  86 256 219  70 283  60\n[20] 281  88  33 101 423  37\n\n\nAlgumas maneiras de pedir o print do vetor de homicídios femininos. Somente o estado 7, todos menos o estado 7, Estado de 1 até 7 etc:\n\nhomic_abs[7] \n\n[1] 95\n\nhomic_abs[-7] \n\n [1]  22  73  22  88 406 264 137 127 101  75 309 200  86 256 219  70 283  60 281\n[20]  88  33 101 423  37\n\nhomic_abs[1:7]\n\n[1]  22  73  22  88 406 264  95\n\n\nPodemos incorporar novos dados no nosso vetor de homicídio feminino, Vamos incorporar o dado do Tocantins na posição 7 e o valor do Distrito Federal na útima posição - 27. Depois trocaremos os dois estados de posição:\n\n#colcar exemplo de inserir no inicio\n\n#inserir no meio e no final \nhomic_abs &lt;- c(homic_abs[1:6], 36,homic_abs[7:25], 32)\nhomic_abs\n\n [1]  22  73  22  88 406 264  36  95 137 127 101  75 309 200  86 256 219  70 283\n[20]  60 281  88  33 101 423  37  32\n\n#troca de posicoes\ntemp &lt;- homic_abs[27]\nhomic_abs[27] &lt;- homic_abs[7]\nhomic_abs[7] &lt;- temp\nhomic_abs\n\n [1]  22  73  22  88 406 264  32  95 137 127 101  75 309 200  86 256 219  70 283\n[20]  60 281  88  33 101 423  37  36\n\n\nString ou Texto:\nString são as variáveis tipo texto. Esse tipo de variável já apareceu na seção anterior quando apresentamos um vetor com a classificação dos Estados. Vejamos mais uma vez. Podemos criar uma variável que contêm “estado homicidio”. Uma segunda maneira é criar um vetor com dois elementos “estado” e “homicidio”. O comando paste cola a variável texto “estado” e a variável texto “homicidio”, separado por um espaço.\n\na &lt;- \"estado homicidio\"\na\n\n[1] \"estado homicidio\"\n\nb &lt;- c(\"estado\",\"homicidio\")\nb\n\n[1] \"estado\"    \"homicidio\"\n\nb[1]\n\n[1] \"estado\"\n\npaste(b[1],b[2],sep=' ')\n\n[1] \"estado homicidio\"\n\n\nFator:\nFator são variáveis de classe. Fator armazenam os valores inteiros na forma de um vetor com as quantidades das k classes e o vetor string dos valores originais. Vejamos o exemplo de um vetor. Podemos análisar os homicídios por região geográfica do país. Assim, classificaremos os estados por região:\n\nregiao &lt;- c(\"N\",  \"NE\",  \"N\",  \"N\",  \"NE\",  \"NE\",  \"CO\",  \"SD\",  \"CO\",  \"NE\",  \"CO\",  \"CO\",  \"SD\",  \"N\",  \"NE\",  \"S\",  \"NE\",  \"NE\",  \"SD\",  \"NE\",  \"S\",  \"N\",  \"N\",  \"S\",  \"SD\",  \"NE\",  \"N\")\nsummary(regiao)\n\nAgora vamos transformar o vetor anterior em um fator\n\nregiao &lt;- factor(regiao)\nsummary(regiao)\n\nCO  N NE  S SD \n 4  7  9  3  4 \n\nlevels(regiao)\n\n[1] \"CO\" \"N\"  \"NE\" \"S\"  \"SD\"\n\n\nO comando levels fornece as classes existentes, no caso acima temos 5, sendo elas 4, 7, 9, 3 e 4.\nFatores podem ser as características de raça, gênero, status familiar, status de saúde, qualidade do atendimento etc.\n\n4.2.1.1 Data Frame ou Banco de Dados\nEsse é um tipo mais geral de variável e consegue lidar na mesma estrutura com variaveis de tipos distintos como numérica, texto e fator. Um banco de dados similar aos outros programas estatísticos. Podemos criar essa variável de forma manual. Nosso banco de dados será composto por 4 variáveis, a primeira o estado, a segunda a região, a terceira o número de homicídios femininos e a quarta o número de feminicidios. As três primeiras já foram incluidas acima e vamos criar somente a quarta. O comando typeof mostra qual o tipo de variável.\n\nfeminic_abs=c(11,  31,  8,  21,  107,  28,  19,  33,  56,  69,  47,  40,  171,  49,  26,  77,  72,  24,  111,  16,  110,  24,  3,  56,  195,  19,  14) \ntypeof(feminic_abs)\n\n[1] \"double\"\n\n\nPara criar o banco de dados utilizamos o seguinte comando:\n\ndata_feminic22&lt;-data.frame(estados, regiao, homic_abs, feminic_abs)  \n\nPodemos modificar o nome das variáveis com o comando names. Entretanto, tem que renomear todas\n\nnames(data_feminic22)&lt;-c(\"estado\", \"regioa\", \"homic_abs\", \"feminic_abs\") \ndata_feminic22\n\nOu podemos renomear somente algumas com o comando reshape:\n\nlibrary(reshape)\ndata_feminic22 &lt;- rename(data_feminic22, c(estado=\"estados\", regioa=\"regiao\"))\ndata_feminic22\n\n               estados regiao homic_abs feminic_abs\n1                 Acre      N        22          11\n2              Alagoas     NE        73          31\n3                Amapá      N        22           8\n4             Amazonas      N        88          21\n5                Bahia     NE       406         107\n6                Ceará     NE       264          28\n7    Distrito Federal      CO        32          19\n8       Espírito Santo     SD        95          33\n9                Goiás     CO       137          56\n10            Maranhão     NE       127          69\n11         Mato Grosso     CO       101          47\n12  Mato Grosso do Sul     CO        75          40\n13        Minas Gerais     SD       309         171\n14                Pará      N       200          49\n15             Paraíba     NE        86          26\n16              Paraná      S       256          77\n17          Pernambuco     NE       219          72\n18               Piauí     NE        70          24\n19      Rio de Janeiro     SD       283         111\n20 Rio Grande do Norte     NE        60          16\n21   Rio Grande do Sul      S       281         110\n22            Rondônia      N        88          24\n23             Roraima      N        33           3\n24      Santa Catarina      S       101          56\n25           São Paulo     SD       423         195\n26             Sergipe     NE        37          19\n27           Tocantins      N        36          14\n\n\nPodemos também listar variáveis do banco de dados, por exemplo, listar colunas de 1 a 2 ou listar por nome das variáveis, conforme apresentado abaixo:\n\ndata_feminic22\ndata_feminic22[,2:3]\ndata_feminic22[1:2,2:3]\ndata_feminic22[c(\"regiao\",\"feminic_abs\")]\n\nEntretanto, inserir dados na mão pode ser uma tarefa muito penosa e existem soluções bem mais simples e rápidas para inserção de dados. Nas seções seguintes veremos aprenderemos mais funções úteis para lidar com banco de dados.\n\n\n4.2.1.2 Trabalhando com as variáveis:\nVamos retomar duas variáveis homic_abs e estado e vamos manipular essas duas variáveis. Primeiramente vejamos o número de elementos, estrutura, classe e nome:\n\nlength(homic_abs) \n\n[1] 27\n\nstr(homic_abs)    \n\n num [1:27] 22 73 22 88 406 264 32 95 137 127 ...\n\nclass(homic_abs)  \n\n[1] \"numeric\"\n\nnames(homic_abs) \n\nNULL\n\n\nPodemos combinar as duas variáveis de forma distintas, por exemplo combinar na forma de um vetor, combinar como coluna ou combinar como linha, vejamos a diferença:\n\n#Precisa mudar essa parte de posição está confuso pois falamos de dataframe e aqui de vetor\ncomb1 &lt;- c(homic_abs,estados)      \ncomb2&lt;- cbind(homic_abs,estados)\ncomb3 &lt;-rbind(homic_abs,estados)\ncomb4 &lt;- data.frame(\n              homic_abs,\n              estados\n              ,stringsAsFactors = F)\ncomb1\ncomb2 \ncomb3\ncomb4\n\nVejamos quais objetos temos e vamos pedir para visualizar os objetos que acabamos de criar. Por fim removeremos o vetor comb1.\n\nls()  \ncomb1\ncomb2\ncomb3\nrm(comb1)              \n\n\n\n\n4.2.2 Importando os Dados\nDisponibilizamos dois banco de dados, um contendo os homicídios e feminicídios por estado e outro com as tentativas. Esses arquivos estão em formato csv (comma separated values).\nPara leitura desse arquivo em csv o seguinte comando é necessário read.csv, indicado que possui cabeçalho e que o separador é “;”\n\ndf_feminic22&lt;-read.csv(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/dados_feminic.csv\", head=TRUE,sep=\";\")\n\ndf_t_feminic22&lt;-read.csv(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/dados_tent_feminic.csv\", head=TRUE,sep=\";\")\n\nPara leitura de arquivos em Stata terá que utilizar o pacote foreign, conforme exemplo abaixo:\n\nlibrary(foreign)\nstata_feminic &lt;- read.dta(\"~/feminic.dta\")\n\nAlém desses, o R é capaz de trabalhar com SQL, SAS, SPSS, Excel entre outros.\n\n\n\n\n\n\nCuidado com o Ponto\n\n\n\nO R usa o formato americano de separação numérica. Usa ponto ao invés da vírgula para separar a unidade dos decimais. No Brasil usamos a vírgula. Isso sempre gera conflito. No seu csv evite usar acentos nas palavras e use ponto como separados dos decimais e não use separador dos milhares. Exemplo: 12500.97\n\n\n\n\n4.2.3 Exportanto os Dados\nPodemos exportar os dados em diferentes formatos. Alguns exemplos são csv, texto delimitado, excel, stata. Vejamos em csv:\n\nwrite.table(df_feminic22, \"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/export_feminic.csv\", sep=\";\")\n\nPara exportar em Stata utilize os seguintes comandos:\n\nlibrary(foreign)\nwrite.dta(df_feminic22, paste(getwd(),\"~/Banco de dados/export_feminic.dta\",sep=''))\n\n\n\n4.2.4 Lidando com Dados Missing\nNão temos informação para as tentativas de feminicidio para os estados de São Paulo e Mato Grosso. Uma maneira de lidar com valores missing seria fazer um subconjunto que veremos mais a frente. Agora seguiremos alguns passos para analisar os valores missing do nosso banco de dados.\nPrimeiramente, analisamos se há valores missing no banco de dados:\n\nis.na(df_t_feminic22)\n\nPodemos desconsiderar os valores missing da análise de interesse, vamos fazer a média do dolar sem considerar os valores missing:\n\nmean(df_t_feminic22$t_feminic_abs) \n\n[1] NA\n\nmean(df_t_feminic22$t_feminic_abs, na.rm=TRUE)\n\n[1] 102.52\n\n\nPodemos criar um novo banco de dados sem os valores missing.\n\ndf_t_feminic22_sem_missing &lt;- na.omit(df_t_feminic22)\nmean(df_t_feminic22_sem_missing$t_feminic_abs)\n\n[1] 102.52\n\nrm(df_t_feminic22_sem_missing)\n\nOutra maneira de excluir os valores missing seria a utilização do comando subset removendo as observações que contenham valor missing. Isso será explicado em seção a frente.\nPode-se também recodificar uma determinada variável para missing. Muito comum nas pesquisas do IBGE os valores missing serem identificados por um número, por exemplo 999999999999. Dessa forma podemos indicar que esse não é número e sim um valor missing da seguinte maneira:\n\ndf_t_feminic22$t_feminic_abs[df_t_feminic22$t_feminic_abs==999999] &lt;- NA\n\nTodos os valores que forem 99 serão exluídos e a celula ficará com um NA\n\n\n\n\n\n\nDados Missing\n\n\n\nDoois pontos importantes, dados missing não é 0 e nunca devem ser substituídos por 0. Pois 0 é um valor e missing é que não sabemos. Outro ponto é que devemos evitar excluir do banco os dados missing, melhor é fazer as contas retirando apensa do cálculo",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maipulando os Dados</span>"
    ]
  },
  {
    "objectID": "manipulacao.html#operando-o-banco-de-dados",
    "href": "manipulacao.html#operando-o-banco-de-dados",
    "title": "4  Maipulando os Dados",
    "section": "4.3 Operando o Banco de Dados",
    "text": "4.3 Operando o Banco de Dados\n\n4.3.1 Criando uma Nova Variável\nVamos criar uma variável que seria a soma dos homicídios e feminicídios no estado. Para criar a variável precisamos dizer primeiro qual o banco de dados em que queremos criar e qual o nome da variável, conforme apresentado na expressão abaixo.\n\nlibrary(reshape)\ndf_feminic22 &lt;- rename(df_feminic22, c(feminico_abs=\"feminic_abs\"))\n\n\ndf_t_feminic22$t_total_abs&lt;- df_t_feminic22$t_feminic_abs + df_t_feminic22$t_homic_abs\n\ndf_feminic22$total_abs&lt;- df_feminic22$feminic_abs + df_feminic22$homic_abs\n\nAgora vamos criar uma variável binária que representa como 1 os estados que possuem a taxa de feminicídio em relação ao total de homicídios maior que 50%. Novamente, precisamos indicar o banco de dados e o nome da variável no banco de dados.\n\ndf_feminic22$mais_50[df_feminic22$part_feminic &lt; 50] &lt;- 0\ndf_feminic22$mais_50[df_feminic22$part_feminic &gt;= 50] &lt;- 1\n\ndf_feminic22$mais_50\n\n [1] 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n\n\n\n\n4.3.2 Operadores Aritméticos e Lógicos\nUtilizando os vetores criados anteriormente:\n\nfeminic_abs&gt;50\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n[13]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n[25]  TRUE FALSE FALSE\n\nfeminic_abs[feminic_abs&gt;50]\n\n [1] 107  56  69 171  77  72 111 110  56 195\n\nfeminic_abs[feminic_abs &lt; 50 | feminic_abs &gt; 100]\n\n [1]  11  31   8  21 107  28  19  33  47  40 171  49  26  24 111  16 110  24   3\n[20] 195  19  14\n\nfeminic_abs[feminic_abs &gt; 50 & feminic_abs &lt; 100]\n\n[1] 56 69 77 72 56\n\nmean(feminic_abs)\n\n[1] 53.22222\n\nfeminic_M &lt;- (feminic_abs[feminic_abs&lt; 100 & feminic_abs &gt; 50])\nfeminic_M\n\n[1] 56 69 77 72 56\n\n\nA tabela abaixo contém alguns operadaroes lógicos frequentemente utilizados.\n\nOperadores Lógicos no R\n\n\nOperador\nSignificado\n\n\n\n\n&lt;\nMenor que\n\n\n&lt;=\nMenor igual\n\n\n&gt;\nMaior que\n\n\n&gt;=\nMaior igual\n\n\n==\nExatamente igual\n\n\n!=\nDiferente\n\n\n!x\nNão x\n\n\nx | y\nx OU y\n\n\nx & y\nx E y",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maipulando os Dados</span>"
    ]
  },
  {
    "objectID": "manipulacao.html#algumas-funções-importantes",
    "href": "manipulacao.html#algumas-funções-importantes",
    "title": "4  Maipulando os Dados",
    "section": "4.4 Algumas Funções Importantes",
    "text": "4.4 Algumas Funções Importantes\n\n4.4.1 Ordenando os Dados\nVamos ordenar os estados dos nossos 2 data frames pela participação do feminicídio no total de homicídios de mulheres.\n\ndf_feminic22_ord&lt;-df_feminic22[order(df_feminic22$part_feminic, decreasing = TRUE),] \n\ndf_t_feminic22_ord&lt;-df_t_feminic22[order(df_t_feminic22$part_t_feminic, decreasing = TRUE),] \n\n\n\n4.4.2 Fazendo Merge\nTemos dois data frames um contendo homicídios e feminicídios e outro contendo as tentativas. Vamos agora juntar os dois. Para isso os bancos devem conter uma chave única que identifica cada linha e essa deve estar presente nos dois bancos. Inclusive devem ter o mesmo nome.No caso em questão podemos usar o estado como nossa chave que irá conectar os dois bancos.\n\ndf_final_feminic_22 &lt;- merge(df_feminic22,df_t_feminic22,by=\"sigla\",keep.all=TRUE)\n\nAgora temos um banco único com homicídios e tentativas de homicídios.\n\n\n4.4.3 Agregando\nVamos criar um banco de dados que contenha os valores médios das variáveis. Para isso vamos agregar fazendo a média por região. Poderíamos utilizar outra função, como a soma, para fazer a agregação:\n\nregiao_media &lt;-aggregate(df_final_feminic_22, by=list(df_final_feminic_22$regiao.x), FUN=mean, na.rm=TRUE)\nregiao_media\n\n  Group.1 sigla estado.x regiao.x homic_abs homic_tx feminic_abs feminic_tx\n1      CO    NA       NA       NA  86.25000 4.250000    40.50000   2.100000\n2       N    NA       NA       NA  69.85714 6.785714    18.57143   1.871429\n3      NE    NA       NA       NA 149.11111 4.355556    43.55556   1.422222\n4       S    NA       NA       NA 212.66667 4.000000    81.00000   1.600000\n5      SD    NA       NA       NA 277.50000 3.300000   127.50000   1.375000\n  part_feminic  rendapc total_abs   mais_50 estado.y regiao.y t_homic_abs\n1     50.02500 2011.250 126.75000 0.5000000       NA       NA    258.2500\n2     30.01429 1175.286  88.42857 0.1428571       NA       NA    161.7143\n3     34.36667 1053.222 192.66667 0.2222222       NA       NA    257.7778\n4     41.53333 1983.667 293.66667 0.3333333       NA       NA    450.6667\n5     43.82500 1842.750 405.00000 0.2500000       NA       NA    455.7500\n  t_homic_tx t_feminic_abs t_feminic_tx part_t_feminic t_total_abs\n1  13.375000     126.33333     6.500000       32.67667    387.6667\n2  24.657143      50.28571     5.285714       26.44286    212.0000\n3   9.311111      85.11111     3.044444       25.72444    342.8889\n4   8.966667     169.66667     3.500000       25.98667    620.3333\n5   8.875000     185.66667     3.000000       26.50000    660.3333\n\n\n\n\n4.4.4 Criando Subconjunto\nSelecionando Variáveis:\nPodemos estar interessado em manter somente algumas variáveis no nosso banco de dados, por exemplo, queremos manter estado e a participação do feminicídio no total. Assim:\n\nvar_sel &lt;- c(\"sigla\", \"part_feminic\", \"part_t_feminic\")\nfeminic_sel &lt;- df_final_feminic_22[var_sel]\n\nfeminic_sel1 &lt;- df_final_feminic_22[c(\"sigla\", \"part_feminic\", \"part_t_feminic\")]\n\nOu podemos fazer indicar as colunas que queremos selecionar. Na segunda opção selecionamos até o final do nosso banco\n\nfeminic_sel2 &lt;- df_final_feminic_22[c(1:3,5:6,12:15)]\nfeminic_sel3 &lt;- df_final_feminic_22[c(1:3,10:ncol(df_final_feminic_22))]\n\n:\nAo fazer o merge as variáveis que tinham o mesmo nome nos dois bancos, como estado, foram mantidas e agora possuem os nomes estado.x e estado.y, vamos manter apenas uma e trocar o seu nome. O comando namesajuda a saber a posição da variável no banco\n\nnames(df_final_feminic_22)\n\n [1] \"sigla\"          \"estado.x\"       \"regiao.x\"       \"homic_abs\"     \n [5] \"homic_tx\"       \"feminic_abs\"    \"feminic_tx\"     \"part_feminic\"  \n [9] \"rendapc\"        \"total_abs\"      \"mais_50\"        \"estado.y\"      \n[13] \"regiao.y\"       \"t_homic_abs\"    \"t_homic_tx\"     \"t_feminic_abs\" \n[17] \"t_feminic_tx\"   \"part_t_feminic\" \"t_total_abs\"   \n\nfinal_fem_22 &lt;- df_final_feminic_22[c(-12)]\n\nlibrary(reshape)\nfinal_fem_22 &lt;- rename(final_fem_22, c(estado.x=\"estados\"))\n\n\n\n\n\n\n\nCuidado ao tirar colunas\n\n\n\nSe fizermos o comando de tirar colunas no mesmo data frame, ao rodar novamente ele continuará sempre tirando a coluna indicada. O ideal seria construir outro data frame como fizemos acima.\n\n\nOu podemos especificar a o nome da coluna que será retirada. Vamos agora tirar a região que ficou duplicada\n\nfinal_fem_22$regiao.y &lt;-  NULL\n\nlibrary(reshape)\nfinal_fem_22 &lt;- rename(final_fem_22, c(regiao.x=\"regiao\"))\n\nsave(final_fem_22, file = \"dados.RData\")\nwrite.table(final_fem_22, \"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.csv\", sep=\";\")\n\nSelecionando Variáveis:\nVamos selecionar as observações dos estados da região Norte e Nordeste:\n\nfem_n_nd &lt;- final_fem_22[ which(final_fem_22$regiao==\"NE\" | final_fem_22$regiao==\"N\"),]\n\nfem_n_nd1 &lt;- final_fem_22[ which((final_fem_22$regiao==\"NE\" | final_fem_22$regiao==\"N\") & final_fem_22$part_feminic&gt;=50),]\n\nOu podemos selecionar as observações em que a participação do feminicídio está entre 30 e 50%:\n\nfem_n_nd2 &lt;- subset(final_fem_22, final_fem_22$part_feminic &gt;= 30 & final_fem_22$part_feminic &lt;= 50 )",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maipulando os Dados</span>"
    ]
  },
  {
    "objectID": "googledrive.html",
    "href": "googledrive.html",
    "title": "5  Google Drive",
    "section": "",
    "text": "5.1 Introdução\nVamos aqui importar os dados de arquivos compartilhados o Google Drive. Vamos usar arquivos que não precisam de autenticação. São aqueles que ” qualquer um com o link” podem acessar. Para isso terá que utilizar o pacote googledrive.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Drive</span>"
    ]
  },
  {
    "objectID": "googledrive.html#introdução",
    "href": "googledrive.html#introdução",
    "title": "5  Google Drive",
    "section": "",
    "text": "5.1.1 Encontrando o ID\nPrimeiramente precisa ter acesso ao link de compartilhamento. Vejamos o link de compartilhamento do arquivo cjpg.rds\nhttps://drive.google.com/file/d/1vs4hH1xgYl0YzSb2uWeH0gBMh8E5QPL2/view?usp=drive_link\nO ID é a seguinte parte do endereço: 1vs4hH1xgYl0YzSb2uWeH0gBMh8E5QPL2\nCom esse string podemos fazer o download do arquivo\n\n\n5.1.2 Fazendo o download\nPrimeiramente vamos excluir a autenticação com o comando drive_deauth(). Poderá checar a exclusão com o comando drive_user(), ele mostrará que não existe usuário logado ou autenticado.\nDepois indicaremos o caminho do arquivo no google drive e com o caminho indicado ele fará o download na pasta do seu computador indicada na parte superior da aba Console. Depois é só fazer a leitura para transformar o arquivo em .Rdata. Vejamos:\n\nlibrary(googledrive)\n\n             \ndrive_deauth()    # retira a autenticação\ndrive_user()      #verifica se existe usuário logado\ncjpg_public &lt;-  drive_get(as_id(\"1vs4hH1xgYl0YzSb2uWeH0gBMh8E5QPL2\"))   # Obtem o endereço do arquivo\ndrive_download(cjpg_public, overwrite = TRUE)    # faz o download do arquivo na pasta de trabalho\n\n\ncjpg&lt;-readRDS(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/cjpg.rds\")   # faz a leitura do arquivo - colocar o endereço do seu computador\n\nPodemos importar outros arquivos agora:\n\n# IMPORTANTO: final_fem_22\nlibrary(googledrive)\ndrive_deauth()    \ndrive_user()      \n\nfinal_fem_public &lt;-  drive_get(as_id(\"1_GxL3EFsE2JU-39muO_ELoz-kx4-hEpO\"))   \ndrive_download(final_fem_public, overwrite = TRUE)   \n\nfinal_fem_22 &lt;- read.csv(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.csv\", head=T ,sep=\";\")\n\n\n# IMPORTANTO: cpopg_metadados\n\ncpopg_public &lt;-  drive_get(as_id(\"1mtLng43ZAf-sygUjjcaqRaNhuXKC7ol_\"))   \ndrive_download(cpopg_public, overwrite = TRUE)   \n\nlibrary(\"readxl\")\n\ncpopg &lt;- read_excel(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/cpopg_metadados.xlsx\")",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Drive</span>"
    ]
  },
  {
    "objectID": "autores.html#alexandre-nicolella",
    "href": "autores.html#alexandre-nicolella",
    "title": "2  Sobre os Autores",
    "section": "2.1 Alexandre Nicolella",
    "text": "2.1 Alexandre Nicolella\n\nProfessor de Economia da FEA-RP/USP, com formação em Economia Aplicada e experiência em microeconometria, avaliação de projetos e análise de dados, com forte ênfase em métodos quantitativos e modelagem estatística voltados à tomada de decisão em políticas públicas e gestão. Atua em pesquisa e ensino de econometria e análise de dados, com histórico de projetos financiados por organismos nacionais e internacionais\n📘 Currículo Lattes •\n🔗 ORCID",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sobre os Autores</span>"
    ]
  },
  {
    "objectID": "autores.html#josé-de-jesus-filho",
    "href": "autores.html#josé-de-jesus-filho",
    "title": "2  Sobre os Autores",
    "section": "2.2 José de Jesus Filho",
    "text": "2.2 José de Jesus Filho\n\nCientista de dados e especialista em jurimetria, atua no desenvolvimento de ferramentas em R para extração e organização de dados judiciais e na aplicação de métodos estatísticos ao contencioso. É autor do pacote tjsp (R) para coleta e organização de processos e decisões do TJSP, com documentação pública e citação formal do projeto; também ministra cursos e publica tutoriais práticos sobre raspagem, estruturação e análise de dados judiciais (tempo de processo, análises de sobrevivência, etc.). Além do trabalho técnico, participa de eventos da Associação Brasileira de Jurimetria (ABJ) apresentando aplicações de aprendizado estatístico, PLN e sistemas distribuídos em dados judiciais .\n📘 Currículo Lattes •\n🔗 LinkedIn",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sobre os Autores</span>"
    ]
  },
  {
    "objectID": "dplyr.html",
    "href": "dplyr.html",
    "title": "6  O Dplyr",
    "section": "",
    "text": "6.1 Tutorial sobre o Uso do dplyr para Manipulação de Dados no R\nEssa seção é uma criação mesclando o que foi explicado em aula, o Chat GPT e um pouco da nossa organização.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O Dplyr</span>"
    ]
  },
  {
    "objectID": "dplyr.html#tutorial-sobre-o-uso-do-dplyr-para-manipulação-de-dados-no-r",
    "href": "dplyr.html#tutorial-sobre-o-uso-do-dplyr-para-manipulação-de-dados-no-r",
    "title": "6  O Dplyr",
    "section": "",
    "text": "6.1.1 Introdução ao dplyr:\nO dplyr é um poderoso pacote do R projetado para tarefas de manipulação de dados. Ele fornece um conjunto consistente de funções que ajudam a simplificar o processo de manipulação de dados, tornando-o mais intuitivo e eficiente. Neste tutorial, vamos nos aprofundar em cada função fornecida pelo dplyr, juntamente com exemplos.\n\n\n6.1.2 Instalação e Carregamento:\nAntes de mergulharmos no dplyr, assegure-se de tê-lo instalado executando:\n\ninstall.packages(\"dplyr\")\n\nEm seguida, carregue o pacote em sua sessão R:\n\nlibrary(dplyr)\n\n\n\n6.1.3 Visão Geral das Funções:\n\nfilter(): Esta função é usada para filtrar linhas com base em condições específicas. No R: dados_filtrados &lt;- filter(dados, condição).\n\n\nVer também as seleções com escopo filter_all(), filter_if() e filter_at() e as seguintes funções: starts_with(), ends_with(), contains() e matches()\n\n\nselect(): Esta função é usada para selecionar colunas específicas do conjunto de dados. No R: colunas_selecionadas &lt;- select(dados, coluna1, coluna2).\n\n\nVer também select_all(), select_if() and select_at() e as seguintes funções starts_with(), ends_with(), contains(). matches() e num_range()\n\n\narrange(): Esta função é usada para reordenar linhas com base em uma ou mais variáveis.No R: dados_ordenados &lt;- arrange(dados, variável)\nmutate(): Esta função é usada para criar ou modificar colunas dentro do conjunto de dados. No R: dados_modificados &lt;- mutate(dados, nova_coluna = cálculo)\ngroup_by() e summarize(): Essas funções são usadas em conjunto para agrupar dados com base em uma variável e, em seguida, resumi-los. No R: dados_agrupados &lt;- group_by(dados, variável)e resumo &lt;- summarize(dados_agrupados, estatística_resumo = função(variável))\n\n\n\n6.1.4 Operador pipe\nO operador pipe (%&gt;%) é uma ferramenta poderosa em R, especialmente quando combinada com o pacote dplyr. Ele permite encadear várias operações de forma mais legível e eficiente, evitando a necessidade de criar variáveis intermediárias. Vamos explicar o uso do pipe em conjunto com o dplyr.\n1. Encadeamento de Operações:\nSem o pipe, para aplicar várias funções consecutivas a um conjunto de dados, você teria que criar variáveis intermediárias ou aninhar as funções, tornando o código menos legível. Por exemplo, para filtrar dados e depois selecionar colunas usando dplyr, você pode fazer:\n\ndados_filtrados &lt;- filter(dados, condicao)\ncolunas_selecionadas &lt;- select(dados_filtrados, coluna1, coluna2)\n\nCom o pipe, você pode encadear essas operações de forma mais concisa:\n\nresultado &lt;- dados %&gt;%\n  filter(condicao) %&gt;%\n  select(coluna1, coluna2)\n\n2. Melhora a Legibilidade do Código:\nO operador pipe torna o código mais legível e fácil de entender, pois as operações são executadas sequencialmente da esquerda para a direita. Isso facilita a compreensão do fluxo de dados e das transformações aplicadas.\n3. Evita a Criação de Variáveis Intermediárias:\nUsar o pipe evita a necessidade de criar variáveis intermediárias para armazenar resultados parciais, o que economiza espaço na memória e torna o código mais eficiente.\n\n\n6.1.5 Utilizando as funções e o pipe\nContinuaremos a utilizar o mesmo banco de dados sobre feminicídio da seção de manipulação.\n\nfinal_fem_22 &lt;- read.csv(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.csv\", head=T ,sep=\";\")\n\n1. Filtrando Dados:\nUtilizamos os símbolos lógicos para utilizar a função filter:\n\n== Igual a\n!= Não é igual a\n&lt; Menor que\n&lt;= Menor igual que\n&gt; Maior que\n&gt;= Maior igual que\n! NÃO Lógico\n& E Lógico\n| OU Lógico\n%in% Verifica se um valor está em uma matriz de vários valores\nis.na() Checa se um valor e N.A.\n\nVamos filtrar o conjunto de dados para incluir apenas estados que possuem pparticipação da taxa de feminicídio acima de 50% :\n\npart50_mais &lt;- filter(final_fem_22, part_feminic &gt;= 50)\n\nPodemos ter multiplos critérios. Utilizando participação acima de 50 e região norte.\n\npart50_mais_reg &lt;- final_fem_22 |&gt; \n  filter(part_feminic &gt;=50 , regiao==\"N\")\n\nPodemos selecionar os estados da região sul ou da região sudeste para compor o nosso banco selecionado.\n\nregiao1 &lt;- final_fem_22 |&gt; \n  filter(regiao==\"S\" | regiao==\"SD\")\n\n\nregiao2&lt;- final_fem_22 |&gt; \n  filter(regiao %in% c('S', 'SD'))\n\nOu ter um banco sem a região norte\n\nregião3 &lt;- final_fem_22 |&gt; \n  filter(regiao!=\"N\")\n\n2. Selecionando Colunas:\nVamos agora montar um banco apenas as colunas de tx de feminicídio e tx de tentativa de feminicídio:\n\ntx_f &lt;- select(final_fem_22, sigla, feminic_tx, t_feminic_tx)\n\nou\n\ntx_f2 &lt;- final_fem_22 |&gt; \n  select(sigla, feminic_tx, t_feminic_tx)\n\nComo temos a sigla dos estados vamos retirar a coluna com os nomes do estados\n\nsem_nome &lt;- final_fem_22 |&gt; \n  select(-estados)\n\nOu podemos tirar o estado indicando a sua posição. Ele está na coluna 2 do banco.\n\nsem_nome1 &lt;- final_fem_22 |&gt; \n  select(-2)\n\nPodemos selecionar renomeando o nome da coluna. Selecionamos feminicídio por estado\n\ntx_f3 &lt;- final_fem_22 |&gt; \n  select(Estados=estados, Taxa_Fem=feminic_tx)\n\n3. Mudando o nome Podemos mudar o nome das variáveis do banco da seguinte forma:\n\nmud_nomes &lt;- final_fem_22 |&gt; \n  select(estados, feminic_tx) |&gt; \n  rename(Estados=estados, Taxa_Fem=feminic_tx)\n\n4. Mudando Posição da Coluna\nPodemos reoarganizar as colunas nos bancos. Vamos deixar asvariáveis de feminicídio todas junta\n\nreord1 &lt;- final_fem_22 |&gt; \n  select(sigla, estados, regiao, feminic_abs, feminic_tx, t_feminic_abs, t_feminic_tx,everything())\n\nOu podemos usar a função relocatepara deixar a participação do feminicídio e da tentativa de feminicídio todas juntas.\n\nreord2 &lt;- final_fem_22 |&gt; \n  relocate(part_feminic,.after=t_feminic_tx)\n\n5. Ordenando Dados:\nVamos ordenar os estado em ordem crescente da taxa de feminicídio:\n\nordenado1 &lt;- arrange(final_fem_22, feminic_tx)\n\nPodemos ordenar o data frame por duas variáveis. Vamos ordenar primeiro por região e depois por taxa de feminicídio.\n\nestado_feminic &lt;- final_fem_22 |&gt; \n  arrange(regiao,feminic_tx)\n\nOu podemos fazer de forma descendente\n\nestado_feminic1 &lt;- final_fem_22 |&gt; \n  arrange(regiao,desc(feminic_tx))\n\n4. Mutando Dados:\nVamos adicionar uma nova coluna ao conjunto de dados representando do total de tentativas de feminicídio, quantas efetivamente terminam em feminicídio. Para isso vamos dividir o feminicídio pelo total de tentativa e feminicídio. :\n\nfinal_fem_22_1 &lt;- final_fem_22 |&gt; \n  mutate(feminic_efetivado =(feminic_abs/(feminic_abs+t_feminic_abs)),  homic_efetivado =(homic_abs/(homic_abs+t_homic_abs)) )\n\nAgora vamos criar uma variável bnária que será igual a 1 se o homicídio efetivado for maior que 0,30.\n\nfinal_fem_22_1 &lt;- final_fem_22_1 |&gt;  \n  mutate(homi_efet_alto = case_when(homic_efetivado &gt;=0.3 ~ 1,\n                             homic_efetivado &lt;0.3 ~ 0))\n\nAbaixo tem-se algumas funções importantes para texto e datas\n\nd4 &lt;- cjpg |&gt; \n  select(processo) |&gt; \n  mutate(seq=str_sub(processo,1,7),\n         digito=str_sub(processo,8,9),\n         ano=str_sub(processo,10,13),\n         segmento=str_sub(processo,14),\n         tribunal=str_sub(processo,15,16),\n         distribuidor=str_sub(processo,17,20))\n\nd5 &lt;- cjpg |&gt;\n  mutate(ano=year(disponibilizacao),\n         mes=month(disponibilizacao,abbr = FALSE,label=TRUE),\n         dia=wday(disponibilizacao,abbr = FALSE,label=TRUE))\n  \nmetadados &lt;- metadados |&gt; \n  mutate(valor_da_acao=tjsp::numero(valor_da_acao))\n\n5. Agrupando e Resumindo Dados:\nVamos agrupar os feminicídios por região e calcular a taxa média por região:\n\nfeminic_agrupado &lt;- final_fem_22 |&gt; \n  group_by(regiao) |&gt; \n summarize(média_feminic = mean(feminic_tx), \n           DP_feminic=sd(feminic_tx,na.rm=TRUE)\n           )\n\nTemos várias estatísticas que podem ser utilizadas:\n\nsumario&lt;- final_fem_22 |&gt; \n  summarize(media=mean(feminic_tx,na.rm=TRUE),\n            soma=sum(feminic_tx,na.rm=TRUE),\n            Desvio_p=sd(feminic_tx,na.rm=TRUE)\n            )\n\nPodemos contar variáveis categóricas ou mesmo binárias. Vamos contar os estados que tiveram taxa de participação do feminicídio acima de 50%.\n\nalta_part &lt;- final_fem_22 |&gt; \n  count(mais_50,sort=TRUE)\n\nContando alta participação do feminicídio por Região\n\nreg_alt_part &lt;- final_fem_22 |&gt; \n  count(regiao, mais_50, sort=FALSE)\n\nEste tutorial forneceu uma visão geral de funções do dplyr. Dominando essas funções, você estará equipado para manipular e analisar conjuntos de dados de forma eficiente no R, facilitando seus fluxos de trabalho de análise de dados. Experimente essas funções em seus próprios conjuntos de dados para explorar seu potencial total.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>O Dplyr</span>"
    ]
  },
  {
    "objectID": "descritiva_v2.html",
    "href": "descritiva_v2.html",
    "title": "7  Análise Descritiva",
    "section": "",
    "text": "7.1 Cadeia de Valor do Conhecimento\nA construção do conhecimento é um caminho longo e complexo. A cadeia de valor do conhecimento é um modelo que busca representar esse caminho. Vejamos a Figura abaixo:\nPara cumprir esse longo percurso a Ciência de Dados tem um papel fundamental na atualidade.\nA Ciência de Dados é uma área interdisciplinar que combina conhecimentos de estatística, matemática, computação, e no nosso caso do Direito, para extrair conhecimento a partir de dados.\nA Ciência de Dados é um processo que envolve diversas etapas, como a coleta de dados, a organização dos dados, a análise dos dados, a interpretação dos resultados, entre outras. Vejamos a figura sobre ciclo de vida da ciência de dados:\nO ciclo de vida da ciência de dados é um modelo que descreve as etapas envolvidas no processo de análise de dados. Esse modelo é composto por cinco etapas principais:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise Descritiva</span>"
    ]
  },
  {
    "objectID": "descritiva_v2.html#cadeia-de-valor-do-conhecimento",
    "href": "descritiva_v2.html#cadeia-de-valor-do-conhecimento",
    "title": "7  Análise Descritiva",
    "section": "",
    "text": "Ermine, J.L (2013): Cadeia de Valor do Conhecimento\n\n\n\n\n\n\n\nEstratégia Interdisciplinas\n\n\n\n\n\n\nCiclo de Vida da Ciência de Dados\n\n\n\n\nColeta de Dados: Busca de informações que possam ser úteis para responder a uma pergunta de pesquisa. Esses dados podem ser coletados de diversas fontes, como pesquisas de campo, bases de dados, entrevistas, entre outros.\nLimpar e Organização: Pesquisador organiza os dados coletados de forma a facilitar a análise. Isso pode envolver a criação de tabelas, limpeza, estruturação, entre outros aspectos.\nAnálise Descritiva: Busca descrever os dados coletados de forma a identificar padrões, tendências, relações entre variáveis, entre outros aspectos. Isso pode envolver a utilização de estatísticas descritivas, gráficos, mapas, entre outros recursos.\nAnálise Exploratória: Explorar os dados coletados de forma a identificar padrões, tendências, relações entre variáveis, entre outros aspectos. Isso pode envolver a utilização de técnicas estatísticas mais avançadas, como regressão, análise de cluster, análise fatorial, entre outras.\nInterpretação dos Resultados: Interpretar os resultados obtidos na análise descritiva e exploratória de forma a responder à pergunta de pesquisa. Isso pode envolver a elaboração de conclusões, recomendações, entre outros aspectos.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise Descritiva</span>"
    ]
  },
  {
    "objectID": "descritiva_v2.html#estatísticas-descritivas-medidas-numéricas-a-partir-da-amostra",
    "href": "descritiva_v2.html#estatísticas-descritivas-medidas-numéricas-a-partir-da-amostra",
    "title": "7  Análise Descritiva",
    "section": "7.2 Estatísticas Descritivas: Medidas Numéricas a partir da amostra",
    "text": "7.2 Estatísticas Descritivas: Medidas Numéricas a partir da amostra\nA análise descritiva é uma etapa fundamental no processo de análise de dados. Ela consiste em descrever os dados coletados de forma a identificar padrões, tendências, relações entre variáveis, entre outros aspectos.\nVamos entender 3 padrões importantes nessa etapa:\n\nMedidas de Posição: São medidas que indicam a posição de um valor em relação aos demais valores de um conjunto de dados.\nMedidas de Variabilidade: São medidas que indicam o grau de dispersão dos valores de um conjunto de dados.\nMedidas de Associação: São medidas que indicam a relação entre duas ou mais variáveis de um conjunto de dados.\n\nVamos entender os resultados dessas medidas para dois conjuntos distintos de variáveis:\n\nVariáveis Discretas: São variáveis que podem assumir um número finito de valores. Por exemplo, o número de filhos de uma pessoa, o número de processos em uma Vara, procedente ou improcedente.\nVariáveis Contínuas: São variáveis que podem assumir um número infinito de valores. Por exemplo, a renda per capita, a taxa de feminicídio, tempo do processo, dosimetria da pena.\n\nCASO DE ESTUDO:\nVamos tentar entender essas medidas a partir de um banco de dados real e pequeno. Será utilizado um banco de dados extraído do Anuário da Segurança Pública (2023), com informações sobre a taxa de feminicídio, a taxa de tentativa de feminicídio, a taxa de homicídio de mulheres, a renda per capita, entre outras variáveis.\nAs medidas numéricas que veremos para as três padrões a serem observados são:\nMEDIDAS DE POSIÇÃO:\n\nMédia,\nMediana,\nModa,\nPercentis,\nQuartis.\n\nMEDIDAS DE VARIABILIDADE:\n\nAmplitude,\nAmplitude interquartil,\nVariância,\nDesvio Padrão e\nCoeficiente de Variação.\n\nMEDIDAS DE ASSOCIAÇÃO:\n\nCovariancia e\nCoeficiente de Correlção.\n\nVamos apresentar as fórmulas de algumas dessas principais medidas.Primeiramente vams importantaro nosso banco de dados:\n\nfinal_fem_22 &lt;- read.csv(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.csv\", head=T ,sep=\";\")\n\nou\n\ninstall.packages(\"remotes\") # baixar o pacote devtools\nremotes::install_github(\"jjesusfilho/cursoESMP\") # baixar o pacote deste curso\nlibrary(cursoESMP) # carregar o pacote deste curso\nfinal_fem_22&lt;-cursoESMP::final_fem_22\n\nVisualizando as primeiras 10 linhas do banco de dados:\n\nlibrary(kableExtra)\n\nfinal_fem_22 |&gt; \n  head(10) |&gt; \n  kbl(digits = 2,  ) %&gt;%\n     kable_styling()\n\n\n\n\nsigla\nestados\nregiao\nhomic_abs\nhomic_tx\nfeminic_abs\nfeminic_tx\npart_feminic\nrendapc\nmais_50\nt_homic_abs\nt_homic_tx\nt_feminic_abs\nt_feminic_tx\npart_t_feminic\n\n\n\n\nAC\nAcre\nN\n22\n5.3\n11\n2.6\n50.0\n1038\n1\n388\n93.4\n16\n3.9\n3.96\n\n\nAL\nAlagoas\nNE\n73\n4.5\n31\n1.9\n42.5\n935\n0\n160\n9.8\n54\n3.3\n25.23\n\n\nAM\nAmazonas\nN\n88\n4.5\n21\n1.1\n23.9\n965\n0\n83\n4.2\n45\n2.3\n35.16\n\n\nAP\nAmapa\nN\n22\n6.0\n8\n2.2\n36.4\n1177\n0\n95\n25.9\n44\n12.0\n31.65\n\n\nBA\nBahia\nNE\n406\n5.6\n107\n1.5\n26.4\n1010\n0\n582\n8.0\n174\n2.4\n23.02\n\n\nCE\nCeara\nNE\n264\n5.8\n28\n0.6\n10.6\n1050\n0\n324\n7.2\n102\n2.3\n23.94\n\n\nDF\nDistrito Federal\nCO\n32\n2.2\n19\n1.3\n59.4\n2913\n1\n208\n14.2\n88\n6.0\n29.73\n\n\nES\nEspirito Santo\nSD\n95\n4.9\n33\n1.7\n34.7\n1723\n0\n450\n23.1\n70\n3.6\n13.46\n\n\nGO\nGoias\nCO\n137\n3.8\n56\n1.6\n40.9\n1619\n0\n364\n10.2\n168\n4.7\n31.58\n\n\nMA\nMaranhao\nNE\n127\n3.7\n69\n2.0\n54.3\n814\n1\n264\n7.7\n106\n3.1\n28.65\n\n\n\n\n\n\n\n\n\n\n\nDescrição do Banco de Dados\n\n\n\n\n\n\nhomic_abs: Número absoluto de homicídios feminínos registrados em 2022.\nhomic_tx: Taxa de homicídios feminino 100 mil mulheres.\nfeminic_abs: Número absoluto de feminicídios registrados em 2022.\nfeminic_tx: Taxa de feminicídios por 100 mil mulheres.\npart_feminic: Participação percentual dos feminicídios no total de homicídios femininos.\nrendapc: Renda per capita do Estado.\nmais_50: Proporção de feminicídios acima de 50%.\nt_homic_abs: Número absoluto da tentativa de homicídios femininos\nt_homic_tx: Taxa de tentativa de homicídios para 100 mil mulheres.\nt_feminic_abs: Número absoluto de tentativa de feminicídios.\nt_feminic_tx: Taxa de tentativa de feminicídios por 100 mil mulheres.\npart_t_feminic: Participação percentual da tentativa de feminicídios em relação ao total de tentativas de homicídios femininos.\n\n\n\n\n\n7.2.1 Gráficos no R\nO ggplot2 é um pacote do R utilizado para criar gráficos estatísticos de forma poderosa e flexível. Apesar de parecer complexo, é fácil de aprender, pois possui princípios básicos simples e poucos casos excepcionais.O ggplot2 é projetado para ser utilizado iterativamente: você começa com os dados brutos e adiciona camadas de anotações ou resumos estatísticos.\nA GRAMÁTICA DOS GRÁFICOS\nO ggplot2 é um pacote do R que implementa a Grammar of Graphics (Wilkinson, 2005), uma abordagem para criar gráficos estatísticos. Ele organiza os gráficos em componentes fundamentais, permitindo flexibilidade e personalização.\nUm gráfico no ggplot2 é construído combinando os seguintes elementos:\n\nDados: A base de dados a ser visualizada.\nMapeamentos estéticos(aes): Define como variáveis são associadas a atributos visuais (cor, forma, tamanho).\nCamadas (layers): Compostas por:\n\nGeoms: Elementos visuais - tipos de gráficos (pontos, linhas, barras).\nStats: Transformações estatísticas ou ajustar modelos.\nEscalas (scales): Determina as esclaas de x e y (eixos, legendas, cores).\n\nSistemas de coordenadas (coord): Define como os dados são posicionados no gráfico (ex.: cartesiano ou polar).\nFacetamento (facet): Divide os dados em subconjuntos para gerar gráficos separados.\nTema (theme): Personaliza detalhes visuais, como fontes e cores de fundo.\n\nClique abaixo para ver alguns tipos de gráficos que podem ser criados com o ggplot2:\n\n\n\n\n\n\nVeja aqui Algumas Possibilidades de Camadas\n\n\n\n\n\n\ngeom_area(): cria um gráfico de área.\n\ngeom_density(): cria um gráfico de densidade de kernel.\n\ngeom_dotplot(): cria um gráfico de pontos.\n\ngeom_freqpoly(): cria um polígono de frequência.\n\ngeom_histogram(): cria um histograma.\n\nstat_ecdf(): plota a função de densidade acumulada empírica.\n\nstat_qq(): cria um gráfico quantil-quantil.\n\ngeom_bar(): cria um gráfico de barras.\n\n\n\n\nVejamos um exemplo inicial de um gráfico da taxa de homicídio feminino e a taxa de feminicídio por Estado no Brasil\nPrimeiro, definindo os dados que serão utilizados e os eixos x e y, vejamos o que ocorre:\n\nlibrary(ggplot2)\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))\n\n\n\n\n\n\n\n\nAgora podemos adicionar uma camada de pontos ao gráfico:\n\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))+\n  geom_point()\n\n\n\n\n\n\n\n\nPodemos fazer com que cada ponto seja associado a uma região do Brasil:\n\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx, colour=regiao))+\n  geom_point()\n\n\n\n\n\n\n\n\nOu podemos fazer que todos os pontos seja azul escuro:\n\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))+\n  geom_point(color=\"darkblue\")\n\n\n\n\n\n\n\n\nPara as cores uma possibilidade bem flexível é usar cores Hexadecimais. Vamos usar a cor #6e94bd para os pontos:\nPodemos mudar o tamanho size, a transparência com alpha e a forma shape dos pontos:\n\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))+\n  geom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\n  scale_x_continuous(limits = c(0, 13))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClique aqui para ver os shapes disponíveis\n\n\n\n\n\n\nshape = 0: quadrado\n\nshape = 1: círculo\n\nshape = 2: triângulo apontando para cima\n\nshape = 3: sinal de mais (+)\n\nshape = 4: cruz (x)\n\nshape = 5: losango\n\nshape = 6: triângulo apontando para baixo\n\nshape = 7: quadrado com cruz\n\nshape = 8: estrela\n\nshape = 9: losango com sinal de mais\n\nshape = 10: círculo com sinal de mais\n\nshape = 11: triângulos apontando para cima e para baixo\n\nshape = 12: quadrado com sinal de mais\n\nshape = 13: círculo com cruz\n\nshape = 14: quadrado com triângulo apontando para baixo\n\nshape = 15: quadrado preenchido\n\nshape = 16: círculo preenchido\n\nshape = 17: triângulo preenchido apontando para cima\n\nshape = 18: losango preenchido\n\nshape = 19: círculo sólido\n\nshape = 20: ponto (círculo menor)\n\nshape = 21: círculo preenchido azul\n\nshape = 22: quadrado preenchido azul\n\nshape = 23: losango preenchido azul\n\nshape = 24: triângulo preenchido apontando para cima azul\n\nshape = 25: triângulo preenchido apontando para baixo azul\n\n\n\n\nAgora vamos adicionar uma escala para o eixo de x entre 0 e 13 e vamos dividir o gráfico por região:\n\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))+\n  geom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\n  scale_x_continuous(limits = c(0, 13))+\n  facet_wrap(~regiao)\n\n\n\n\n\n\n\n\nPode-se também adicionar uma linha de tendência ao gráfico:\n\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))+\n  geom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\n  scale_x_continuous(limits = c(0, 13))+\n  geom_smooth(method = \"lm\", se=FALSE , color=\"orange\")+\n  facet_wrap(~regiao)\n\n\n\n\n\n\n\n\nE por fim pode-se mudar o título e os rótulos dos eixos:\n\nlibrary(ggthemes)\nggplot(data = final_fem_22, aes(x = homic_tx, y = feminic_tx))+\ngeom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\nscale_x_continuous(limits = c(0, 13))+\ngeom_smooth(method = \"lm\", se=FALSE , color=\"orange\")+\nfacet_wrap(~regiao)+  \nlabs(title=\"Homicídios e Feminicídios por Região\", x=\"Homicídios\", y=\"Feminicídios\")+\ntheme_bw()\n\n\n\n\n\n\n\n\nPara os temas pode acessar a Galeria ou Exemplos Dinâmicos.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise Descritiva</span>"
    ]
  },
  {
    "objectID": "descritiva_v2.html#medidas-de-posição",
    "href": "descritiva_v2.html#medidas-de-posição",
    "title": "7  Análise Descritiva",
    "section": "7.3 Medidas de Posição:",
    "text": "7.3 Medidas de Posição:\nMÉDIA AMOSTRAL:\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\n\nmean(final_fem_22$feminic_tx)\n\n[1] 1.651852\n\n\nA média da taxa de Feminicídio entre os Estados do Brasil é de 1,65 por 100 mil mulheres em 2022 .\n\nlibrary(kableExtra)\n\nmean_1&lt;- aggregate(final_fem_22[4:15], list(final_fem_22$regiao), mean,  na.rm=T)\n\nt(mean_1) %&gt;% \n  kbl(digits = 2) %&gt;%\n     kable_styling()\n\n\n\n\nGroup.1\nCO\nN\nNE\nS\nSD\n\n\nhomic_abs\n86.25000\n69.85714\n149.11111\n212.66667\n277.50000\n\n\nhomic_tx\n4.250000\n6.785714\n4.355556\n4.000000\n3.300000\n\n\nfeminic_abs\n40.50000\n18.57143\n43.55556\n81.00000\n127.50000\n\n\nfeminic_tx\n2.100000\n1.871429\n1.422222\n1.600000\n1.375000\n\n\npart_feminic\n50.02500\n30.01429\n34.36667\n41.53333\n43.82500\n\n\nrendapc\n2011.250\n1175.286\n1053.222\n1983.667\n1842.750\n\n\nmais_50\n0.5000000\n0.1428571\n0.2222222\n0.3333333\n0.2500000\n\n\nt_homic_abs\n258.2500\n161.7143\n257.7778\n450.6667\n455.7500\n\n\nt_homic_tx\n13.375000\n24.657143\n9.311111\n8.966667\n8.875000\n\n\nt_feminic_abs\n126.33333\n50.28571\n85.11111\n169.66667\n185.66667\n\n\nt_feminic_tx\n6.500000\n5.285714\n3.044444\n3.500000\n3.000000\n\n\npart_t_feminic\n32.67667\n26.44286\n25.72444\n25.98667\n26.50000\n\n\n\n\n\nMEDIANA:\nOrganize os dados em ordem crescente.\n\nPara um numero impar de observações a mediana é o valor que ocupa a posição central.\npara um número par de observações, a mediana é a média dos dois valores centrais.\n\n\nmedian(final_fem_22$feminic_tx)\n\n[1] 1.5\n\n\nVeja que a mediana é um valor diferente da média. Elas medem coisas diferentes. Veja o exemplo abaixo\n\nmedian(c(1,3, 14))\n\n[1] 3\n\nmean(c(1,3, 14))\n\n[1] 6\n\n\nMediana e média são bastante distintas. Vamos fazer uma tabela com a Média e Mediana, para o nosso conjunto de dados.\n\nfun1 &lt;- function(x, na.rm = TRUE) c(Média=mean(x, na.rm = TRUE), Mediana=median(x, na.rm = TRUE))\n\nmedian_1 &lt;- (sapply(final_fem_22[4:15], fun1))\n\nt(median_1) %&gt;% \n  kbl(digits = 2) %&gt;%\n     kable_styling()\n\n\n\n\n\nMédia\nMediana\n\n\n\n\nhomic_abs\n145.33\n95.00\n\n\nhomic_tx\n4.77\n4.50\n\n\nfeminic_abs\n53.22\n33.00\n\n\nfeminic_tx\n1.65\n1.50\n\n\npart_feminic\n37.76\n38.90\n\n\nrendapc\n1447.15\n1267.00\n\n\nmais_50\n0.26\n0.00\n\n\nt_homic_abs\n283.70\n264.00\n\n\nt_homic_tx\n13.79\n10.00\n\n\nt_feminic_abs\n102.52\n88.00\n\n\nt_feminic_tx\n4.14\n3.60\n\n\npart_t_feminic\n26.88\n29.73\n\n\n\n\n\nMODA:\nModa é o valor que ocorre com mais frequência.\n\nmoda &lt;- function(x, na.rm=T) {\nmodal &lt;- unique(x, na.rm=T)\nmodal[which.max(tabulate(match(x, modal)))]\n}\nmoda(final_fem_22$feminic_tx)\n\n[1] 1.3\n\n\n\nfun1 &lt;- function(x, na.rm = TRUE) c(Média=mean(x, na.rm = TRUE), Mediana=median(x, na.rm = TRUE), Moda=moda(x))\n\nmedian_1 &lt;- (sapply(final_fem_22[4:15], fun1))\n\nt(median_1) %&gt;% \n  kbl(digits = 2) %&gt;%\n     kable_styling()\n\n\n\n\n\nMédia\nMediana\nModa\n\n\n\n\nhomic_abs\n145.33\n95.00\n22.0\n\n\nhomic_tx\n4.77\n4.50\n4.5\n\n\nfeminic_abs\n53.22\n33.00\n19.0\n\n\nfeminic_tx\n1.65\n1.50\n1.3\n\n\npart_feminic\n37.76\n38.90\n50.0\n\n\nrendapc\n1447.15\n1267.00\n1010.0\n\n\nmais_50\n0.26\n0.00\n0.0\n\n\nt_homic_abs\n283.70\n264.00\n373.0\n\n\nt_homic_tx\n13.79\n10.00\n4.2\n\n\nt_feminic_abs\n102.52\n88.00\n54.0\n\n\nt_feminic_tx\n4.14\n3.60\n2.3\n\n\npart_t_feminic\n26.88\n29.73\nNA\n\n\n\n\n\n\n7.3.0.1 Visualizando as medidas por categorias\nGráfico de Pizza\nO gráfico de pizza indica as proporções de uma determinada variável de classe. Aqui utilizamos a média do número de feminicídios absoluto por região.\n\nggplot(mean_1, aes(x = \"\", y = feminic_abs, fill = Group.1)) +\n  geom_col(width = 1, color = \"lightgray\") +\n  coord_polar(theta = \"y\") +\n  labs(title = \"Média Feminicídio por Região\", fill = \"Região\") +\n  theme_minimal() +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())+ \n  scale_fill_brewer(palette=\"PuBu\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercício\n\n\n\nPERGUNTA: Qual a proporção de estados que possuem taxas de feminicídio acima de 50%?\n\n\n\n\n\n\n\n\nVeja a Resposta\n\n\n\n\n\nRESPOSTA: Participação é uma variável disecreta do tipo 0 ou 1 (binárias) que indica se a taxa de feminicídio é maior que 50%.\n\nlibrary(dplyr)\nfinal_fem_22 |&gt; \n  group_by(mais_50) |&gt;\n  summarise(cont = n()) |&gt; \n  mutate(mais_50F = factor(mais_50, levels = c(0, 1), labels = c(\"Abaixo de 50%\", \"Acima de 50%\"))) |&gt; \n  ggplot( aes(x=\"\", y =cont, fill=mais_50F)) +\n  geom_bar(stat=\"identity\", width = 1, color = \"lightgray\") +\n  coord_polar(theta = \"y\", start=0)+\n  labs(title = \"Participação do Feminicídio no Total de Homicídios Femininos\", fill = \"Participação Feminicídio\") +\n  theme_minimal() +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())+\n  scale_fill_brewer(palette=\"Blues\")\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Barra\nEm geral a utilização do gráfico de barras está relacionado ao entendimento da frequência de valores associados a uma determinada categoria. Por exemplo, imagine que temos um banco de dados com as pessoas classificadas como: i)Não trabalha; ii)Trabalha e iii)Desempregado. Teríamos três categorias e a frequência de pessoas em cada categoria. Poderíamos ainda dividir essa categoria entre homens e mulheres. Essa é a utilização mais padrão do gráfico de barras. Vejamos alguns gráficos:\n\nggplot(final_fem_22, aes(x = reorder(estados, -feminic_tx), y = feminic_tx)) +\n  geom_bar(stat = \"identity\", fill = \"salmon2\", alpha=0.5, color = \"lightsalmon4\") +\n  labs(title = \"Taxa de Feminicídio por Estado\",\n       x = \"\",\n       y = \"Tx. por 100 mil\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 9)) \n\n\n\n\n\n\n\n\nVamos fazer um gráfico de barras para a participação do Feminicídio no total de homicídios femininos por Estado.\n\nggplot(final_fem_22, aes(x = reorder(estados, -part_feminic), y = part_feminic)) + # Ordem decrescente\n  geom_col( fill = \"#3f566f\", alpha=0.7, color = \"#4c5471\") +                      # Adiciona as barras, cor das barras, transparencia \n  labs(                                                                            # e cor da borda\n    title = \"Participação da Taxa de Feminicídio no Total por Estado\",             # Título do gráfico\n    x = \"\",                                                                        # Rótulo do eixo x\n    y = \"Tx. por 100 mil\"                                                          # Rótulo do eixo y\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 9))              # Ajusta o tamanho do texto do eixo x\n\n\n\n\n\n\n\n\nGráfico de Barras Agrupadas\nVamos fazer um gráfico de barras agrupadas para a taxa de feminicídio e taxa de tentativa de feminicídio por região. Vamos passo a passo:\n\nTransformar os dados em formato long\n\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Transformar os dados no formato long\ntx_long &lt;- final_fem_22 %&gt;%\n  arrange(desc(feminic_tx)) %&gt;%                          # Ordena pelos valores de feinic_tx, decrescente\n  select(estados, regiao, feminic_tx, t_feminic_tx) %&gt;%  # Seleciona as colunas de interesse\n  pivot_longer(cols = c(feminic_tx, t_feminic_tx),       # Converte para formato longo\n               names_to = \"tipo\",                        # Nome da coluna que identificará as variáveis\n               values_to = \"taxa\")                       # Nome da coluna que armazenará os valores\n\nhead(tx_long)\n\n# A tibble: 6 × 4\n  estados            regiao tipo          taxa\n  &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1 Rondonia           N      feminic_tx     3.1\n2 Rondonia           N      t_feminic_tx   2.9\n3 Mato Grosso do Sul CO     feminic_tx     2.9\n4 Mato Grosso do Sul CO     t_feminic_tx   8.8\n5 Acre               N      feminic_tx     2.6\n6 Acre               N      t_feminic_tx   3.9\n\n\n\nCriar o gráfico de barras lado a lado\n\n\n# Criar o gráfico de barras lado a lado\nggplot(tx_long, aes(x = reorder(estados, -taxa), y = taxa, fill = tipo)) +\n  geom_bar(stat = \"identity\",alpha=0.7,  position = \"dodge\", color = \"white\") +               # Barras lado a lado posição dodge faz isso\n  labs(\n    title = \"Comparação das Taxas de Feminicídio por Estado\",                     # Título do gráfico\n    x = \"\", # Rótulo do eixo x                                                    # Rótulo do eixo x, y e legenda\n    y = \"Tx. por 100 mil\",                                                        \n    fill = \"Tipo de Taxa\"                                                                \n  ) +\n  scale_fill_manual(\n        values = c(\"feminic_tx\" = \"#7185cc\", \"t_feminic_tx\" = \"#c2986f\"),              # Definir as cores para cada variável\n        labels = c(\"feminic_tx\" = \"Feminicídio\", \"t_feminic_tx\" = \"Tentativa de Fem.\") # Definir os rótulos para cada variável\n                    ) +  \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))                    # Ajusta o tamanho do texto do eixo x\n\n\n\n\n\n\n\n\nGráfico de Linha\nO gráfico de linha ou pontos é muito utilizado para visualização da evolução de séries. É um gráfico que nos permite ver a evolução dos salários entre homens e mulheres, evolução dos preços dos alimentos, evolução do número de casos de feminicídio, evolução dos processos em determinada Vara.\nNosso banco é uma fotografia e não uma evolução, o que torna esse tipo de visualização menos útil. Vejamos a como a taxa de feminicídio evolui com o aumento do número total Homicídios Femininos.\n\n# Ordenar os dados pelo número absoluto de homicídios do menor para o maior\nfinal_fem_22 &lt;- final_fem_22[order(-final_fem_22$homic_abs),]\n\n# Criar o gráfico com ggplot2\nggplot(final_fem_22, aes(x = homic_abs)) +\n# Linha para 't_feminic_tx'\n  geom_line(aes(y = t_feminic_tx, color = \"Tentativa\"), linewidth = 0.8, alpha=0.4) +\n  geom_point(aes(y = t_feminic_tx, color = \"Tentativa\"), size = 3, shape = 17) +\n# Linha para 'feminic_tx'\n  geom_line(aes(y = feminic_tx, color = \"Feminicídio\"), linewidth = 0.8, alpha=0.4) +\n  geom_point(aes(y = feminic_tx, color = \"Feminicídio\"), size = 3, shape = 19) +\n# Escala de cores para as linhas\n  scale_color_manual(\n    values = c(\"Tentativa\" = \"#c2986f\", \"Feminicídio\" = \"#7185cc\")\n  ) +\n# Títulos e rótulos\n  labs(\n    title = \"Tentativa e Feminicídio: Taxa\",\n    x = \"Total de Homicídios\",\n    y = \"Taxa / 100 mil\",\n    color = \"Tipo\"\n  ) +\n # Ajustes de tema\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),  # Centraliza o título, tamanho e negrito\n    legend.position = \"top\",                                           # Posiciona a legenda no topo\n    legend.title = element_text(size = 10),                            # Ajusta o tamanho do título da legenda\n    legend.text = element_text(size = 9)                               # Ajusta o tamanho do texto da legenda\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercício\n\n\n\nPERGUNTA: Será que quanto maior for a renda per capita, menor será a taxa de feminicídio?\n\n\n\n\n\n\n\n\nVeja a Resposta\n\n\n\n\n\nRESPOSTA: Renda e taxa de feminicídio são variáveis continuas. O ideal seria utilizar um gráfico de dispersão para verificar a relação entre essas variáveis.\n\nggplot(data = final_fem_22, aes(x = rendapc, y = feminic_tx))+\ngeom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\ngeom_smooth(method = \"lm\", se=TRUE , color=\"orange\")+\nlabs(title=\"Renda pc e taxa de Feminicídios por Estado\", x=\"Renda pc\", y=\"Tx 100 mil\")+\ntheme_bw()\n\n\n\n\n\n\n\n\n\n\n\nPERCENTIL:\nO p-esimo percentil é um valor tal que ao menos p por cento das observações são menores ou iguais à ele e pelo menos (100-p) por cento das observações são maiores ou iguais a esse valor.\nEtapas para calcular o p-ésimo percentil\nEtapa 1: Organize os dados em ordem crescente.\nEtapa 2: Calcule um indice, \\(i\\) tal que \\[i = \\frac{p}{100} \\times n\\] onde \\(p\\) é o percentil procurado\nEtapa 3:\n\nSe \\(i\\) não for um número inteiro, arredondeo-o para cima. O próximo número inteiro maior que \\(i\\) denota a posição do p-esimo percentil.\nSe \\(i\\) for um número inteiro o p-esimo percentil será a média dos valores que o ocupam as posições \\(i\\) e \\(i + 1\\).\n\n\nquantile(final_fem_22$feminic_tx, probs = c(0.10,0.30,0.60,0.85), na.rm=T)\n\n 10%  30%  60%  85% \n0.96 1.30 1.66 2.24 \n\n\nQUARTIS:\nOs quartis são medidas estatísticas que dividem um conjunto de dados ordenados em quatro partes iguais. É um caso particular do percentil.\nO três quartis são:\nPrimeiro Quartil (Q1): Representa o valor abaixo do qual está situada a primeira quarta parte (ou 25% inferiores) dos dados quando eles estão ordenados em ordem crescente. O primeiro quartil é o valor que divide os dados em 25% (ou 0.25) abaixo e 75% (ou 0.75) acima desse ponto.\nSegundo Quartil (Q2): Corresponde à mediana dos dados, dividindo o conjunto em duas metades iguais. É o valor que separa os 50% inferiores dos 50% superiores dos dados.\nTerceiro Quartil (Q3): Indica o valor acima do qual está situada a terceira quarta parte (ou 25% superiores) dos dados quando eles estão ordenados. Assim como o primeiro quartil, o terceiro quartil divide os dados em 75% (ou 0.75) abaixo e 25% (ou 0.25) acima desse ponto\n\nquantile(final_fem_22$feminic_tx, na.rm=T)\n\n  0%  25%  50%  75% 100% \n0.60 1.30 1.50 1.95 3.10",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise Descritiva</span>"
    ]
  },
  {
    "objectID": "descritiva_v2.html#medidas-de-variabilidade",
    "href": "descritiva_v2.html#medidas-de-variabilidade",
    "title": "7  Análise Descritiva",
    "section": "7.4 Medidas de Variabilidade",
    "text": "7.4 Medidas de Variabilidade\nAMPLITUDE:\nAmplitude é a diferença entre o valor mínimo e máximo de uma série de dados.\n\\[\\text{Amplitude} = \\text{Maior Valor} - \\text{Menor Valor}\\]\n\nmin_max&lt;-range(final_fem_22$feminic_tx)\namp&lt;-min_max[2]-min_max[1]\namp\n\n[1] 2.5\n\n\nAMPLITUDE INTERQUANTIL:\nA amplitude interquartil é dada pela diferenca entre o terceiro (\\(Q_3\\)) e o primeiro quartil (\\(Q_1\\)).\n\\[IQR = Q_3 - Q_1\\]\n\nqs&lt;-quantile(final_fem_22$feminic_tx, na.rm=T)\niq&lt;-qs[4]-qs[2]\nIQR(final_fem_22$feminic_tx)\n\n[1] 0.65\n\niq\n\n 75% \n0.65 \n\n\nVARIÂNCIA E DESVIO PADRÃO AMOSTRAL:\nA variância amostral é uma medida estatística que indica o quão dispersos estão os dados em relação à média amostral. Em outras palavras, ela quantifica a extensão das diferenças individuais entre os valores observados e a média da amostra.\nDefinimos a variância amostral como:\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\] onde \\(\\bar{x}\\) é a média amostral.\nVejamos a variância da taxa de feminicídio e do femnicídio absoluto:\n\nvar(final_fem_22$feminic_tx, na.rm=T)\n\n[1] 0.3818234\n\nvar(final_fem_22$feminic_abs, na.rm=T)\n\n[1] 2364.718\n\n\nO Desvio padrão amostral é derivado da variância. Podemos qualcular essa estatística da seguinte maneira:\n\\[s = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\]\nPara os nossos dados anteriores:\n\n# Tx Feminicídio\nsd(final_fem_22$feminic_tx, na.rm=T)\n\n[1] 0.6179186\n\n#Feminicídio Absoluto\nsd(final_fem_22$feminic_abs, na.rm=T)\n\n[1] 48.62837\n\n\nVamos consolidar agora nossas estatísticas descritivas em uma tabela:\n\nfun1 &lt;- function(x, na.rm = TRUE) c(Média=mean(x, na.rm = TRUE), Mediana=median(x, na.rm = TRUE), Var=var(x, na.rm = TRUE), DP=sd(x, na.rm = TRUE))\n\nest_descrit &lt;- (sapply(final_fem_22[4:15], fun1))\n\nt(est_descrit) %&gt;% \n  kbl(digits = 1) %&gt;%\n     kable_styling()\n\n\n\n\n\nMédia\nMediana\nVar\nDP\n\n\n\n\nhomic_abs\n145.3\n95.0\n13985.0\n118.3\n\n\nhomic_tx\n4.8\n4.5\n4.4\n2.1\n\n\nfeminic_abs\n53.2\n33.0\n2364.7\n48.6\n\n\nfeminic_tx\n1.7\n1.5\n0.4\n0.6\n\n\npart_feminic\n37.8\n38.9\n175.8\n13.3\n\n\nrendapc\n1447.1\n1267.0\n243910.6\n493.9\n\n\nmais_50\n0.3\n0.0\n0.2\n0.4\n\n\nt_homic_abs\n283.7\n264.0\n25685.8\n160.3\n\n\nt_homic_tx\n13.8\n10.0\n286.9\n16.9\n\n\nt_feminic_abs\n102.5\n88.0\n5692.1\n75.4\n\n\nt_feminic_tx\n4.1\n3.6\n5.9\n2.4\n\n\npart_t_feminic\n26.9\n29.7\n83.3\n9.1\n\n\n\n\n\nCOEFICIENTE DE VARIAÇÃO:\nO coeficiente de variação (CV) é uma medida de dispersão relativa que expressa a variabilidade dos dados como uma porcentagem da média.\n\\[\\text{CV} = \\left( \\frac{\\text{Desvio padrão}}{\\text{Média}} \\right) \\times 100\\%\\]\n\n# O R nao tem nehuma funçao para isso, mas podemos fazer isso rapidamente\nprint((sd(final_fem_22$feminic_abs, na.rm = TRUE)/ mean(final_fem_22$feminic_abs, na.rm = TRUE)) * 100)\n\n[1] 91.36854\n\nprint((sd(final_fem_22$t_feminic_abs, na.rm = TRUE)/ mean(final_fem_22$t_feminic_abs, na.rm = TRUE)) * 100)\n\n[1] 73.59146\n\n\nPara entendermos vamos supor que existam duas variáveis com mesmo desvio padrão, igual a 10. A primeira terá média de 10 e a segunda de 20, vejamos a diferença no coeficiente de variação.\n\\(\\text{CV}_{X_1}=\\frac{10}{10}.100=100\\%\\) e \\(\\text{CV}_{X_2}=\\frac{10}{20}.100=50\\%\\)\nA variabilidade relativa é menor para a segunda variável. No exemplo acima a taxa de feminicídio tem uma variabilidade relativa maior (91%) do que a tentativa de feminicídio (74%) entre os Estados Brasileiros.\n\n7.4.0.1 Visualizando a Distribuição dos Dados\nBoxplot\nO boxplot é um gráfico que traz muitas informações e pode ser visto como a distribuição de probabilidade dos dados. O box ou caixa contém 50% dos dados. O limite superior indica o percentil de 75% (Q3) e o limite inferior indica o percentil de 25% (Q1). A linha que corta o box indica a mediana, ou seja, Q2. Os bigodes são calculados com base na distância interquantílica, ou seja,\nLimite inferior: Q1-1,5(Q3-Q1)\nLimite superior:Q3+1,5(Q3-Q1)\nDados fora desses limites são classificados como suspeitos de serem outliers. Podemos observar a assimetria dos dados quando a mediana não está no meio da caixa, indicando maior densidade na menor distância entre os quartis Q1 ou Q3 e a mediana Q2. Vejamos agora o boxplot da taxa de feminicídio e da taxa de feminicídio por região.\n\nggplot(final_fem_22, aes(y = feminic_tx)) +\n  geom_boxplot(fill = \"steelblue\", color = \"darkblue\", alpha=0.7,  # Linhas tracejadas no boxplot\n    outlier.shape = 16, outlier.color = \"red\", outlier.size = 3) + # Boxplot com preenchimento azul e bordas pretas\n  labs(\n    title = \"Boxplot da Taxa de Feminicídio no Brasil, 2022\", # Título do gráfico\n    x = \"\",                                                   # Sem rótulo no eixo x\n    y = \"Tx de Feminicídio / 100 mil\"                        # Rótulo do eixo y\n  ) +\n  coord_flip() + # Inverte os eixos para horizontalidade\n  scale_x_continuous(limits = c(-0.8, 0.8)) +                      # Limita o eixo y entre 0 e 6\n  theme_bw() + # Tema limpo e moderno\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"), # Centraliza e estiliza o título\n    axis.text.y = element_text(size = 10), # Ajusta o tamanho do texto no eixo y\n    axis.title.y = element_text(size = 12) # Ajusta o tamanho do rótulo do eixo y\n  )\n\n\n\n\n\n\n\n\n\nggplot(final_fem_22, aes(x = regiao, y = feminic_tx, fill = regiao)) +\n  geom_boxplot( \n    color = \"darkblue\", alpha=0.7,                                     # Boxplot linha azul, outlier vermelho e transparete \n    outlier.shape = 16, outlier.color = \"red\", outlier.size = 3\n              ) + \n  labs(\n    title = \"Boxplot da Taxa de Feminicídio no Brasil, 2022\",         # Título do gráfico, x e y e nome da legenda\n    x = \"Região\",                                                   \n    y = \"Tx de Feminicídio / 100 mil\",\n    fill = \"Região\"  # Rótulo do eixo y\n      ) +\n  coord_flip() +                                                      # Inverte os eixos para horizontalidade\n  theme_bw() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"), # Centraliza e estiliza o título\n    axis.text.y = element_text(size = 10),                           # Ajusta o tamanho do texto no eixo y\n    axis.title.y = element_text(size = 12),                          # Ajusta o tamanho do rótulo do eixo y\n    legend.position = c(0.9,0.7)                                     # Posição da Legenda quadrado de 1x1\n  )+\nscale_fill_brewer(palette=\"Blues\")\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nO Violin Plot é muito parecido com o BoxPlot mas com a densidade de kernel rotacionada em cada um dos lados. Assim, indica a distribuição dos dados em cada ponto e vem anotado a mediana na forma de um ponto ou marca e um pequeno boxplot no centro do violin plot.\n\nlibrary(dplyr)\n\nfinal_fem_22 &lt;- final_fem_22 %&gt;%\n  mutate(N_NE_CO = case_when(\n    regiao %in% c(\"N\", \"NE\", \"CO\") ~ 1,  # Região Norte, Nordeste e Centro-Oeste recebem 1\n    TRUE ~ 0                             # Demais regiões recebem 0\n  ))\n\n# Verificar a distribuição da variável N_NE_CO por região\ntable(final_fem_22$N_NE_CO, final_fem_22$regiao)\n\n   \n    CO N NE S SD\n  0  0 0  0 3  4\n  1  4 7  9 0  0\n\n\n\nggplot(final_fem_22, aes(x = factor(N_NE_CO, labels = c(\"Sul e Sudeste\", \"Norte, Nord. e C.Oeste\")),  # Transformando em fatores\n                         y = feminic_tx, fill = factor(N_NE_CO))) +                                   # Taxa de Feminicídio por Fator\n  geom_violin(trim = FALSE, color = \"white\") +                                                        # Cria o gráfico de violino\n  scale_fill_manual(values = c(\"lightblue\", \"steelblue\")) +                                           # Cor das violas\n  stat_summary(fun=\"median\", geom = \"point\", shape=19, size=3, color=\"darkorange\"  ) +                # Vamos colocar o ponto mediana\n  labs(\n    title = \"Violin Plots da taxa de feminicídio por região do Brasil\",\n    x = \"Região\",\n    y = \"Taxa de Feminicídio (por 100 mil)\",\n    fill = \"Região\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),              # Centraliza o título\n    axis.text.x = element_text(size = 12),                                         # Ajusta o tamanho dos rótulos no eixo X\n    axis.text.y = element_text(size = 10),                                         # Ajusta o tamanho dos rótulos no eixo Y\n    legend.position = \"none\"  )\n\n\n\n\n\n\n\n\nGráfico de Densidade\nVisualizar a distribuição empírica dos dados fornece uma grande quantidade de informação. Um gráfico básico em análise descritiva é o histograma, o qual fornece a distribuição de probabilidade empírica dos dados em um formato de barras. A área do histograma é igual a 1 e altura da sua barra da a densidade de observações em cada classe. “#c2986f”, “Feminicídio” = “#7185cc”)\n\nggplot(final_fem_22, aes(x = feminic_tx)) +\n  geom_histogram(bins = 8, fill = \"#7185cc\", color = \"white\", alpha = 0.7) +    # bins são os números de barras\n# Linha vertical com 70% de transparência, que mostra a média da taxa de feminicídio\n  geom_vline(aes(xintercept = mean(feminic_tx)), color = \"#c2986f\", size = 1, alpha = 0.7,linetype = \"dashed\") +  \n  labs(\n    x = \"Taxa de Feminicídio\",                                                  # Título do eixo X, y e grafico\n    y = \"Frequência\",  \n    title = \"Histograma da Taxa de Feminicídio\"   \n  ) +\n  theme_minimal() + \n theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),              # Centraliza o título\n    axis.text.x = element_text(size = 12),                                         # Ajusta o tamanho dos rótulos no eixo X\n    axis.text.y = element_text(size = 10))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nUma outra maneira de visualizar os dados é utilizando uma distribuição continua e não mais a discreta. Para isso, utiliza-se a densidade de Kernel para visualização da distribuição de probabilidade da taxa de feminicídio. Vejamos\n\n ggplot(final_fem_22, aes(x = feminic_tx)) +\n  geom_density(fill = \"#7185cc\", color = \"darkblue\", alpha = 0.6) +   # Curva de densidade e preenchimento\n   scale_x_continuous(limits = c(0, 4)) +                               # Limita o eixo X entre 0 e 10  \n  labs(\n    x = \"Taxa de Feminicídio\",                                          # Título do eixo X, Y e Gráfico\n    y = \"Densidade\",  \n    title = \"Densidade de Kernel para a Taxa de Feminicídio\"  \n  ) +\n  theme_minimal()  + \n theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),              # Centraliza o título\n    axis.text.x = element_text(size = 12),                                         # Ajusta o tamanho dos rótulos no eixo X\n    axis.text.y = element_text(size = 10))\n\n\n\n\n\n\n\n\nPodemos analisar mais de uma variável juntamente no gráfico acima. Vamos ver a Taxa de Feminicídio e a Tentativa de Feminicídio\n\n ggplot(final_fem_22) +\n  geom_density(aes(x = feminic_tx), fill = \"#7185cc\", color = \"#7185cc\", alpha = 0.6) +     # Curva de densidade e preenchimento\n  geom_density(aes(x = t_feminic_tx), fill = \"#c2986f\", color = \"#c2986f\", alpha = 0.6) +   # Curva de densidade e preenchimento\n  scale_x_continuous(limits = c(0, 15)) +                                                   # Limita o eixo X entre 0 e 10  \n  labs(\n    x = \"Taxa de Feminicídio\",                                                             # Título do eixo X, Y e Gráfico\n    y = \"Densidade\",  \n    title = \"Densidade de Kernel para a Taxa de Feminicídio\",\n    fill = \"Tipo\" \n  ) +\n  theme_minimal()  +\n theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),              # Centraliza o título\n    axis.text.x = element_text(size = 12),                                         # Ajusta o tamanho dos rótulos no eixo X\n    axis.text.y = element_text(size = 10)\n    )\n\n\n\n\n\n\n\n\nOutra forma útil de visualizar os dados a é distribuição por classe, por exemplo distribuição de salários entre homens e mulheres, distribuição do tempo do processo por vara, distribuição da taxa de feminicídio por região. Vamos utilizar a densidade de Kernel para analisar a distribuição dos valores da taxa de feminicídio por região. Para isso precisa instalar o pacote sm.\n\nggplot(final_fem_22, aes(x = feminic_tx, fill = factor(N_NE_CO))) + # Curvas de densidade para as duas regiões\n  geom_density(alpha = 0.6, color = \"white\") +\n  scale_fill_manual(values = c(\"1\" = \"#7185cc\", \"0\" = \"#c2986f\"), \n                    labels = c(\"Norte, Nordeste, Centro-Oeste\", \"Sul e Sudeste\")) +\n  scale_x_continuous(limits = c(0, 5)) +\n  # Adicionar rótulos e título\n  labs(\n    x = \"Taxa de Feminicídio\", \n    y = \"Densidade\",  \n    title = \"Comparação da Taxa de Feminicídio entre Regiões\", \n    fill = \"Região\"\n  ) +\n  \n  # Ajustar o tema e a aparência do gráfico\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.text.x = element_text(size = 12),\n    axis.text.y = element_text(size = 10),\n    legend.position = \"top\",  # Posição da legenda\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercício\n\n\n\nPERGUNTA: Gostaria de ter uma distribuição das Tentativa de Feminicídio por Estado mostrando os quartis, mediana e média.\nDica: Utlizar a densidade e o boxplot?\n\n\n\n\n\n\n\n\nVeja a Resposta\n\n\n\n\n\nRESPOSTA: Aqui segue uma sugestão de plotar a densidade juntamente com o boxplot, possibilitando em um mesmo gráfico uma maior quantidade de observação. Veja abaixo:\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Criar o histograma com ggplot2\ndens_t_f &lt;- ggplot(final_fem_22, aes(x = t_feminic_tx)) +\n  geom_density(fill = \"#7185cc\", color = \"darkblue\", alpha = 0.6) +\n  geom_vline(aes(xintercept = mean(t_feminic_tx, na.rm=TRUE)), color = \"#c2986f\", size = 1, alpha = 0.7,linetype = \"dashed\") +  \n  scale_x_continuous(limits = c(0, 15)) +\n  labs(x = \"Taxa de Feminicídio\", y = \"Frequência\") +\n  theme_minimal()  +\n  theme(\n    panel.background = element_blank(),  # Remove o fundo do gráfico\n    plot.background = element_blank(),   # Remove o fundo do gráfico\n    axis.line = element_blank(),         # Remove as linhas dos eixos X e Y\n    axis.ticks = element_blank()        # Remove os ticks dos eixos X e Y\n  )\n\n\n# Criar o boxplot com ggplot2\nbp_t_f &lt;- ggplot(final_fem_22, aes(x = feminic_tx)) +\n  geom_boxplot(fill = \"#c2986f\", color = \"darkblue\", outlier.shape = 16, outlier.size = 3) +\n  labs(x = NULL, y = NULL) +\n  theme(\n    panel.background = element_blank(),  # Remove o fundo do painel\n    plot.background = element_blank(),   # Remove o fundo do gráfico\n    axis.line = element_blank(),         # Remove as linhas dos eixos X e Y\n    axis.ticks = element_blank(),        # Remove os ticks dos eixos X e Y\n    axis.text = element_blank() )         # Remove os textos dos eixos X e Y\n  \n\n# Organizar os gráficos usando grid.arrange\ngrid.arrange( bp_t_f, dens_t_f,\n             ncol = 1, heights = c(1, 4))",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise Descritiva</span>"
    ]
  },
  {
    "objectID": "descritiva_v2.html#medidas-de-associação",
    "href": "descritiva_v2.html#medidas-de-associação",
    "title": "7  Análise Descritiva",
    "section": "7.5 Medidas de Associação",
    "text": "7.5 Medidas de Associação\nCOVARIÂNCIA AMOSTRAL\nCovariância entre duas variáveis: A covariância entre duas variáveis X e Y é uma medida estatística que descreve como essas variáveis variam juntas. Em outras palavras, a covariância indica a tendência de X e Y de se moverem na mesma direção (covariância positiva) ou em direções opostas (covariância negativa).\n\\[\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\\]\n\n cov(final_fem_22$feminic_tx, final_fem_22$homic_tx)\n\n[1] 0.4648575\n\n\nCORRELAÇÃO\nO coeficiente de correlação entre duas variáveis X e Y é uma medida estatística que descreve a força e a direção da relação linear entre essas variáveis. O coeficiente de correlação é frequentemente representado pelo coeficiente de correlação de Pearson, \\(r_{XY}\\).\n\\[r_{XY} = \\frac{\\text{Cov}(X, Y)}{s_X s_Y}\\] O coeficiente de correlação de Pearson é uma medida amplamente utilizada para avaliar a relação linear entre variáveis, pois fornece uma interpretação padronizada da força e direção da relação, independentemente das unidades das variáveis.\n\n cor(final_fem_22$feminic_tx, final_fem_22$homic_tx)\n\n[1] 0.3589068\n\n\n\n7.5.0.1 Visualizando a Associação\nScatter Plot\nO Scatter Plot é conhecido como o gráfico de dispersão. Ele relaciona duas ou três variáveis, ou seja, plota \\(X\\) contra \\(Y\\). Muito utilizado para ver o comportamento conjunto de duas séries.\nComo já visto que renda pc e a taxa de feminicídio não mostram um comportamento conjunto:\n\nggplot(data = final_fem_22, aes(x = rendapc, y = feminic_tx))+\n  geom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\n  scale_y_continuous(limits = c(0, 5)) +\n  geom_smooth(method = \"lm\", se=TRUE , color=\"orange\")+\n  labs(title=\"Renda pc e taxa de Feminicídios por Estado\", x=\"Renda pc\", y=\"Tx 100 mil\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nAo observar o homicídio absoluto e o feminicídio absoluto, estados com maiores números de homicídio tendem a ter maior número de feminicídios.\n\n# Primeiro vou fzer a correlação\ncorrel1&lt;-cor(final_fem_22$part_feminic, final_fem_22$homic_tx)\n\n#Depois montamos o Gráfico e anotamos a correlação no gráfico\nggplot(data = final_fem_22, aes(x = part_feminic, y = homic_tx))+\n  geom_point(color=\"#6e94bd\", size=3, alpha=0.7,  shape=17) +\n  geom_smooth(method = \"lm\", se=TRUE , color=\"orange\")+\n  labs(title=\"Taxa de Homicídio por 100 mil mulheres e a Participação do Feminicídio\",\n       x=\"Participação do Feminicídio\", \n       y=\"Taxa de Homicídio\")+\n  annotate(\n    \"text\", \n    x=40, y=9,\n    label = paste(\"Correlação: \", round(correl1, 2)), \n    color = \"#6e94bd\", size = 4, hjust = 0\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nCorrelograma\nCorrelograma é uma maneira de analisar todas as correlações de umaúnca vez a partir de uma matriz.\n\nlibrary(corrgram)\ncorrel&lt;- final_fem_22[c(5,7,8,9,13,15,16)]\ncorrgram(correl, order=TRUE, lower.panel=panel.shade, upper.panel=panel.cor,  main=\"Correlação entre as diversas variáveis\") \n\n\n\n\n\n\n\n\n\n#install.packages(\"ggcorrplot\")\nlibrary(ggcorrplot)\ncorrel2&lt;- round(cor(final_fem_22[c(5:9, 12)]),2)\nggcorrplot(correl2, hc.order = TRUE, type = \"lower\",        # Matriz de correlação, coloca de forma ordenada e inferior\n           lab = TRUE, lab_size = 3, lab_col = \"#616161\",   #  mostra os valores tamanho 4\n           outline.col = \"white\",\n           ggtheme = ggplot2::theme_minimal,\n           colors = c(\"#6D9EC1\", \"white\", \"#c2986f\"))+     # cores utilizadas\n# Colocando nomes  \n  labs(title = \"Correlação entre as Diversas Variáveis\")+   # Título\n#Trocando os nomes das variáveis\n  scale_y_discrete(labels = c(\"Feminic. Abs.\", \"Part. Feminic.\", \"Renda pc\",  # Nomes no eixo X\n                              \"Taxa Homic.\", \"Homic. Abs.\")) +  \n  scale_x_discrete(labels = c(\"Part. Feminic.\", \"Renda pc\", \n                              \"Taxa Homic.\", \"Homic. Abs.\", \"Taxa Feminic.\"))  # Nomes no eixo Y\n\n\n\n\n\n\n\n\nSalvando nosso banco para a próxima seção\n\nsave(final_fem_22, file = \"final_fem_22.RData\")",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Análise Descritiva</span>"
    ]
  },
  {
    "objectID": "estatistica.html",
    "href": "estatistica.html",
    "title": "8  Uma Introdução à Estatística",
    "section": "",
    "text": "8.1 Porque Estudar Estatística?\nPodemos dizer que a existência da estatística e de outras ciências está conectada a existência de problemas. Não somente a ciência mas o nosso trabalho está conectado a superação de problemas cotidianos. Tomar decisão é o dia a dia do gestor.\nSegundo Popper “we study not disciplines, but problems. Often, problems transcend the boundaries of a particular discipline”\nA questão central é: Como solucionamos os problemas? Utilizamos a melhor estratégia? A solução foi boa?\nflowchart LR\n  A[PROBLEMA] --&gt; B[Estratégia de Decisão]\n  B --&gt; C{Solução é BOA?}",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Uma Introdução à Estatística</span>"
    ]
  },
  {
    "objectID": "estatistica.html#porque-estudar-estatística",
    "href": "estatistica.html#porque-estudar-estatística",
    "title": "8  Uma Introdução à Estatística",
    "section": "",
    "text": "8.1.1 Os dois sistemas cognitivos\nOs livros abaixo são boas referências sobre a tomada de decisão.\n\n\n\n\n\n\nKahneman: Rápido e Devagar- Duas Formas de Pensar\n\n\n\n\n \n\n\n\n\n\nBazerman e Moore: Processo Decisório\n\n\n\n\n\nExistem dois sistemas que utilizamos para tomar decisão. O chamado Sistema 1 e o chamado Sistema 2. Segue uma breve descrição de cada um:\nSistema 1:\n\nIntuitivo, rápido, automático, sem esforço, implícito e emocional\n\nPressa,\nFalta de tempo,\nProblemas menos importante\nMais Falhas/Erros\n\n\nSistema 2\n\nRaciocíonio lento, consciente, esforçado, explícito, lógico\n\nRequer tempo,\nMais recursos\nProblemas mais importante\nMenos Falhas\n\n\nPara o Sistema 1 usamos a nossa intuição que chamamos de Heurística. Vejamos um pouco mais sobre esse sistema.\nHEURÍSTICA\nSão rotinas inconscientes ou atalhos que o nosso cérebro utiliza para lidar com a complexidade.\n\nModelo/Regras Intuitivas.\nPróprio do Sistema 1.\nApesar de processo sofisticado, são passíveis de falhas. Intuição falha\n\nUm Exemplo\nVeja a figura abaixo retirada do livro do Bazerman.Responda rápido.\nQual delas tem o tampo mais quadrado?\n\n\n\nBazerman: Exemplo das mesas\n\n\nSe você achou que é a segunda mesa, você está alinhado com a grande maioria. Nesse caso você usou o seu sistema 1\nVamos repitir a pergunta:\nQual delas tem o tampo mais quadrado?\nAgora use uma régua para medir as mesas. Usamos aqui o sistema 2. Mais tempo e recursos são utlizados. Qual mesa agora você considera mais quadrada? Mudou sua opinião?\nCom a régua vemos que as mesas são iguais. Isso mostra que a nossa intuição FALHA.\nTipos de Heurísticas\n\nHeurística da disponibilidade: Usamos o que está mais próximo na memória para calcular a probabilidade.\nHeurística da representatividade: Buscamos aquilo que reforça o padrão.\nHeurística da hipótese positiva: Assumimos que uma determinada hipótese é verdadeira e não olhamos o contrafactual.\nHeurística do afeto: Decisão considera o emocional. Seu humor afetam as decisões.\n\nPara contornar os problemas da intuição e seus viéses na tomada de decisão o primeiro passo é compreender que eles existem e estarmos alerta. E para problemas maiores o uso do sistema 2 torna-se relevante.\nUma das principais ferramentas do sistema 2 é a Estatística. Com os avanços computacionais essa ciência tem se destacado como um dos elementos centrais do data science. Abaixo a figura resume as diversas áreas de desenvolvimento da análise de dados, obviamente não exaustiva:\n\n\n\nFluxograma da Análise de Dados\n\n\nNosso objetivo é explorar nessa seção a análise descritiva. Chamado hoje no Business Intelligence, que e uma das áreas do Data Science.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Uma Introdução à Estatística</span>"
    ]
  },
  {
    "objectID": "estatistica.html#conceitos-básicos-de-estatística",
    "href": "estatistica.html#conceitos-básicos-de-estatística",
    "title": "8  Uma Introdução à Estatística",
    "section": "8.2 Conceitos Básicos de Estatística",
    "text": "8.2 Conceitos Básicos de Estatística\nNovamente começamos com um problema e esse definirá a nossa análise. Vejamos alguns problemas que poderiam nos interessar…\n\n\n\n\n\n\nProblema 1\n\n\n\nO prefeito de Ribeirão Preto vai lançar uma política que fornece vouchers de alimentação para mulheres que estão em situação de pobreza.\nProblema: Qual o valor que devo reservar ao programa? Quantas mulheres serão atendidas?\n\n\n\n\n\n\n\n\nProblema 2\n\n\n\nO TJSP vai lançar um programa para reduzir o tempo médio em processos de feminicídio.\nProblema: Qual o tempo médio de um processo de feminicídio?\n\n\n\n\n\n\n\n\nProblema 3\n\n\n\nO O governo federal vai lançar um programa para capacitar mulheres que estão fora do mercado de trabalho.\nProblema: Quantas mulheres serão alvos dessa política?\n\n\n\n8.2.1 A Variável Aleatória\nO problema nos define a população que estou interessado. Vamos seguir, a princípio, com o nosso probelma 1 para definirmos alguns conceitos importantes.\nNo problema 1: me interessa compreender a renda das mulheres que moram em Ribeirão Preto em dado ano. Para ficar simples vamos abreviar o que nos interessa\n\\[X=\\text{Renda das mulheres que moram em Ribeirão Preto em determinado ano}\\] Agora posso utilizar o X no lugar do nome. Olhando para a população e pensando que cada nível de renda pode ser representada por uma cor, teremos a seguinte imagem pouco de como a renda se distribui nessa população:\n\n\n\nRenda pc das mulheres de Ribeirão Preto.\n\n\nA questão é: quais cores existem e quantas peças de cada cor temos? Para isso usamos um experimento\nEXPERIMENTO ALEATÓRIO\nO experimento em ciências sociais aplicadas em geral está associada a observação sistemática de pessoas, cidades, empresas ou processos. A ideia é:\n\nSortear pessoas e observar a sua caracteristica de forma indefinida e sempre na mesma condição.\n\nNão consigo dizer o que vai sair no próximo sorteio, apenas consigo descrever os resultados possíveis\nSe repetir o experimento um número grande de vezes uma regularidade aparece.\n\nSe eu conseguir sortear de forma indefinida e na mesma condição as mulheres que moram em Ribeirão Preto e perguntar sobre a sua renda. Eu consigo reorganizar a figura acima da seguinte forma:\n\n\n\nRenda pc das mulheres de Ribeirão Preto.\n\n\nESPAÇO AMOSTRAL\nAgora conseguimos organizar os nossos resultados em um lugar chamado espaço amostral. Nele teremos todas as cores (azul, branca, amarela…) que podem acontecer e o número de peças de cada cor (a chance). Em outras palavras teremos todos os possíveis valores de \\(X\\) e suas probabilidades.\nVARIÁVEL ALEATÓRIA\nQuanto representamos esse espaço amostral em formato de números é o que chamamos de Variável Aleatória (V.A.). A V.A. é a combinação de tudo que pode acontecer, ou seja, todas as rendas que existem associadas a probabilidade de cada uma das rendas acontecerem.\nExistem dois tipos principais de variáveis aleatórias: discretas e contínuas.\nVARIÁVEIS ALEATÓRIAS DISCRETAS\nÉ um tipo de variável que conseguimos colocar em lista, seja finita ou infinita \\(x_1; x_2;...; x_n;...\\) e associa-se a cada um dessses valores uma probabilidade \\(p(x_1); p(x_2);...; p(x_n);...\\)\nPodemos pensar aqui se a pessoa é casada, solteira, divorciada, viúva ou outra condição. Se no processo classificamos como homicídio ou feminicídio, se mora na área urbana ou rural…\nNa figura abaixo iremos coletar de 20 processos de homicídio e gostariamos de saber quando é classificado como feminicídio e quanto é classificado como homicídio (p=0,3).\n\nfeminicidio &lt;- 0:20\n\nplot(feminicidio,dbinom(feminicidio,size=20,prob=.3),\n     type='h',\n     main='Distribuição Binomial (n=20, p=0.3)',\n     ylab='Probabilidade',\n     xlab ='Feminicídio',\n     lwd=3)\n\n\n\n\nDistribuição Normal\n\n\n\n\nVARIÁVEIS ALEATÓRIAS CONTÍNUAS\nPor outro lado, uma variável aleatória contínua pode assumir infinito valores dentro de um intervalo específico. Agora temos infinitas possibilidades de resultados para \\(X\\) e agora associamos uma função \\(f(x)\\) que irá descrever o comportamento da probabilidade.\nPor exemplo, a altura de uma pessoa, a sua renda, a sua idade, o tempo que demora um processo.\nAbaixo temos uma representação de uma distriuição continua da renda das mulheres em Ribeirão Preto, chamada distribuição normal:\n\nrm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.\nx&lt;-seq(700,1300,1)\nfdnorm&lt;-dnorm(x = x, mean = 1000, sd=100)  \nfdanorm&lt;-pnorm(q = x, mean = 1000, sd=100)\ncurve(dnorm(x,1000,100),xlim=c(700,1300),main='',xaxt=\"n\",xlab=\"Renda pc Mulheres\", ylab=\"f(x)\",col=\"darkblue\",cex.axis=0.65, cex.lab=0.8) \naxis(1,at=c(900, 1000, 1100),labels =\n       c(\"-DP(X)\",\"E(x)\",\"DP(x)\"),cex.axis=0.65, cex.lab=0.8) \nlines(x=c(1000,1000),y=c(0,fdnorm[x==1000]),lty=2, col=\"black\") \nlines(x=c(1100,1100),y=c(0,fdnorm[x==1100]),lty=2, col=\"black\")\nlines(x=c(900,900),y=c(0,fdnorm[x==900]),lty=2, col=\"black\")\n\n\n\n\nDistribuição Normal\n\n\n\n\n\n8.2.1.1 Esperança e Variância\nO fomato das distribuições vistas dependem principalmente de dois parâmetros: A esperança que é uma medida de centralidade e a variância que é uma medida de dispersão.\nESPERANÇA - \\(E(X)\\)\nÉ uma medida de centralidade da variável aleatória. É definida como a média ponderada de todos os possíveis resultados de \\(X\\), onde os pesos são dados pelas probabilidades desses resultados ocorrerem. A esperança de \\(X\\), \\(E(X)\\), é calculada como:\n\\[E(X) = \\sum_{x} x \\cdot P(X = x)\\]\npara variáveis discretas.\n\\[E(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x)\\],\npara variáveis contínuas\nVARIÂNCIA POPULACIONAL - \\(Var(X)\\)\nA variância é uma medida que captura como os dados populacionais se dispersão em relação a sua média (ou esperança).\n\\[Var(X) =\\frac{1}{N} \\sum_{1}^{N} (X_i-E(X))^2\\] Ou podemos assim representar: \\[\\text{Var}(X) = E(X^2) - [E(X)]^2\\].\nDESVIO PADRÃO POPULACIONAL - \\(DP(X)\\)\nA variância é uma medida ao quadrado. Se estamos falando da renda seria uma medida da dispersão ao quadrado, ou seja, em \\(R\\$^{2}\\). Para retornar a unidade original usamos o desvio padrão que é:\n\\[DP(X)=\\sqrt{Var(X)}\\]\nVejamos o que acontece quando mudamos a esperança e o desvio padrão. No gráfico em azul temos a esperança igual a 10 e devio padrão de 2,5. No gráfico em vermelho temos esperança de 20 e desvio padrão de 10. E no grafico em verde temos esperança de 10 e desvio padrão de 1. Nota-se que quanto menor o desvio padrão mais concentrados são os valores que podem acontecer.\n\ncurve(dnorm(x,mean=10,sd=sqrt(2.5)),xlim=c(0,30),ylim=c(0.0,0.4),xaxs=\"i\",yaxs=\"i\",ylab=\"\", col=\"darkblue\") \npar(new=T)\ncurve(dnorm(x,mean=20,sd=sqrt(10)),xlim=c(0,30),ylim=c(0.0,0.4),xaxs=\"i\",yaxs=\"i\",ylab=\"\", col=\"darkred\") \npar(new=T)\ncurve(dnorm(x,mean=20,sd=sqrt(1)),xlim=c(0,30),ylim=c(0.0,0.4),xaxs=\"i\",yaxs=\"i\",ylab=\"\", col=\"darkgreen\") \n\n\n\n\nMédias e desvios distintos entre distribuições\n\n\n\n\n\n\n\n8.2.2 Variáveis Aleatórias Bidimensionais\nMuito provavelmente nos interessa observar mais de uma característica de um experimento. Por exemplo, não somente a renda das mulheres em Ribeirão Preto nos interessa, mas o seu consumo alimentar também julgamos importante para o projeto.\nPortanto, queremos observar duas características de forma simultânea das mulheres: sua renda e seu consumo alimentar. Ou seja, duas características simultaneamente do mesmo experimento \\(\\epsilon\\) que foi observar as mulheres no município.\nApesar de termos coletados duas informações, temos na realidade três informações. A informação da renda, a informação do consumo alimentar e a informação de como renda e consumo alimentar interagem.\nVISUALIZAÇÃO GRÁFICA\nVejamos agora um exemplo de variável aleaória bidimensional:\nNormal Bivariada:\nAbaixo tem-se uma variável aleatória \\((X,Y)\\) com distribuição normal bivariada com a esperança de \\(X\\) igual a 1, de \\(Y\\) igual a 0, o desvio-padrões iguais a 3 e 2 respectivamente. Aqui consideremaos a correlação de 1 (veremos mais a frente esse conceito)\n\nlibrary(mnormt)\n\n#Para tornar reproduzível\nset.seed(0)\n\n#cCriando a normal bivariada\nx     &lt;- seq(-3, 3, 0.1) \ny     &lt;- seq(-3, 3, 0.1)\nmu    &lt;- c(1, 0)\nsigma &lt;- matrix(c(3, 1, 1, 2), nrow=2)\nf     &lt;- function(x, y) dmnorm(cbind(x, y), mu, sigma)\nz     &lt;- outer(x, y, f)\n\n#Criando um gráfico de superfície\npersp(x, y, z, theta=-30, phi=25, expand=0.6, ticktype='detailed')\n\n\n\n\nNormal Bivariada\n\n\n\n\nSurge aqui um conceito importante que tenta medir como as características da população se relacionam - uma medida do relacionamento. Assim:\nO que acontece com o consumo de alimentos quando a renda das mulheres sobem?\n\n8.2.2.1 Covariância e Correlação\nDuas medidas que tentam mensurar o “grau de associação” linear entre X e Y são:\nCOVARIÂNCIA\n\\[Cov(X,Y)=E[(X-E(X))(Y-E(Y))]= E(X.Y)-E(X).E(Y)\\]\nEla mede a variabilidade conjunta de uma variável aleátoria multidimensional. Como no caso da variância, ela sofre do efeito das escalas de medidas. Para corrigir dividimos pelos desvios padrões. Surge dessa maneira a medida de correlação.\nCORRELAÇÃO\n\\[\\rho_{X,Y}=\\frac{E[(X-E(X))(Y-E(Y))]}{\\sqrt{Var(X)Var(Y)}}=\\frac{Cov(X,Y)}{DP(X).DP(Y)}\\]\n\n\n\n\n\n\nCorrelação\n\n\n\nA correlação mede o GRAU DE ASSOCIAÇÃO LINEAR. Associações não lineares não são capturadas pela correlação.\n\n\n\n\n\n\n\n\nLendo a Correlação\n\n\n\nA correlação \\(\\rho_{X,Y}\\) varia de -1 até 1. Sendo que:\n\\(\\rho\\) próximo a 1 e -1 indicam alto grau de linearidade e \\(\\rho\\) próximo a 0 indica ausência de relação linear - mas não diz nada sobre relações não-lineares.\n\n\nVISUALIZAÇÃO GRÁFICA\nVeja no gráfico abaixo que a variável 4 e 5 possuem correlação perfeita, igual a -1. E as variáveis 3 e 1 não possuem grau de associação linear, correlação próxima a 0.\n\n\nWarning: pacote 'psych' foi compilado no R versão 4.4.1\n\n\n\n\n\nGráfico de correlação para variáveis simuladas v1 a v5",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Uma Introdução à Estatística</span>"
    ]
  },
  {
    "objectID": "estatistica.html#conceitos-básicos-inferência-estatística",
    "href": "estatistica.html#conceitos-básicos-inferência-estatística",
    "title": "8  Uma Introdução à Estatística",
    "section": "8.3 Conceitos Básicos Inferência Estatística",
    "text": "8.3 Conceitos Básicos Inferência Estatística\nDado a nossa pergunta ou problema, gostariamos de saber as carcateística de uma população.\nEntretantom um processo de levantamento de informações é em geral caro e em muitas situações é destrutivo. Em ciências sociais estamos interessados em características de pessoas, empresas, municípios, estados, países etc. Não é destrutivo mas é uma coleta cara. Por exemplo, o Censo demográfico de 2010 custou R$ 1,3 bilhões, ou aproximadamente R$ 2,2 bi em reais de 2020. O valor é de aproximadamente R$ 35,00 por domicílio.\nDessa forma nosso objetivo aqui é:\n\n\n\n\n\n\nObjetivo\n\n\n\nA partir de uma amostra da população realizar inferência sobre toda a população\n\n\n\n8.3.1 Exemplos do príncipio no dia a dia\nPense nessas situações:\n\nPara medir a glicose muitos pacientes usam uma gota de sangue e um pequeno aparelho. A partir dele sabem quanto tem no corpo todo, basta uma gota para termos boa certeza de quanto é taxa de glicose!\nPara saber se a quantidade de sal está adequada em uma grande panela de arroz, basta uma pequena colher de chá para termos uma boa certeza!\nAbacaxis às vezes são vendidos em caminhões na rua. Quando paramos provamos e são doces. Compramos 4 por 10. Qual a certeza que esses que vc está levando estejam também doces? É diferente das situações anteriores?\n\nCom certeza vc deve ter pensado que essas situações tem grau de certeza variáveis. A diferença está em quão homogênea é a característica na população, o sal no arroz e a glicose no sangue devem ser muito bem distribuidas, ou seja, bem homogêneas. Já a doçura no abacaxi deve ter distribuição maior e provar apenas um abacaxi não nos dá uma ideia do todo.\nEsse é um erro muito comum, a partir de uma ou poucas observações dizer que o todo se comporta da mesma maneira, esse erro se agrava quando maior é a heterogeneidade!!!\n\n\n8.3.2 População, Amostra, Parâmetros e Estimadores\n\n\n8.3.3 População e amostra\n\n\n\n\n\nflowchart LR\n  A[POPULAÇÃO] --&gt; B[Totalidade das observações sob Investigação]\n  A --&gt; C[AMOSTRA]\n  C --&gt; D[Subconjunto da População]\n\n\n\n\n\n\nA definição da população depende da pergunta de pesquisa ou análise. Se queremos saber qual o salário médio dos empregados do setor industrial no estado de São Paulo para determinado ano, nossa população são todos os funcionários das indústrias instaladas no estado de São Paulo para esse ano. Se queremos os determinantes do desempenho escolar dos alunos do ensino fundamental no Brasil em 2019, nossa população será esse grupo de alunos nesse ano. Se quisermos avaliar o gasto municipal no ano anterior as eleições no Brasil, temos nossa população formada pelos municípios para o ano de análise.\n\n\n\n\n\n\nPopulação\n\n\n\nQuem define a população é o objetivo do seu trabalho!! Ou seja, seu problema de pesquisa\n\n\n\n\n8.3.4 Amostragem Aleatória Simples\nExistem várias maneiras de fazer uma análise aleatória, uma delas é a simples. Vejamos primeiro um processo de amostragem não aleatório e que possui tendenciosidade. A figura abaixo mostra esse processo[^7]:\n\n\n\nUm processo de amostragem viesádo. Fonte:Data Basecamp\n\n\nObserva-se que existe uma supervalorização do vermelho e uma subvalorização do azul. Chegariamos a conclusão, caso isso fosse uma pesquisa eleitoral, que o candidato vermelho, segunda amostra teria mais chance de ganhar e o azul quase nenhuma chance. O que não condiz com a população. Dizemos que temos uma amostra viesada ou tendenciosa.\nUm processo de amostragem aleatório requer que as características presentes na população estejam presentes na amostras e estejam balanceadas, ou seja, que a sua leitura represente bem o todo.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Uma Introdução à Estatística</span>"
    ]
  },
  {
    "objectID": "estatistica.html#estatística-e-parâmetro",
    "href": "estatistica.html#estatística-e-parâmetro",
    "title": "8  Uma Introdução à Estatística",
    "section": "8.4 Estatística e Parâmetro",
    "text": "8.4 Estatística e Parâmetro\n\n\n\n\n\nflowchart LR\n  A[PARÂMETRO] --&gt; B[Medida que descreve uma característica da população]\n\n\n\n\n\n\nOs parâmetros definem as características de uma população. Qual a renda média da população, qual o desemprego médio da população, qual o desempenho médio educacional, qual a expectativa de vida média na população etc. São características que em geral não observamos.\nUma pergunta, qual o tempo médio que demora um processo de feminicídio? Perceba que mesmo características da população que conhecemos são de difíceis de conhecermos. Temos que nos valer de uma parte e tentar estimar o que seriam os valores dessas características.\n\n\n\n\n\nflowchart LR\n  A[ESTATÍSTICA] --&gt; B[Medida que descreve uma característica da amostra]\n\n\n\n\n\n\nSejam \\(x_1, x_2,..., x_{n}\\) os valores medidos a cada para cada medição de \\(X\\). Podemos definir uma estatística como:\n\\[ t= H (x_1, x_2, ..., x_{n})\\]\nAlguns exemplos de T:\n\\[ \\text{Média}: \\ \\overline{x}=\\frac{\\sum_{i=1}^{n} x_i}{n}\\]\n\\[ \\text{Variância:} \\ s^{2}= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\overline{x})^{2} \\]\n\\[ x_{(1)}: Min\\{x_1, ..., x_n\\}\\]\nVejamos a tabela abaixo que já faz uma primeira associação entre estatística e parâmetro:\n\n\n\n\n\n\n\n\n\nParâmetro\n\nEstatística\n\n\n\n\n\nEsperança\n\\(E(X)=\\mu\\)\n\\(\\bar{X}\\)\nMédia\n\n\nVariância Pop.\n\\(Var(X)=\\sigma^2\\)\n\\(S^2;\\sigma^2\\)\nVariância Amostral\n\n\nMediana Pop.\nMd\nmd\nMediana Amostral\n\n\nProporção Pop.\np\n\\(\\hat{p}\\)\nProporção Amostral\n\n\n\nTabela 1 - Parâmetros populacionais e as Estatísticas associadas\nComo regra geral, os parâmetros são representados por letras gregas e as estatística com letras do nosso alfabeto (latino) ou letra grega com com chapéu para indicar que é uma estatística.\nESTIMADORES Um estimador é uma estatística calculada a partir da amostra que é usada para estimar um parâmetro desconhecido da população. Nos permitem fazer inferências sobre os parâmetros com base nos dados amostrais. Por exemplo, a média amostral é um estimador da média populacional, e a proporção amostral é um estimador da proporção populacional.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Uma Introdução à Estatística</span>"
    ]
  },
  {
    "objectID": "estatistica.html#teste-de-hipotese-parâmetros.",
    "href": "estatistica.html#teste-de-hipotese-parâmetros.",
    "title": "8  Uma Introdução à Estatística",
    "section": "8.5 Teste de Hipotese (Parâmetros).",
    "text": "8.5 Teste de Hipotese (Parâmetros).\nAlgumas vezes gostariamos de testar se uma teoria é verossímil com a realidade ou mesmo testar teorias diferentes e verificar qual seria a mais plausível com base na realidade.\nDessa forma, gostariamos de testar se uma sobre a população é mais plausível, ou seja, se os dados amostrais trazem evidências que apoiam ou não essa hipótese. Por exemplo:\n\nVerificar se um determinado medicamento não tem efeito sobre a mortalidade causada por um determinado virus ou se possui efeito.\nVerificar se a quantidade de gordura anunciado pelo fabricante de um produto realmente está correta ou é maior.\nSe a afirmativa de um canditado de que possui a maioria dos votos é verdadeira ou é menor.\nVerificar se as rendas entre duas comunidades são as mesmas para podermos lançar uma política de apoio\nVerifcar se uma política do aumento do recurso as empresas não afeta falência ou se tem efeito.\nse o tempo médio de encaminhamento e deferimento de medida protetiva de urgência é menor do que 48 horas em casos de violência doméstica e familiar contra a mulher.\n\nAqui faremos algo muito parecido a presunção de inocência, assumimos que a pessoa ou empresa é “inocente”.\n\nO medicamento não tem efeito,\nO teor de gordura está certo,\nO candidato tem maioria,\nAs comunidades possuem a mesma renda,\nO recurso financeiro não afeta o número de falências\nO deferimento ocorre em até 48h.\n\n\n\n\n\n\n\nA Intuição\n\n\n\nPartimos da premissa de que a hipótese inicial é a correta e tentamos verificar com os fatos (dados amostrais) se essa hipótese colocada é verossímil.\n\n\n\n8.5.1 Construíndo a Hipótese Nula\nImaginemos o seguinte caso. Um estudo sobre a eficiência do Judiciário sugere que a duração média de um determinado tipo de processo na Justiça Federal é de 600 dias. No entanto, advogados e operadores do direito argumentam que, na prática, esse tempo pode ser maior devido a atrasos processuais e recursos frequentes. A questão é: a estimativa oficial está correta ou os operadores do direito têm razão?\nDessa forma, temos duas hipóteses distintas:\n\nA primeira afirma que o tempo médio de duração do processo, \\(\\mu\\), é de 600 dias,\nA segunda sugere que a duração real é superior a 600 dias.\n\nVamos assumir que a estimativa oficial está correta até que se prove o contrário, e chamaremos essa afirmativa de hipótese nula (\\(H_0\\)):\n\\[H_0: \\quad \\mu = 600 \\text{ dias}\\]\nJá a hipótese alternativa (\\(H_1\\)), que representa a teoria concorrente dos operadores do direito, sugere que a média real de duração do processo é maior:\n\\[ H_1: \\quad \\mu &gt; 600 \\text{ dias}\\]\nNosso problema, então, é decidir se devemos aceitar ou rejeitar a hipótese nula \\(H_0\\) — de que o tempo médio é de 600 dias — em favor da hipótese alternativa \\(H_1\\), que afirma que os processos, na realidade, levam mais tempo para serem concluídos.\nJuntas:\n\\[H_0: \\quad \\mu = 600\\] \\[H_1: \\quad \\mu &gt; 600\\]\nEsse modelo pode ser aplicado utilizando dados reais de processos judiciais para verificar se a alegação dos operadores do direito se sustenta estatisticamente.\n\n\n8.5.2 O Teste Estatístico\nQual dessas duas hipóteses é mais plausível?\nPara isso devemos nos valer de um processo de amostragem, onde faremos \\(n\\) medições do tempo médio dos processos (que chamaremos de X), \\(X_1, X_2, ..., X_n\\), e obteremos os valores em dias do tempo médio de cada processo amostrado \\(x_1, x_2,...x_n\\).\nCom base na amostra devemos realizar algum tipo de cálculo que nos permite inferir se rejeitamos ou não \\(H_0\\), se é plausível ou não a hipótese colocada. Isso é o que chamamos de teste estatístico:\n\\[T = h(X_1,X_2, . . .,X_n)\\]\nDecidindo qual o \\(T\\) utilizar - a função \\(h\\) que será aplicado aos valores da amostra - devemos compreender qual é a distribuição dessa estatística sob a condição de que a hipótese \\(H_0\\) for a verdadeira.\n\n\n\n\n\n\nA Intuição\n\n\n\nQueremos aqui saber se a amostra tivesse sido extraída de contratos com esperança do tempo de duração , \\(\\mathbb{E(X)}\\), de 600 dias, quais seriam os valores típicos para a distribuição do estimador T? Dessa forma, podemos comparar esses valores típicos com o que obtivemos no processo de amostragem.\n\n\nVejamos no nosso exemplo, gostariamos de verificar a hipótese de que a esperança do tempo médio, \\(\\mu\\) é de 600 dias. Como já vimos uma boa alternativa de teste estatístico poderia ser a média, \\(\\bar{X}\\). Assim o teste estatístico seria:\n\\[\\bar{X} = \\frac{\\sum_i^n(X_i)}{n}\\]\nCom base na amostra observada \\(x_1, x_2, ..., x_3\\) poderiamos obter a estimativa do tempo médio, ou seja, \\(\\bar{x}\\). Como saber se essa média calculada nos traz mais evidência a favor de \\(H_0\\) ou \\(H_1\\)? Veja a Figura abaixo para pensarmos no problema.\nA figura considera o estimador \\(\\bar{X}\\). A esquerda temos os valores do estimador que atestam que a hipótese \\(H_0\\) é a mais plausível, quando mais próxima a estimativa de 600 maior evidência que \\(H_0\\) é verdadeira. Ao caminhar para a direita, os valores do estimador se distanciam de 600, e mais evidência de que \\(H_0\\) não é plausível.\nDessa forma, precisamos de um ponto no qual (interrogação na figura) onde valores menores do estimador são favoráveis a hipótese nula e valores maiores são mais favoráveis a hipótese alternativa. Por exemplo, se no nosso processo de amostragem obtivemos a estimativa de \\(\\bar{x}=700\\), isso é mais favorável a \\(H_0\\) ou \\(H_1\\)?\n\n\n\n\nTeste de Hipótese - intuição \n\n\n\nPara saber qual seriam os valores típicos do estimador \\(\\bar{X}\\) sob \\(H_0\\) imagine que a população \\(X\\) seja \\(N(600,100^2)\\), ou seja, tem esperança 600 e desvio padrão populacional de 100. Essa é a afirmação da Justiça Federal, ou seja, nosso \\(H_0\\).\nJá sabemos que um processo de amostragem cada uma das \\(n\\) medições \\(X_1, X_2, ...,X_n\\) possuem a mesma distribuição de \\(X\\). E também sabemos por definição que o estimador \\(\\bar{X}\\) terá uma distribuição:\n\\[N(600,\\frac{100^2}{n})\\].\nSupondo que retiramos uma amostra de 25 processos, logo os valores típicos do estimador sob \\(H_0\\) são \\(N(600,\\frac{100^2}{25})\\). Vejamos abaixo a simulação do estimador \\(\\bar{X}\\), os valores típicos para esse caso, e onde se encontra o valor de 700.\n\nx&lt;-seq(500,700,0.1) \nfdnorm&lt;-dnorm(x = x, mean = 600, sd=20)   \nregiao=seq(560,640,0.01)\ncord.x &lt;- c(min(regiao),regiao,max(regiao))\ncord.y &lt;- c(0,dnorm(regiao,mean=600, sd=20),0) \ncurve(dnorm(x,600,20),xlim=c(500,700),xlab=expression(bar(x)),type=\"l\",\n      col=\"steelblue4\",lwd=2, ylab=expression(paste(\"f(\", bar(x),\n      \")\")),xaxt=\"n\",cex.axis=0.65, cex.lab=0.8 ) \naxis(1,at=c(500,540,560,580, 600, 620, 640,660, 700),labels =\n       c(500,540,560,580, 600, 620, 640,660, 700),cex.axis=0.7, cex.lab=0.8) \npolygon(cord.x,cord.y,col='lightgray')\nabline(v=600, col=\"steelblue4\", lty=2, lwd=2)\ntext(602, 0.001, expression(mu))\n\n\n\n\nValores típicos da distribuiçao da média sob H0\n\n\n\n\nNo centro temos a \\(E(\\bar{X})=\\mu=600\\). Observamos que para o tamanho amostral que retiramos e sob \\(H_0\\) os valores típicos oscilam mais ou menos entre 560 e 640 - dois desvios padrão para cima e para baixo (lembrem-se que nesse intervalo temos mais de 95% das observaçoes).\nQuanto retiramos a amostra e calculamos o valor da média obtivemos \\(\\bar{x}=700\\). Observe no gráfico acima onde está o valor de 700, muita a frente e notamos claramente que a probabilidade de obtermos esse valor de média com uma amostra retirada da população \\(N(600,100^2)\\), é praticamente 0.\nPortanto, existem evidências de que essa amostra não veio de uma população conforme descrita pela Justiça Federal e sim de processos com tempo médio (esperança) maior do que 600. Portanto, dizemos que rejeitamos \\(H_0\\).\n\n\n8.5.3 Erro Tipo I (EI) e Erro Tipo II (EII)\nAqui precisamos distinguir duas ideias, a primeiro é a existência da verdadeira população e a segunda é o que achamos ser a verdadeira população com base na análise que fizemos. Aqui surge o que chamamos de erro estatístico. Não temos como fugir dele, somente controlá-lo. Vejamos a tabela abaixo que resume as possibilidades:\n\n\nTipos de Erros em Estatística, EI e EII.\n\n\nA Decisão\n(\\(H_0\\)) é verdadeiro\n(\\(H_1\\)) é verdadeiro\n\n\n\n\nRejeitar (\\(H_0\\))\nErro Tipo I (EI)\nCorreto\n\n\nNão Rejeitar (\\(H_0\\))\nCorreto\nErro Tipo II (EII)\n\n\n\n\nObserve que a nossa decisão pode incorrer em dois erros diversos.\n\n\n\n\n\n\nErro Tipo I e Erro Tipo II\n\n\n\nErro Tipo I e Erro Tipo II\nErro Tipo I (EI): ocorre quando “indevidamente” rejeitamemos \\(H_0\\). Nesse caso \\(H_0\\) era verdadeira e rejeitamos.\\\nErro Tipo II (EII): ocorre quando “indevidamente” não rejeitamos \\(H_0\\). Nesse caso não rejeitamos \\(H_0\\) e na verdade \\(H_1\\) é verdadeira.\n\n\nO primeiro erro é o chamado na literatura médica de falso negativo, ou seja, classifica a pessoa não portadora da doença (negativa) e na verdade ela possui.\nO segundo tipo é o falso positivo, onde classifica-se a pessoa com a doença quando na realidade ela não possui.\nNosso desafio agora é estabelecer um critério de decisão, o ponto a partir do qual dizemos que \\(H_0\\) não parece mais provável (Figura Teste de Hipótese - Intuição). Essa chamaremos de região crítica ou de rejeição.\n\nEI \\((\\alpha)\\)- Dizer que o tempo médio é maior que 600, quando na realizadade ela é de 600.\nEII \\((\\beta)\\)- Dizer que tempo médio é de 600 quando na realidade ela é maior do que 600.\n\nVamos retomar o nosso exemplo. Foi retirada uma amostra de \\(n=25\\) processos e por definição a distribuição de \\(\\bar{X}\\) sob \\(H_0\\) será \\(N(600,\\frac{100^2}{25})\\). A hipótese a ser testada será:\n\\[H_0: \\qquad \\mu=600\\] \\[H_1: \\qquad \\mu&gt;600\\]\nUma maneira de acharmos o valor a partir do qual teremos a região crítica ou de rejeição, seria controlar o Erro Tipo I \\((\\alpha)\\). Podemos dizer que gostariamos de cometer o Erro Tipo I em apenas 5% dos casos.\n\n\n\n\n\n\nErro Estatístico\n\n\n\nA chance de retirarmos um amostra e o valor da estimativa ser maior que o valor de decisão é de 5% dos casos, os outros 95% sempre cairão na área de aceitação.\n\n\n\\[P(Z_{\\bar{X}} \\geq z_c|H_0)=0.05= \\alpha\\] Olhando a tabela temos:\n\\[P(Z_{\\bar{X}} \\geq 1.65|H_0)=0.05= \\alpha\\] \\[z_c =\\frac{\\bar{X}+600}{100/5}\\] \\[1.65 = \\frac{\\bar{X}+600}{20}\\] \\[\\bar{X}=600+1.65*20=633\\]\nPortanto,\n\\[P(\\bar{X}\\geq 633|H_0)=0.05= \\alpha\\]\n\n\n\n\n\n\nTabela Normal\n\n\n\n\n\n\n\n\n\nTabela da Norma Padrão \n\n\n\n\n\n\nLogo, temos agora uma regra de decisão que tenta controlar o Erro Tipo I. A nossa regra de decisão agora é rejeitar \\(H_0\\) toda vez que o valor calculado da estimativa de \\(\\bar{X}\\) for maior do que 633 e aceitar quando for menor. Assim nossa região crítica será:\n\\[RC =\\{\\bar{x} \\in \\mathbb{R} | \\bar{x} \\geq 633\\}\\]\nIsso implica que a probabilidade de rejeitarmos \\(H_0\\) (de que o tempomédio não é de 600), e na verdade ela ser de 600 é de 5%.\nVejamos o gráfico:\n\nx&lt;-seq(500,700,0.1) \nfdnorm&lt;-dnorm(x = x, mean = 600, sd=20)  \nfdnorm1&lt;-dnorm(x = x, mean = 660, sd=20)\nregiao=seq(633,700,0.01)\ncord.x &lt;- c(min(regiao),regiao,max(regiao))\ncord.y &lt;- c(0,dnorm(regiao,mean=600, sd=20),0) \ncurve(dnorm(x,600,20),xlim=c(500,700),xlab=expression(bar(x)),type=\"l\",\n      col=\"steelblue4\",lwd=2, ylab=expression(paste(\"f(\", bar(x),\n      \")\")),xaxt=\"n\",cex.axis=0.65, cex.lab=0.8 ) \naxis(1,at=c(500,540,560,580, 600, 620, 633,660, 700),labels =\n       c(500,540,560,580, 600, 620, 633,660, 700),cex.axis=0.7, cex.lab=0.8) \npolygon(cord.x,cord.y,col='wheat4')\nabline(v=633, col=\"steelblue4\", lty=2, lwd=2)\ntext(600, 0.001, expression(mu))\ntext(660, 0.005, expression(paste(\"EI=\", alpha, \"=0.05\")))\n\n\n\n\nErro Tipo I e a Região Crítica\n\n\n\n\nEm cinza tem-se a região crítica descrita acima. Logo todos os valores calculados de \\(\\bar{X}\\) que cairem acima de 633, dizemos que rejeitamos \\(H_0\\). Entretanto, percebam que poderiam fazer parte desta distribuição, apesar da chance ser pequena, 5%.\nComo não sabemos a distribuição sob \\(H_1\\) não conseguimos calcular a probabilidade de não rejeitar \\(H_0\\) e na verdade ela pertencer a distribuição de \\(H_1\\).\n\n\n8.5.4 Procedimento Geral do Teste de Hipótese\n\n8.5.4.1 Teste para um parâmetro populacional\nTemos interesse em uma característica da população. Como vimos por exemplo, o tempo médio de um processo \\(X\\), ou mais especificamente na sua esperança \\(E(X)=\\mu\\). Contruímos o teste sobre o parâmetro, podendo ser unicaudal ou bicaudal.\nHipótese Bicaudal:\nO teste bilateral ou bicaudal podemos observar valores maiores ou menores em relação a hipótese nula. Assim não temos nenhum conhecimento que nos permita dizer que podemos ter valores somente maiores ou somente menores. Temos a seguinte formulação geral:\n\\[H_0: \\qquad \\theta=\\theta_0\\]\n\\[H_1: \\qquad \\theta \\neq \\theta_0\\]\nNível de significância\nRetomando o nosso exemplo, ao rejeitarmos \\(H_0\\) podemos cometer o erro de dizer que o tempo médio é maior que 600 dias, mas na realidade o tempo era efetivamente 600 dias.\nTentamos controlar esse tipo de erro que é o nosso Erro Tipo I (EI). Temos que definir qual seria o tamanho desse erro, 10%, 5%, 1% etc. Esse percentual é o que chamamos de nível de significância. Quem define esse tamanho é o pesquisador e em geral, em ciência sociais, utilizamos os níveis acima.\n\n\n\n\n\n\nNível de Signficãncia\n\n\n\nNível de Significância:É a probabilidade máxima aceitável de cometer o erro tipo I e chamamos de \\(\\alpha\\), sendo um valor entre \\(0&lt;\\alpha&lt;1\\)\n\n\nDessa forma, faremos o teste de hipótese para o parâmetro \\(\\theta\\) ao nível de significância de \\(\\alpha\\). No nosso caso dizemos que iremos testar se o tempo médio dos processos é de 600, \\(H_0: \\mu=600\\), ao nível de 5% de significância.\nValor Crítico e Região Crítica\nCom base no nível de significância conseguiremos estabelecer qual é o valor crítico e qual seria a região de rejeição. Para o nosso caso encontramos o valor crítico de 633 e a nossa região foi estabelecida como \\(RC =\\{\\bar{x} \\in R | \\bar{x} \\geq 633\\}\\). Conforme calculamos anteiormente.\nAssim, de forma geral tem-se:\n\\[RC =\\{T \\in C| H_0\\}\\] \\[P(T \\in C|H_0)\\leq\\alpha\\]\n\n\n\n\n\n\nTeste Unilateral e Bilateral\n\n\n\nA região critica depende do teste estatístico escolhido e se a hipótese é unilateral, ou seja, apenas de um lado da distribuição ou bilateral, os dois lados da distribuição. No caso unilateral utilizamos o nível de siginifcância, \\(\\alpha\\), todo de um lado apenas. Se for o teste bilateral dividimos o nível de significância, ou seja, utilizamos\\(\\alpha /2\\), metade para cada lado.\n\n\nO Teste de Hipótese\nFazemos nosso processo de amostragem e obtemos o valor do teste estatístico. Se o valor do teste ficar fora da região crítica dizemos que não rejeitamos \\(H_0\\). Para o nosso caso, que não existe evidências de que o tempo médio dos processos é maior do 600 dias.\nCaso o teste estatítico produza uma estimativa na região crítica, rejeitamos \\(H_0\\), há evidências de que o tempo médio é maior do que aquela postulada pela Justiça Federal.\n\n\n\n8.5.5 Os Cinco passos para a contrução do teste de hipótese\n\nEstabeleça as hipótese nula \\(H_0\\) e a hipótese alternativa \\(H_1\\)\nDefina qual estimador do parâmetro populacional \\(\\theta\\) que será usado para testar \\(H_0\\): média, desvio padrão amostral, proporção amostral etc\nDefina o nível de significância - \\(\\alpha\\) e estabeleça qual o valor e a região crítica.\nCalcule a estimativa do teste estatístico.\nSe não pertencer a Região Crítica não rejeitamos \\(H_0\\), caso contrário rejeitamos a hipótese nula \\(H_0\\).\n\n\n\n8.5.6 Introdução ao Teste de Hipótese de Duas Populações (de Médias)\nO teste de hipótese é uma técnica estatística fundamental usada para tomar decisões baseadas em evidências amostrais. O teste de hipótese de duas populações é aplicado quando queremos comparar as médias de duas populações distintas e determinar se existe uma diferença estatisticamente significativa entre elas. Vamos explorar os principais conceitos deste teste:\nFormulação das Hipóteses\nNo teste de hipótese de duas populações, formulamos duas hipóteses:\n\nHipótese Nula (\\(H_0\\)): Esta é a hipótese inicial que assume que não há diferença entre as médias das duas populações. Geralmente, é representada como\n\n\\[H_0: \\mu_1 = \\mu_2\\],\nonde \\(\\mu_1\\) e \\(\\mu_2\\) são as médias das duas populações.\n\nHipótese Alternativa (\\(H_a\\) ou \\(H_1\\)): Esta é a hipótese que queremos testar, indicando que há uma diferença significativa entre as médias das duas populações. Pode ser definida como: \\[H_a: \\mu_1 &gt; \\mu_2\\] ou \\[H_a: \\mu_1 &lt; \\mu_2\\] ou\n\n\\[H_a: \\mu_1 \\neq \\mu_2\\].\nEstatística do Teste\nO teste de hipótese de duas populações geralmente envolve o cálculo de uma estatística de teste específica para comparar as médias das amostras das duas populações. Uma das estatísticas comuns é o teste t de Student, especialmente quando as variâncias populacionais são desconhecidas e podem ser diferentes entre as populações.\nDecisão do Teste\nApós calcular a estatística de teste, comparamos o valor observado da estatística com um valor crítico ou calculamos um valor p associado. O valor p é a probabilidade de obter uma estatística de teste tão extrema quanto a observada, assumindo que a hipótese nula seja verdadeira. Com base no valor p (geralmente comparado com um nível de significância pré-definido, como 0,05), tomamos uma decisão de rejeitar ou não rejeitar a hipótese nula.\nConclusão do Teste\nA conclusão do teste de hipótese de duas populações nos permite determinar se há evidências estatísticas suficientes para rejeitar a hipótese nula em favor da hipótese alternativa. Essa decisão tem implicações importantes em áreas como pesquisa científica, análise de dados e tomada de decisões em negócios e saúde.\n\n\n8.5.7 Aplicação Prática sobre Teste de Hipótese\nEssa seção tem o objetivo de aplicar o Teste de Hipótese utilizando o R. Vamos utilizar nosso banco de dados sobre feminicídio. E vamos criar uma variável que identifica se pertence as regiões norte, nordeste e centro oeste ou as regiões sul e sudeste.\n\n#carregando o pacote para ler arquivos em excel\nload(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.Rdata\")\n\n# Criando a Binária que indica a região N, NE e CO\nlibrary(dplyr)\n\nfinal_fem_22 &lt;- final_fem_22 %&gt;%\n  mutate(N_NE_CO = case_when(\n    regiao %in% c(\"N\", \"NE\", \"CO\") ~ 1,  # Regiões N, NE e CO recebem 1\n    TRUE ~ 0                             # Demais regiões recebem 0\n  ))\n\nVamos olhar as médias entra as regiões norte, nordeste e centro oeste e as regiões sul e sudeste.\n\nlibrary(kableExtra)\n\nmean_1&lt;- aggregate(final_fem_22[4:15], list(final_fem_22$N_NE_CO), mean,  na.rm=T)\n\nt(mean_1) %&gt;% \n  kbl(digits = 2) %&gt;%\n     kable_styling()\n\n\n\n\nGroup.1\n0.00\n1.00\n\n\nhomic_abs\n249.71\n108.80\n\n\nhomic_tx\n3.60\n5.18\n\n\nfeminic_abs\n107.57\n34.20\n\n\nfeminic_tx\n1.47\n1.72\n\n\npart_feminic\n42.84\n35.98\n\n\nrendapc\n1903.14\n1287.55\n\n\nmais_50\n0.29\n0.25\n\n\nt_homic_abs\n453.57\n224.25\n\n\nt_homic_tx\n8.91\n15.50\n\n\nt_feminic_abs\n177.67\n78.79\n\n\nt_feminic_tx\n3.25\n4.42\n\n\npart_t_feminic\n26.24\n27.09\n\n\n\n\n\nPode-se observar que existem em todas as variáveis existem diferenças entre as duas regiões. A questão é: Essas diferenças são estatisticamente significativas?\nAproveitando um gráfico anterior que fizemos, vamos olhar a distribuição das taxas de feminicídio entre as regiões.\n\nlibrary(ggplot2)\nggplot(final_fem_22, aes(x = factor(N_NE_CO, labels = c(\"Sul e Sudeste\", \"Norte, Nord. e C.Oeste\")),  # Transformando em fatores\n                         y = feminic_tx, fill = factor(N_NE_CO))) +                                   # Taxa de Feminicídio por Fator\n  geom_violin(trim = FALSE, color = \"white\") +                                                        # Cria o gráfico de violino\n  scale_fill_manual(values = c(\"lightblue\", \"steelblue\")) +                                           # Cor das violas\n  stat_summary(fun=\"median\", geom = \"point\", shape=19, size=3, color=\"darkorange\"  ) +                # Vamos colocar o ponto mediana\n  labs(\n    title = \"Distribuição da taxa de feminicídio por região do Brasil\",\n    x = \"Região\",\n    y = \"Taxa de Feminicídio (por 100 mil)\",\n    fill = \"Região\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),              # Centraliza o título\n    axis.text.x = element_text(size = 12),                                         # Ajusta o tamanho dos rótulos no eixo X\n    axis.text.y = element_text(size = 10),                                         # Ajusta o tamanho dos rótulos no eixo Y\n    legend.position = \"none\"  )\n\n\n\n\n\n\n\n\nObserva-se diferença entre as duas regiões, mas será que as médias são estatisticamente diferentes? Para isso temos que utilizar o Teste de Hipótese.\nRelembrando os Passos\nO teste de hipótese segue um processo estruturado de cinco etapas:\n\nDefinir as Hipóteses – Comece formulando a hipótese nula (\\(H_0\\)) e a hipótese alternativa (\\(H_1\\)). Em geral, a hipótese nula representa a ausência de efeito ou diferença, enquanto a hipótese alternativa sugere a existência de um efeito ou diferença.\nDefina o Estimador - Defina qual estimador vai utilizar para inferir sobre o parâmetro populacional \\(\\theta\\), o qual será usado para testar \\(H_0\\): média, desvio padrão amostral, proporção amostral etc\nEstabelecer os Critérios de Decisão – Defina o nível de significância (\\(\\alpha\\)), que representa a probabilidade de rejeitar a hipótese nula quando ela é verdadeira (Erro Tipo I). Valores comuns de (\\(\\alpha\\)) são 0,05 (5%) e 0,01 (1%).\nAnalisar os Dados da Amostra – Calcule o teste estatístico com base nos dados amostrais. Esse teste estatístico vai ser utilizado para a comparação com os valores típicos que poderiam acontecer se a amostra tivesse vindo de \\(H_0\\).\nTomar uma Decisão e Interpretar os Resultados – Compare o valor calculado no teste com o valor crítico ou utilize o p-valor. Se o p-valor for menor que o nível de significância \\(\\alpha\\), rejeita-se a hipótese nula em favor da hipótese alternativa. Caso contrário, não há evidências suficientes para rejeitar a hipótese nula, indicando que o efeito não é estatisticamente significativo.\n\nTeste de Hipótese no R\nA função t.test() no R possui vários argumentos importantes para a realização do teste t.\n\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE,\n       conf.level = 0.95, …)\n\nAbaixo estão os principais:\n\nx, y: As duas amostras de dados a serem comparadas.\n\nalternative: Define a hipótese alternativa do teste (exemplo: \"two.sided\", \"greater\", \"less\").\n\nmu: Especifica um valor de referência para a média populacional no teste.\n\npaired: Indica se o teste t deve ser pareado (TRUE) ou não pareado (FALSE).\n\nvar.equal: Especifica se as variâncias das amostras devem ser assumidas como iguais (TRUE) ou não (FALSE).\n\nconf.level: Define o nível de confiança do intervalo (padrão de 95%).\n\nEsses parâmetros permitem personalizar o teste conforme a necessidade da análise estatística.\nTeste sobre o Parâmetro\nUma reportagem em um portal de notícias indicou que a média da taxa de feminicídio nos estados brasileiros é igual a 2 assassinatos por 100 mil mulheres. Com base nos dados que coletamos gostaríamos de saber se eles carregam evidências favoráveis ou contra ao artigo.\n\nDefinir as Hipóteses\n\n\\[H_0: \\mu = 2\\] \\[H_1:\\mu \\neq 2 \\]\n\nDefina o Estimador\n\nVamos utilizar a Média Amostral, \\(\\bar{X}\\), para testar a hipótese nula \\(H_0\\).\n\nEstabelecer os Critérios de Decisão\n\nVamos adotar \\(\\alpha= 0,05\\) (5%).\n\nAnalisar os Dados da Amostra\n\n\nt.test(x=final_fem_22$feminic_tx, y = NULL,\n       alternative = c(\"two.sided\"),\n       mu = 2, \n       paired = FALSE, \n       var.equal = TRUE,\n       conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  final_fem_22$feminic_tx\nt = -2.9276, df = 26, p-value = 0.007011\nalternative hypothesis: true mean is not equal to 2\n95 percent confidence interval:\n 1.407411 1.896292\nsample estimates:\nmean of x \n 1.651852 \n\n\n\nTomar uma Decisão e Interpretar os Resultados\n\nEntendendo o resultado do teste:\nOne Sample t-test: Teste de uma amostra -&gt; taxa de feminicídio\ndata:  final_fem_22$feminic_tx: Indica a variável que utilizou\nt = -2.9276, df = 26, p-value = 0.007011: O valor do teste t, o grau de liberdade e o p-valor\nalternative hypothesis: true mean is not equal to 2: A hipótese alternativa é que a média não é igual a 2\n95 percent confidence interval: 1.407411 1.896292 : O intervalo de confiança de 95% para a média, indicando que a verdadeira média populacional está nesse intervalo.\nsample estimates: mean of x  = 1.651852: A média amostral é de 1.65, a qual é a estimativa da verdadeira média populacional.\nComo o p-valor foi menor que o nível de significância \\(\\alpha\\), rejeita-se a hipótese nula em favor da hipótese alternativa. Há evidências de que a média nacional é menor do que 2.\nTeste Unicaudal\nPodemos testar também a seguinte hipótese com nível de significância de 1%:\n\\[H_0: \\mu = 2\\] \\[H_1:\\mu &lt; 2 \\] No R:\n\nt.test(x=final_fem_22$feminic_tx, y = NULL,\n       alternative = c(\"less\"),\n       mu = 2, \n       paired = FALSE, \n       var.equal = TRUE,\n       conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  final_fem_22$feminic_tx\nt = -2.9276, df = 26, p-value = 0.003505\nalternative hypothesis: true mean is less than 2\n99 percent confidence interval:\n     -Inf 1.946607\nsample estimates:\nmean of x \n 1.651852 \n\n\nNovamente como o p-valor foi menor que o nível de significância \\(\\alpha\\), rejeita-se a hipótese nula em favor da hipótese alternativa. Há evidências de que a média nacional é menor do que 2.\nTeste para Diferença de Médias\n\nTaxa de Feminicídio\n\nVamos testar a hipótese de que as taxas de feminicídio são iguais entre as regiões norte, nordeste e centro oeste e as regiões sul e sudeste. Vamos permitir que as variâncias sejam diferentes entre as duas regiões\n\\[H_0: \\mu_{N-NE-CO} = \\mu_{S-SD}\\] \\[H_1: \\mu_{N-NE-CO} \\neq \\mu_{S-SD}\\]\n\nt.test(feminic_tx~N_NE_CO, data=final_fem_22,\n       alternative = c(\"two.sided\"),\n       var.equal = FALSE,\n       conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  feminic_tx by N_NE_CO\nt = -1.2049, df = 20.949, p-value = 0.2417\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6640345  0.1768916\nsample estimates:\nmean in group 0 mean in group 1 \n       1.471429        1.715000 \n\n\nComo o p-valor foi maior que o nível de significância \\(\\alpha\\), não rejeita-se a hipótese nula. Não há evidências de que as regiões possuem taxas diferentes de feminicídio.\n\nRenda pc\n\nVejamos agora a renda per capita a 1% de significância :\n\nt.test(rendapc~N_NE_CO, data=final_fem_22,\n       alternative = c(\"two.sided\"),\n       var.equal = FALSE,\n       conf.level = 0.99) \n\n\n    Welch Two Sample t-test\n\ndata:  rendapc by N_NE_CO\nt = 4.6401, df = 22.301, p-value = 0.0001225\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n99 percent confidence interval:\n 242.1018 989.0840\nsample estimates:\nmean in group 0 mean in group 1 \n       1903.143        1287.550 \n\n\nObserva-se que rejeita-se a hipótese nula, e há evidências de que a média da renda per capita entre as duas regiões são diferentes.\nAmostras Pareada\nVamos testar a diferença de média entre amostra pareadas. A amostra pareada é quando temos o mesmo indivíduo observado duas vezes. Vamos testar se existe diferença entre a participação do feminicídio sobre os homicídios e a participação da tentativa de feminicídio sobre as tentativas de feminicídio feminino.\n\\[H_0: \\mu_{part-feminic} = \\mu_{part-t-feminic}\\] \\[H_1: \\mu_{part-feminic} \\neq \\mu_{part-t-feminic}\\]\nUtilizando a média amostral para testar a hipótese nula \\(H_0\\) e considerando um nível de significância de 95%. No R temos:\n\nt.test(x=final_fem_22$part_feminic, y=final_fem_22$part_t_feminic,\n       alternative = c(\"two.sided\"),\n       paired = TRUE, \n       var.equal = FALSE,\n       conf.level = 0.95)\n\n\n    Paired t-test\n\ndata:  final_fem_22$part_feminic and final_fem_22$part_t_feminic\nt = 3.457, df = 24, p-value = 0.002049\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  4.105405 16.269795\nsample estimates:\nmean difference \n        10.1876 \n\n\nComo o p-valor foi menor que o nível de significância \\(\\alpha\\), rejeita-se a hipótese nula em favor da hipótese alternativa. Há evidências de que a média da participação do feminicídio é maior do que a média da participação da tentativa de feminicídio. A estimativa dessa diferença é de 10 pontos percentuais a mais para participação do feminicídio.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Uma Introdução à Estatística</span>"
    ]
  },
  {
    "objectID": "lineares.html",
    "href": "lineares.html",
    "title": "9  Modelos Lineares",
    "section": "",
    "text": "9.1 Introdução\nVamos iniciar o estudo de modelos lineares começando pela Regresão Linear Simples (RLS). Mais específicamente, vamos estudar a RLS no contexto de dados de corte transversal. Tal abordagem, segmentada por tipos de dados, facilíta o entendimento das hipóteses do modelo.\nOs conceitos estatísticos aplicados no estudo de RLS são os mesmos apresentados na seção anterior.\nO matérial desta seção é baseado na 4ed do livro de Jeffrey Wooldridge, Introdução à Econometria: Uma Abordagem Moderna, de 2013. Embora o título do livro remeta à econometria, os modelos apresentados no livro são aplicaveis à diversas Ciências Sociais, tais como o Direito, Ciência Política, Sociologia, Psicologia Empírica etc…\nA análise de Regressão é um instrumento poderoso para os centistas sociais. Podemos verificar como determinadas variáveis importantes nas ciencias sociais interagem com outras, as vezes em uma relação de causa e efeito.\nAqui também teremos que ter uma questão ou problema. Anteriormente gostariamos de entender o que estava acontecendo, qual a Renda pc das mulheres, qual a taxa de feminicídio… Agora queremos ir mais a fundo, porque determinados bairros tem mais crime do que outros? O que determina a renda pc de uma mulher?\nComeçamos com o modelo mais geral da população:\n\\[y = \\beta_0 + \\beta_1x + u \\]\nNesta equação, \\(y\\) é a variavel dependente ou também denominada de variável explicada; \\(x\\) é a variável explicativa e \\(u\\) é o termo de erro. Essa equação é uma equação de regressão linear simples.\nVejamos um exemplo\nGostariamos de explicar a taxa de crimes nos bairros de uma cidade e consideramos que os níveis de desemprego nas localidades são importantes. Nosso modelo de regressão poderia ser específicado da seguinte maneira:\n\\[Crime_i = \\beta_0 + \\beta_1Desemprego_i + u\\]\nO subscrito \\(i\\) se refere a um bairro hipotético da cidade. Note que nesse caso, \\(i\\) é o subscrito que relaciona nossas unidades de observação (bairros). Nossas unidades de observação podem variar a depender do contexto em estudo: países, cidades, bairros, estados, pessoas, processos, varas, tribunais….\nNote que, novamente o termo de erro, \\(u\\), está presente na equação. O termo de erro, não observado, capta tudo aquilo que afeta \\(Crime\\), mas que não estamos controlando, ou seja, que não é explicado pelo \\(Desemprego\\).\nVoltemos a equação básica:\n\\[y = \\beta_0 + \\beta_1x + u \\]\nNa análise de RLS estamos interessados nos parâmetros \\(\\beta_0\\) e sobretudo \\(\\beta_1\\). A razão primordial para isso é que, tudo o mais constante, a relação acima aponta que\n\\[\\Delta y = \\beta_1 \\Delta x \\] , se \\[\\Delta u = 0\\]\nIsto é, se tudo o mais que afeta \\(y\\) permanecer inalterado, uma variáção em \\(x\\), \\(\\Delta x\\), terá um impacto de \\(\\beta_1 \\Delta x\\) em \\(y\\). No exemplo da criminalidade, teremos que:\n\\[\\Delta crime = \\beta_1 \\Delta desemprego \\]\nAssim, o aumento de 1 ponto no desemprego irá ter o efeito de \\(\\beta_1\\) unidades de crimes em média. Por isso, quando conseguimos estimar os parêmetros \\(\\beta\\) estamos mais próximos de entender as relações entre \\(x\\) e \\(y\\) em nossas aplicações.\nCabe destacar algo bastante importante.As relações acima não encerram a questão da causalidade. Como podemos inferir um impacto causal do desemprego na criminalidade se estamos ignorando todos os demais fatores que ficaram de fora do modelo - fatores que são captados em \\(u\\) não observados.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#introdução",
    "href": "lineares.html#introdução",
    "title": "9  Modelos Lineares",
    "section": "",
    "text": "9.1.1 Alguns Exemplos de Regressão Aplicados ao Contexto Jurídico:\nConsidere os seguintes exemplos de RLS aplicadas ao contexto do Direito\nPrevisão de Sentenças com Base na Gravidade do Crime:\n\nVariável dependente \\((Y)\\): Tamanho da pena,\nVariável independente\\((X)\\): Gravidade do Crime,\nObjetivo: Identificar se a gravidade do crime tem influência sobre tamanho da pena.\n\nNesse exemplo \\(\\beta_0\\) é o intercepto da regressão e \\(\\beta_1\\) o coeficiente de regressão que representa como a gravidade do crime influência o tamanho da pena, \\(u\\) é o termo de erro.\n\\[Dpena = \\beta_0 + \\beta_1{\\text{Gravidade}} + u\\]\nAnálise de Fatores que Influenciam o Tempo de Julgamento:\n\nVariável Dependente (\\(Y\\)): Tempo de Duração do Processo\nVariável Independente (\\(x\\)), Tipo de Processo (por exemplo, criminal, civil, administrativo).\nObjetivo: Identificar se o tipo de processo tem impacto no tempo que um caso leva para ser concluído.\n\n\\[Tempo = \\beta_0 + \\beta_1  Processo  + u\\]\nPrevisão de Probabilidade de Recurso com Base em Decisões Anteriores:\n\nVariável Dependente (\\(Y\\)): Probabilidade de Entrada com Recurso\nVariável Independente (\\(X\\)): Resultado da Decisão Anterior (por exemplo, deferimento ou negação do recurso).\nObjetivo: Determinar a probabilidade de um recurso ser apresentado com base no resultado de decisões passadas.\n\n\\[Recurso = \\beta_0 + \\beta_1 Resultado_{-1} + u \\]\nAnálise de Relação entre Número de Testemunhas e Veredito:\n\nVariável Dependente (\\(Y\\)): Veredito (por exemplo, culpado ou inocente)\nVariável Independente (\\(X\\)): Número de Testemunhas.\nObjetivo: Investigar se o número de testemunhas influencia a decisão.\n\n\\[ Veredito = \\beta_0 + \\beta_1 Testemunhas + u \\]",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#hipóteses-sobre-o-comportamento-do-termo-de-erro",
    "href": "lineares.html#hipóteses-sobre-o-comportamento-do-termo-de-erro",
    "title": "9  Modelos Lineares",
    "section": "9.2 Hipóteses sobre o comportamento do Termo de Erro",
    "text": "9.2 Hipóteses sobre o comportamento do Termo de Erro\nSe especificamos o modelo com \\(\\beta_0\\), podemos assumir que \\(u\\), tem média igual a zero. Em notação de esperança matemática essa hipótese equivale a:\n\\[E(u) = 0\\]\nPodemos definir uma segunda hipotese sobre \\(u\\). Uma hipótese forte, é a de que \\(u\\) e \\(x\\) são independentes. Tal hípótese é crucial do modelo de RLS:\n\\[E(u|x) = 0\\]\nJustas as hipóteses de \\(E(u) = 0\\) e \\(E(u|x) = 0\\) são denominadas de hipotese de média condicional zero.\nDessa forma, no nosso modelo inicial:\n\\[y = \\beta_0 + \\beta_1x + u\\]\nAplicando o operador \\(E( \\cdot |x )\\), obtemos\n\\[ E( y |x ) = \\beta_0 + \\beta_1x \\]\nAssim, na equação acima, temos um aumento de uma unidade em \\(x\\), implica em um aumento no valor esperado (ou em média) de \\(y\\) na magnitude de \\(\\beta_1\\).\nA equação acima, é caracterizada como função de regressão populacional. Essa função nos fornece a relação entre os diferentes níveis de \\(x\\) é o nível médio de \\(y\\), isto é \\(E( y |x )\\).\n\n\n\nFunção de Regressão Populacional, fonte: Anderson David R., Sweeney Dennis J., Williams Thomas A. (2019), Statistics for Business & Economics, Cengage Learning; 14th edition\n\n\nAgora, podemos voltar a nossa equação base e verificarmos o progresso feito no entendimento do modelo:\n\\[y = \\beta_0 + \\beta_1x + u \\]\n\\[y = E( y |x ) + u \\]\nNa equação acima, \\(E( y |x )\\) é chamada de parte sistemática de \\(y\\). Isto é, a parte sistematicamente explicada por \\(x\\). Já o termo de erro não observado, \\(u\\) é a parte não sistemática de \\(y\\), não explicada por \\(x\\).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#estimação-dos-parâmetros-da-rls",
    "href": "lineares.html#estimação-dos-parâmetros-da-rls",
    "title": "9  Modelos Lineares",
    "section": "9.3 Estimação dos Parâmetros da RLS",
    "text": "9.3 Estimação dos Parâmetros da RLS\nNão conhecemos \\(\\beta_0\\) e \\(\\beta_1\\) queremos estimadores desses parametros: \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\).\nSuponha que tenhamos uma amostra aleatória da população: \\(\\{(x_i, y_i): i = 1, ..., n\\}\\). Poderia ser uma amostra de crimes e taxa de desemprego por bairros.\nEm nosso modelo base:\n\\[y_i = \\beta_0 + \\beta_1x_i + u_i \\]\nonde \\(u_i\\) é o erro aleatório da \\(i\\)-ésima observação.\nComo estimamos \\(\\beta_0\\) e \\(\\beta_1\\)?\nApós algum esforço algébrico, tem-se:\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\ne\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\nOnde, \\(\\bar{x}\\) e \\(\\bar{y}\\) são as médias amostrais de \\(X_i\\) e \\(Y_i\\), respectivamente.\nOs estimadores \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) são chamados de estimadores de Mínimos Quadrados Ordinários (MQO).\nA ideia é que os parâmetros que estimamos, \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\), são os parâmetros que minimizam a soma dos quadrados das diferenças entre nossos \\(y_i\\) observados e seus valores preditos definidos como\n\\[y_i- \\hat{y}_i =y_i -  \\hat{\\beta}_0 + \\hat{\\beta}_1x_i=u_i\\]\nQueremos minimizar o quadrado dessa diferença. Vejamos graficamente:\n\n\n\nValores Ajustados e Resíduos de MQO\n\n\nAntes de passarmos para o proxímo tópico de RLS, cabe mencionar uma nota sobre terminologia: quando estimamos equações de RLS do tipo \\(y = \\beta_0 + \\beta_1 x + u\\), dizemos que “rodamos a regressão de \\(y\\) sobre \\(x\\) !”\n\n9.3.1 Teste de Hipótese sob um Único Parametro\nAgora precisamos testar se nossa estimativa é estatisticamente igual a zero ou se ela é diferente de zero.\nHá evidências que a nossa variável \\(X\\) afeta a nossa variável \\(Y\\) na população?\nNa maior parte do tempo testamos hipóteses do tipo\n\\[H_0: {\\beta_j}=0\\] Na hipótese principal testamos a situação que \\(X\\) não afeta \\(Y\\) na população sob análise. Na hipótese alternativa:\n\\[H_1: \\beta_j \\neq 0\\]\nTestamos a hipótese que \\(X\\) afeta \\(Y\\) nessa população. Graficamente temos a figura abaixo onde a média populacional é igual a 0:\n\n\n\nTeste de hipótese, Fonte:CUEMATH\n\n\nEstatística de Teste:\nEste é um valor calculado a partir dos dados da amostra que é usado para avaliar a probabilidade de observar tal valor se a hipótese nula fosse verdadeira (\\(H_0: {\\beta_j}=0\\)). Ele que vai dizer se estamos na área amarela ou azul da figura acima.\n\n9.3.1.1 Valor-p:\nO valor-p é a probabilidade de observar uma estatística de teste tão extrema quanto, ou mais extrema do que, aquela calculada a partir dos dados da amostra, sob a suposição de que a hipótese nula seja verdadeira. Em outras palavras, mede quão provável é obter os resultados observados se a hipótese nula for correta.\nInterpretação: Um valor-p pequeno (tipicamente menor que um nível de significância predefinido, comumente 0,05) sugere que os dados observados são improváveis de ter ocorrido se a hipótese nula fosse verdadeira. Portanto, fornece evidências contra a hipótese nula, e os pesquisadores podem rejeitar a hipótese nula em favor da hipótese alternativa. Por outro lado, um valor-p grande indica que os dados observados são consistentes com a hipótese nula, e as evidências evidências são insuficientes para rejeitá-la.\n\n\n\n9.3.2 Aplicação - Estimando uma Regressão Linear Simples\nVamos utilizar aqui o nosso banco de dados anterior sobre feminicídio. É um banco com poucas obserações e possui fim didátco, apesar de serem dados reais.\nUma questão incial é entender quem será \\(Y\\) e quem será \\(X\\). Estamos querendo entender a taxa de feminicídio. A questão é, quais são seus determinantes? Aqui precisamos de alguma teoria…mas por hora, vamos ter como hipótese inicial que quanto maior a taxa de homícídio (ou maior a violência no estado) espera-se que em média a taxa de feminicídio aumente:\n\\[Feminc_{tx} = \\beta_0 + \\beta_1 Homici_{tx} + u\\]\nApesar de parecer “muito complicado” na teoria, na prática o R estima uma RLS em segundos. A função para estimar é alm, ou linear model.\n\n### Aplicação da RLS - Método dos Mínimos Quadrados Ordinários\n\n#carregando o pacote para ler arquivos em excel\nload(\"C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.Rdata\")\n\n#Vamos chamar a partir de agora nosso banco de dados\ndados&lt;- final_fem_22 \n\n##Função para rodar a regressão\nmodelo &lt;- lm(data = dados, feminic_tx ~ homic_tx)\n\n\n#Essa função resume a regressão, ja testar a hipótese sobre o coeficiente e da outras estatisticas que abordaremos a seguir.\nsummary(modelo)\n\n\nCall:\nlm(formula = feminic_tx ~ homic_tx, data = dados)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.28942 -0.30169  0.03482  0.30070  1.18192 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.14673    0.28607   4.009 0.000485 ***\nhomic_tx     0.10580    0.05503   1.923 0.065990 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5882 on 25 degrees of freedom\nMultiple R-squared:  0.1288,    Adjusted R-squared:  0.09397 \nF-statistic: 3.697 on 1 and 25 DF,  p-value: 0.06599\n\n\n\n9.3.2.1 Entendendo os Resultados\n1. Resíduos (Residuals): apresenta a distribuição dos resíduos, observa-se que a mediana dos resíduos está próxima a zero.\n2.Coeficientes Em seguida, abaixo tem-se Coefficients com 5 colunas.\nColuna 1: Nome da variável\nColuna 2: Coeficiente estimado \\(\\hat{\\beta_0}\\) o intercepto e \\(\\hat{\\beta_1}\\) do termo da variável homicídio\nColuna 3: É o desvio padrão da estimativas de \\(\\beta\\)\nColuna 4: É a estatistica t utilizada para fazer o teste de hipótese e neste caso, tem-se: \\[H_0:\\beta_0=0\\] \\[H_1:\\beta_0 \\neq 0\\] e \\[H_0:\\beta_1=0\\] \\[H_1:\\beta_1 \\neq 0\\]\nColuna 5: É o p-valor, a probabilidade de encontrarmos valores mais extremos da estatística t. Os asteriscos indicam *** signicante a 0,1%; ** a 1%; * a 5%; e \\(.\\) a 10%.\nInterpretando o Coeficiente\nO coeficiente do intercepto nos diz que quando a taxa de homicídio for 0, ainda existira uma taxa de feminicídio de 1,14 pontos (\\(\\beta_0\\)), sendo significante a 0,1%. E cada 1 ponto de aumento na taxa de homicídio, aumenta em 0,106 (\\(\\beta_1\\)) pontos a taxa de feminicídio, significante a 10%.\n3. \\(R^2\\): Em seguida temos o \\(R^2\\) e \\(R^2\\) ajustado. Essas estatísticas calculam quanto da variância da taxa de feminicídio pode ser explicada pela variância da taxa de homicídio. Quando estão próximos a 1 explicam muito, quanto estão próximos a 0 explicam pouco. No caso o nosso \\(R^2\\) ficou baixo e portanto boa parte da variabilidade do feminicídio, não foi explicada pela taxa de homicídio.\n4. Estaística F: E por fim a estatística F mostra o grau de ajustamento do modelo. Se ela for significativa diz que o modelo é bem ajustados aos dados. As variáveis explicativas incluídas são importantes para a explicação da taxa de feminicídio.Essa estatística testa a seguinte hipótese.\n\\[H_0:\\beta_1=...=\\beta_k=0\\] contra\n\\[H_1: H_0\\text{ é falsa}\\]\nVISUALMENTE\nAbaixo tem-se os dados utilizados e a nossa reta de regressão estimada acima. Veja como ela se ajusta a nuvem de pontos.\n\n# Ajustando a reta de regessão.\n plot(dados$homic_tx, dados$feminic_tx,\n     main = \"Taxa de Feminicídio vs.Tx de homicidio\",\n     xlab = \"Taxa de Homicidio\",\n     ylab = \"Taxa de Feminicídio\",\n     col = \"steelblue\",          # Cor dos pontos\n     pch = 16,              # Forma dos pontos (círculos sólidos)\n     cex = 1.0,         # Tamanho dos pontos\n     abline(modelo, col = \"lightsalmon3\", lwd = 3))",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#caracteristicas-do-método-dos-mínimos-quadrados-ordinários-em-determinadas-amostras-de-dados",
    "href": "lineares.html#caracteristicas-do-método-dos-mínimos-quadrados-ordinários-em-determinadas-amostras-de-dados",
    "title": "9  Modelos Lineares",
    "section": "9.4 Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados",
    "text": "9.4 Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados\n\n9.4.1 Valores Estimados e Resíduos\nUma vez estimados \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) temos os valores ajustados ou também denominados de valores preditos ou fitted values.\nPara uma observação qualquer \\(i\\), seu valor estimado é:\n\\[\\hat{y}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\]\nTodos os valores \\(\\hat{y}\\) estarão sobre a reta de regressão.\nO resíduo, tal qual definido anteriormente, será a diferença entre o valor ajustado \\(\\hat{y}\\) e o verdadeiro \\(y\\) em nosso banco de dados:\n\\[\\hat{u}_i = y_i - \\hat{y}_i\\]\nSe \\(\\hat{u_i} &gt; 0\\) a regressão subestima \\(y_i\\). Se \\(\\hat{u_i} &lt; 0\\) a reta superestima \\(y_i\\). O cenário ideal é quando \\(\\hat{u}_i= 0\\), algo que quase nunca acontece.\n\n##Obtendo os residuos da regressão\nresid &lt;- residuals(modelo)\n k &lt;- density(resid)\n  plot(k, xlab=\"Erro\",main=\"Densidade de Kernel para o erro\")\n  polygon(k, col=\"burlywood3\", border=\"burlywood4\")\n\n\n\n\n\n\n\n\n\n\n9.4.2 Propriedades Algébricas do MQO\n\nA Soma dos Resíduos é zero:\n\n\\[\\sum_{i=1}^{n} \\hat{u}_i = 0\\]\n\nsum(resid)\n\n[1] 1.387779e-16\n\n\n\nA covariancia amostral entre a variavel explicativa e os resíduos é zero:\n\n\\[\\sum_{i=1}^{n} x_i \\hat{u}_i = 0\\]\n\n#obtendo a variável x\nx &lt;- dados$homic_tx\n\n#somando com os residuos\nsum(x*resid)\n\n[1] 8.326673e-17\n\n\n\nO ponto \\((\\bar{x},\\bar{y})\\) sempre estará sob a reta de regressão.\nA média dos valores estimados, \\(\\bar{\\hat{y}}\\) é igual a média dos valores observados \\(\\bar{y}\\).\n\n\n#y dos dados \ny &lt;- dados$feminic_tx\nmean(y)\n\n[1] 1.651852\n\n#y estimado\ny_hat &lt;- fitted.values(modelo)\nmean(y_hat)\n\n[1] 1.651852\n\nmean(y)==mean(y_hat)\n\n[1] TRUE\n\n\n\nNote que as estimatvas de MQO decompõe \\(y\\) em 2 partes: 1) os valores ajustados \\(\\hat{y}\\) e os resíduos \\(\\hat{u}\\).\nOs valores de \\(\\hat{y}\\) e \\(\\hat{u}\\) são não correlacionados na amostra!\n\n\n\n9.4.3 Qualidade do Ajuste\nNesta seção vamos responder a seguinte questão: “Quão bem \\(x\\) explica \\(y\\) ?”\nConsidere as seguintes definições\nSoma dos Quadrado Totais (SQT):\n\\[\\text{SQT} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\n\n# Calcular a média da variável dependente (feminic_tx)\nmedia_feminic_tx &lt;- mean(dados$feminic_tx)\n\n# Calcular a Soma dos Quadrados Totais (SQT)\nsqt &lt;- sum((dados$feminic_tx - media_feminic_tx)^2)\nsqt\n\n[1] 9.927407\n\n\nSoma dos Quadrados Explicados (SQE):\n\\[\\text{SQE} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\n\n# Calcular a Soma dos Quadrados Explicados (SQE)\nsqe &lt;- sum((y_hat - media_feminic_tx)^2)\nsqe\n\n[1] 1.27879\n\n\nSoma dos Quadrados dos Resíduos (SQR):\n\\[\\text{SQR} = \\sum_{i=1}^{n} \\hat{u}_i^2\\]\n\nresiduos_quadrados &lt;- residuals(modelo)^2\nsqr &lt;- sum(residuos_quadrados)\nsqr\n\n[1] 8.648617\n\n\nO resultado mais importante é o seguinte:\n\\[SQT =  SQE + SQR\\]\nÉ apartir dessa iguadade que podemos mostrar algo sobre o ajuste dos MQO.\nDívidindo ambos os lados por \\(SQT\\) teremos\n\\[1 = \\frac{\\text{SQE}}{\\text{SQT}}  + \\frac{\\text{SQR}}{\\text{SQT}}\\]\nRearranjando os termos\n\\[R^2 = \\frac{\\text{SQE}}{\\text{SQT}} = 1 - \\frac{\\text{SQR}}{\\text{SQT}}\\]\n\n#r-quadrado do modelo\nr_quadrado &lt;- 1 - (sqr/sqt)\nr_quadrado\n\n[1] 0.1288141\n\n\nNovamente, o \\(R^2\\) é a porcentagem da variação de y que é explicada por \\(x\\). O valor de \\(R^2\\) sempre estará na RLS entre \\(0\\) e \\(1\\).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#unidade-de-medida-e-forma-funcional",
    "href": "lineares.html#unidade-de-medida-e-forma-funcional",
    "title": "9  Modelos Lineares",
    "section": "9.5 Unidade de Medida e Forma Funcional",
    "text": "9.5 Unidade de Medida e Forma Funcional\n\n9.5.1 Unidade de Medida\nVejamos o efeito de mudanças na unidade de medida das variáveis na nossa regressão.\n1. Se a variável dependente (\\(Y\\)) é multiplicada por uma constante \\(c\\), então as estimativas do intercepto e da inclinação também serão multiplicadas por \\(c\\). Vamos multiplicar por 10 e ver o que acontece:\n\nlibrary(dplyr)\ndados &lt;- dados |&gt; \n  mutate(feminic_tx10 =feminic_tx*10 )\n\nmodelo1 &lt;- lm(data = dados, feminic_tx10 ~ homic_tx)\n\nsummary(modelo)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 1.146731 0.28607061 4.008560 0.0004846694\nhomic_tx    0.105805 0.05503129 1.922633 0.0659903845\n\nsummary(modelo1)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 11.46731  2.8607061 4.008560 0.0004846694\nhomic_tx     1.05805  0.5503129 1.922633 0.0659903845\n\n\n2. Se a variável explicativa (\\(X\\)) é multiplicada por uma contante \\(c\\), então o coeficiente de inclinação será dívido por \\(c\\). Nada acontece com o intercepto. Vamos multiplicar agora a taxa de homicídio por 10\n\ndados &lt;- dados |&gt; \n  mutate(homic_tx10 =homic_tx*10 )\n\nmodelo2 &lt;- lm(data = dados, feminic_tx ~ homic_tx10)\n\nsummary(modelo)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 1.146731 0.28607061 4.008560 0.0004846694\nhomic_tx    0.105805 0.05503129 1.922633 0.0659903845\n\nsummary(modelo2)$coefficients\n\n             Estimate  Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 1.1467311 0.286070605 4.008560 0.0004846694\nhomic_tx10  0.0105805 0.005503129 1.922633 0.0659903845\n\n\n\nSe a variável explicativa (\\(X\\)) é dividida por uma constante \\(c\\), então o coeficiente de inclinação é multiplicado por \\(c\\). Nada acontece com a constante.\n\n\ndados &lt;- dados |&gt; \n  mutate(homic_tx_10 =homic_tx/10 )\n\nmodelo3 &lt;- lm(data = dados, feminic_tx ~ homic_tx_10)\n\nsummary(modelo)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 1.146731 0.28607061 4.008560 0.0004846694\nhomic_tx    0.105805 0.05503129 1.922633 0.0659903845\n\nsummary(modelo3)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 1.146731  0.2860706 4.008560 0.0004846694\nhomic_tx_10 1.058050  0.5503129 1.922633 0.0659903845\n\n\n\n\n\n\n\n\nImportante\n\n\n\nO \\(R^2\\) e os testes de hipótese não depende das unidades de nossas variáveis.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#incorporando-não-linearidades-nas-variáveis",
    "href": "lineares.html#incorporando-não-linearidades-nas-variáveis",
    "title": "9  Modelos Lineares",
    "section": "9.6 Incorporando não linearidades (nas variáveis!)",
    "text": "9.6 Incorporando não linearidades (nas variáveis!)\nVamos agora incorporar a não linearidade nas variáveis e ver como fica a sua interpretação. Abaixo tem-se uma tabela para interpretar os coeficientes e será explicada nos exemplos que virão a seguir.\n\n\n\n\n\n\n\n\n\nModelo\nDependente\nExplicativa\nInterpretação de \\(\\beta_1\\)\n\n\n\n\nNivel-nível\n\\(y\\)\n\\(x\\)\n\\(\\beta_1 \\Delta x\\)\n\n\nNível-Log\n\\(y\\)\n\\(ln(x)\\)\n\\((\\beta_1/100)\\%\\) \\(\\Delta x\\)\n\n\nLog-Nível\n\\(ln(y)\\)\n\\(x\\)\n\\(\\% \\Delta y = (100 \\beta_1) \\Delta x\\)\n\n\nLog_log\n\\(ln(y)\\)\n\\(ln(x)\\)\n\\(\\% \\Delta y = \\beta_1\\% \\Delta x\\)\n\n\n\n\n9.6.1 Modelo Nível-Nível\nEsse é o modelo que fizemos anteriormente e nos diz que uma variação absoluta em \\(X\\) terá um efeito de \\(\\beta\\) em Y.\n\nmodelo_nn&lt;- lm(data=dados, dados$feminic_tx ~ dados$homic_tx)\nsummary(modelo_nn)$coefficients\n\n               Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)    1.146731 0.28607061 4.008560 0.0004846694\ndados$homic_tx 0.105805 0.05503129 1.922633 0.0659903845\n\n\nO aumento de uma unidade na taxa de homicídio aumenta em 0,1058 a taxa de feminicídio.\n\n\n9.6.2 Modelo Nível-log\nPrimeiramente vamos criar as variáveis em ln:\n\ndados$Lnfeminic_tx &lt;- log(dados$feminic_tx)\ndados$Lnhomic_tx &lt;- log(dados$homic_tx)\n\nO modelo Nível-Log considerá \\(Y\\) no nível e \\(X\\) no log. As variações relativas de \\(X\\) implicam em variações absolutas constantes em \\(Y\\).\n\nmodelo_nl&lt;- lm(data=dados, dados$feminic_tx ~ dados$Lnhomic_tx)\nsummary(modelo_nl)$coefficients\n\n                  Estimate Std. Error  t value   Pr(&gt;|t|)\n(Intercept)      0.7246404  0.4453288 1.627203 0.11623380\ndados$Lnhomic_tx 0.6238311  0.2900917 2.150462 0.04138118\n\n\nA variação relativa de 1% na taxa de homicídio (\\(X\\)), aumenta a taxa de feminicídio em 0,00623 \\((\\beta_1/100)\\)\n\n\n9.6.3 Modelo Log-Nível\nNesse modelo \\(Y\\) está em log e \\(X\\) em nível. Variações absolutas em \\(X\\) implicam em variações exponenciais de \\(Y\\).\n\nmodelo_ln&lt;- lm(data=dados, dados$Lnfeminic_tx~ dados$homic_tx)\nsummary(modelo_ln)$coefficients\n\n                 Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)    0.21824231 0.18282873 1.193698 0.2438007\ndados$homic_tx 0.04525001 0.03517069 1.286583 0.2100267\n\n\nPara cada incremento adicional da taxa de homicídio, tem-se um incremento de 4,5% na taxa de feminicídio \\(\\% \\Delta y = (100 \\beta_1) \\Delta x\\).\nmodelo_ll &lt;- lm(data=dados, dados\\(Lnfeminic_tx~ dados\\)Lnhomic_tx) summary(modelo_ll)$coefficients\n\n\n9.6.4 Modelo Log-Log\nNesse modelo tanto \\(X\\) como \\(Y\\) estão em log. Variações relativas em \\(X\\) implicam em variações relativas em \\(Y\\)\n\nmodelo_ll &lt;- lm(data=dados, dados$Lnfeminic_tx~ dados$Lnhomic_tx)\nsummary(modelo_ll)$coefficients\n\n                     Estimate Std. Error     t value  Pr(&gt;|t|)\n(Intercept)      0.0009590929   0.284865 0.003366832 0.9973404\ndados$Lnhomic_tx 0.2915325798   0.185564 1.571062163 0.1287399\n\n\nUm aumento de 1% na taxa de homicídios aumenta em 0,29% a taxa de feminicídio nos estados brasileiros.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#regressão-linear-múltipla-rlm",
    "href": "lineares.html#regressão-linear-múltipla-rlm",
    "title": "9  Modelos Lineares",
    "section": "9.7 Regressão Linear Múltipla (RLM)",
    "text": "9.7 Regressão Linear Múltipla (RLM)\nA RLM é uma extensão natural da RLS. Entretanto, ao invés de termos apenas uma variável explicativa podemos ter \\(k\\) variáveis dependentes\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\ldots + \\beta_k x_k + u\\] A hípótese fundamental é a de que:\n\\[E(u|x_1, . . . ,x_k) = 0\\]\nProblemas que façam com que um dos \\(x_1, ...., x_k\\) ser correlacionado com \\(u\\), invalida a hipótese acima. Tal hipótese implica em não viés do MQO.\n\n9.7.1 Exemplos de Regressão Multipla no Contexto Jurídico\nDeterminação de Fatores que Afetam o Valor de Indenizações:\n\nVariável Dependente (\\(Y\\)): Valor da Indenização.\nVariáveis Independentes (\\(X\\)): Idade da Vítima, Gravidade do Dano, Jurisdição.\nObjetivo: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas\n\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Gravidade + \\beta_2 Jurisdisção + u\\]\nAnálise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais:\n\nVariável Dependente (\\(Y\\)): Taxa de Condenação.\nVariáveis Independentes(\\(X\\)): Idade do Réu, Tipo de Crime, Local do Julgamento.\nObjetivo: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.\n\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Local  + \\beta_3  crime + u\\]\nAnálise de Fatores que Influenciam a Taxa de Feminicídio:\n\nVariável Dependente (\\(Y\\)): Taxa de Feminicídio por 100.000 mulheres\nVariáveis Independentes (\\(X\\)): Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher.\nObjetivo: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.\n\n\\[Tx.feminicidio = \\beta_0 + \\beta_1 desemprego + \\beta_2 educ + \\beta_3 política + u \\]\n\n\n9.7.2 A Interpretação da Equação de Regressão de MQO\nTal qual no modelo de RLS, temos que:\n\\[\\Delta y = \\beta_1 \\Delta x_1 + \\beta_2 \\Delta x_2 + ...+\\beta_k \\Delta x_k\\]\nO coeficiente \\(\\beta_j\\), com \\(j=1,...k\\), mede o efeito do incremento de uma unidade de \\(x_j\\) em \\(y\\). Isso continua com a mesma interpretação para modelos que usam o log.\nSuponha que estejamos interessados no efeito de \\(x_1\\). Podemos fazer o seguinte exercício: Tudo o mais constante qual o efeito de \\(x_1\\) em \\(y\\):\n\\[\\frac{\\Delta y}{\\Delta x} = \\beta_1 \\]\nManter outros fatores fixos permite o cientista social, “mimetizar” um experimento, o qual é muito utilizado nas Ciências Naturais. Obviamente, isso não é tão simples assim. Entretanto, manter outros fatores fixos, e supondo que \\[E(u|x_1, . . . ,x_k) = 0\\] , nos aproxíma de afirmações de cunho causal.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "lineares.html#estimação-dos-k-parametros-na-rlm",
    "href": "lineares.html#estimação-dos-k-parametros-na-rlm",
    "title": "9  Modelos Lineares",
    "section": "9.8 Estimação dos k parametros na RLM",
    "text": "9.8 Estimação dos k parametros na RLM\n\n9.8.1 Estimadores de MQO\nA mecânica para conseguirmos estimativas de \\(\\beta_j\\), fica uma pouco mais complicada. Felizmente, os softwares, tais como o R, fornecem essas estimativas com muita facilidade. Não obstante, convêm apresentar uma abordagem para o cálculo desses parâmetros. Utilizaremos álgebra de matrizes para mostrar um “algoritmo” para calcular esses \\(\\beta_j\\). Em verdade, esse é o algoritmo utilizado pela função lm() do R no computo dos estimadores.\nConsidere o modelo para a \\(i\\)-ésima observação\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_{2} x_{i2} + \\beta_{3} x_{i3} + \\ldots + \\beta_k x_{ik} + u\\]\ncomo temos \\(n\\) observações (tamanho de nossa amostra), podemos organizar esses valores em vetores e matrizes.\nAssim, representamos o modelo de RLM da seguinte maneira:\n\\[\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{u}\\]\nQueremos encontrar um estimador de \\(\\boldsymbol{\\beta}\\), que chamaremos de \\(\\boldsymbol{\\hat{\\beta}}\\). Esse estimador sera dado por:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\]\nOnde \\(\\mathbf{X}^T\\) é a matriz transposta de \\(\\mathbf{X}\\), e \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\) é a matriz inversa da seguinte multiplicação de matrizes, \\((\\mathbf{X}^T \\mathbf{X})\\)\nEsse vetor de estimativas irá nos fornecer os valores para os \\(\\beta_j\\), com \\(j=1,...,k\\).\n\n\n9.8.2 Estimativas de MQO\nVamos analisar o efeito da taxa de homicídio e da renda per capita sobre a taxa de feminicídio. Ou seja:\n\\[Feminic_i = \\beta_0 + \\beta_1 Homic_{i} + \\beta_{2} Rendapc_i  + u\\]\n\n### Estimando uma regressão Linear Multipla\n\nrlm &lt;- lm(data = dados, feminic_tx ~ homic_tx + rendapc)\n\nsummary(rlm)\n\n\nCall:\nlm(formula = feminic_tx ~ homic_tx + rendapc, data = dados)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.32561 -0.31655 -0.00645  0.31188  1.11840 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 0.8852186  0.5368738   1.649    0.112  \nhomic_tx    0.1167346  0.0588882   1.982    0.059 .\nrendapc     0.0001447  0.0002499   0.579    0.568  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5962 on 24 degrees of freedom\nMultiple R-squared:  0.1408,    Adjusted R-squared:  0.06921 \nF-statistic: 1.967 on 2 and 24 DF,  p-value: 0.1618\n\n\nA leitura dos resultados é análoga ao que já fizemos anteriormente. O aumento de uma unidade na taxa de homicídio aumenta em 0,11 a taxa de feminicídio e é significante a 10%. Veja o p-valor de 0.059. Entretanto, não há evidências de que a renda pc tem efeito sobre a taxa de feminicídio.\nAssim, estados mais violentos tendem a ter mais feminicídios. Entretanto, estados mais pobres e mais ricos possuem taxas similares de feminicídio.\n\n\n9.8.3 Qualidade do Ajuste na Regressão Linear Multipla\nUm fato importante sobre \\(R^2\\) é que ele nunca diminui ao incluirmos variáveis no modelo. Isso decorre de propriedades algébricas desse indicador. Em nosso exemplo:\n\nModelo somente com Homicídio: \\(R^2=0,12\\)\nModelo com Homicídio e Renda: \\(R^2=0,14\\)\n\nVeja que o \\(R^2\\) aumentou ao incluir a renda pc.\nPor isso, os softwares geralmente reportam uma outra estatística de ajuste o R-quadrado ajustado, \\(\\bar{R}^2\\):\n\\[\\bar{R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\]\nO \\(\\bar{R}^2\\), tal como expresso acima conta com a presença do \\(R^2\\) original, mas note que, no denominador estamos dividindo por \\(n-k-1\\), onde \\(n\\) é o tamanho da amostra em mãos, e \\(k-1\\) se referem ao fato de termos \\(k\\) variáveis explicativas e uma constante. Portanto, o \\(\\bar{R}^2\\), impões uma penalidade à inclusão de variáveis independentes em um modelo de regressão. No nosso modelo:\n\nModelo somente com Homicídio: \\(R^2=0,12\\) e \\(\\bar{R}^2= 0,094\\)\nModelo com Homicídio e Renda: \\(R^2=0,14\\) e \\(\\bar{R}^2= 0,069\\)\n\nPodemos notar que a inclusão da renda pc contribui muito pouco para a explicação do modelo mas prejudicou as estimativas (menor grau de liberdade). Dessa forma observa-se que o \\(R^2_{ajust}\\) ou \\(\\bar{R}^2\\) ficou menor.\nNão obstante, cabe destacar que um \\(R^2\\) baixo não é uma evidencia definitiva contra nosso modelo. Em ciências sociais é comum verificarmos \\(R^2\\) relatvamente pequenos. Isso significa que, embora coletivamente as variáveis explicativas não expliquem muito das variações de \\(Y\\), é possível que as estimativas de MQO sejam efeitos parciais confiáveis- tudo o mais constante - de cada \\(X_j\\) sobre \\(Y\\).\n\n\n9.8.4 Uso de Variáveis Qualitativas na Análise de RLM\nO uso de variáveis binárias é a alternativa para utilizarmos variáveis qualitativas (discretas ou categóricas) no modelo de regressão. Dessa forma, variáveis que possuem categorias (homem/mulher; regiões, típo de homicídio). Considere os exemplos mencionados anteriormente.\nDeterminação de Fatores que Afetam o Valor de Indenizações:\n\nVariável Dependente \\((Y)\\): Valor da Indenização.\nVariáveis Independentes \\((X)\\): Idade da Vítima, Gravidade do Dano, Jurisdição.\nObjetivo: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas\n\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Gravidade + \\beta_2 Jurisdisção + u\\]\nAnálise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais:\n\nVariável Dependente \\((Y)\\): Taxa de Condenação.\nVariáveis Independentes \\((X)\\): Idade do Réu, Tipo de Crime, Local do Julgamento.\nObjetivo: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.\n\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Local  + \\beta_3  crime + u\\]\nAnálise de Fatores que Influenciam a Taxa de Feminicídio:\n\nVariável Dependente \\((Y)\\): Taxa de Feminicídio por 100.000 mulheres.\nVariáveis Independentes \\((X)\\): Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher.\nObjetivo: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.\n\n\\[Tx.feminicidio = \\beta_0 + \\beta_1 desemprego + \\beta_2 educ + \\beta_3 política + u \\]\nEm todos esses exemplos temos o que denominamos de variáveis qualitativas. No primeiro exemplo, gravidade e jurisdição não são quantificaveis. No segundo exemplo, o tipo de crime, e a localidade. Por fim, no último exemplo, a variável que indica ou não a presença de uma política pública de proteção a mulher também é qualitativa.\nFelizmente, isso não traz problemas adcionais a estimação de MQO. Pelo menos quando tais variáveis aparecem como variáveis explicativas. Mais a frente veremos o que acontece quando a variável dependente é do tipo qualitativa.\nDuas coisas mudam quando temos as variáveis qualitativas como regressores:\n\nEm primeiro lugar temos que organizar essas informações no banco de dados de modo coerente.\nEm segundo lugar, a introdução de variáveis qualitativas muda a interpretação dos resultados.\n\nFatores qualitativos geralmente aparecem na forma de informações binárias, também denominadas,dummy.\n\n\n\n\n\n\nVariável Binária ou Dummy:\n\n\n\nÉ uma variável que assume o valor 1 se a “condição occorre” e 0 caso o contrário.\n\n\nExemplos:\nO crime de feminicídio ocorreu na região central? * Variável Binária: crime_centro (exemplo de nome) * se sim, ela recebe o valor 1, * se não ocorreu a variável recebe o valor 0.\nNesse caso teremos uma coluna em nosso banco de dados denominada crime_centro, na qual a linha recebe o valor 1 caso o crime tenha ocorrido no centro e 0 caso o contrário.\nO crime foi considerado grave?\n\nVariável Binária: cri_grav (exemplo de nome)\nSe sim, recebe o valor 1,\ncaso contráriorecebe o valor 0.\n\nOutras binárias….\n\n1 se mulher e 0 se for homem.\n1 se for autodeclarado não-branco e 0 se for branco\n1 se trabalha e 0 se não trabalha\n1 se feminicídio e 0 se não for feminicídio.\n\nUtilizar 1 ou 0 é, de fato, um criterio arbitrário, mas essa escolha reside justamente na vantagem de se capturar informações importantes que tornam a interpretação dos resultados mais simples.\nAs observações que receberam 1 como valor serão comparadas com aquelas observações que ficaram com o valor 0. Essas observações seriam nosso “grupo de comparação/grupo base”.\n\n9.8.4.1 Uma única Variável Dummy Independente\nA forma mais simples de incorporar uma variável qualitativa e simplismente adicionarmos a variável binária na equação de regressão.\nConsidere o exemplo prático retirado do livro do Wooldridge. Queremos entender os determinantes dos salários. Obviamente, o exemplo é bastante simples, mas será útil para avaliarmos a questão.\n\\[salario_h = \\beta_0 + \\delta_0 feminino + \\beta_1 educ + \\beta_3 exper + \\beta_4perm + u \\] Nesse exemplo, estamos tentando entender os quais os fatores que afetam os salários dos indivíduos. O salário pode ser explicado pela educação, experência, tempo no cargo e pelo fato de ser mulher. A variável feminino assume o valor 1, se o individuo for do sexo feminino e 0, caso o contrário. Na equação \\(\\delta_0\\) mede a diferença no salário entre homems e mulheres, dado os mesmos anos de estudos, experiência e tempo no cargo, e evidentemente o mesmo \\(u\\).\n\nif(!require(wooldridge)){\n    install.packages(\"wooldridge\")\n    library(wooldridge)}\n\ndata(wage1, package= \"wooldridge\")\n\nwage_f&lt;-lm(wage ~ female+educ+exper+tenure, data=wage1)\nsummary(wage_f)\n\n\nCall:\nlm(formula = wage ~ female + educ + exper + tenure, data = wage1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7675 -1.8080 -0.4229  1.0467 14.0075 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.56794    0.72455  -2.164   0.0309 *  \nfemale      -1.81085    0.26483  -6.838 2.26e-11 ***\neduc         0.57150    0.04934  11.584  &lt; 2e-16 ***\nexper        0.02540    0.01157   2.195   0.0286 *  \ntenure       0.14101    0.02116   6.663 6.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.958 on 521 degrees of freedom\nMultiple R-squared:  0.3635,    Adjusted R-squared:  0.3587 \nF-statistic:  74.4 on 4 and 521 DF,  p-value: &lt; 2.2e-16\n\n\nO coeficiente estimado para a binária que identifica mulheres foi de -1,81 e significativa a 1%. Isso indica que em média, uma mulher ganha menos US$ 1,81 por hora do que um homem com a mesma educação, experiência e tempo no cargo.\nVeja a figura abaixo ela mostra o efeito da binária feminina. Note que para um mesmo nível de escolaridade, o salário do homem é maior do que o salário da mulher. A dummy “desloca” o intercepto.\n\n\n\nVariáveis binárias graficamente, fonte: SEmantic Scholar\n\n\nPodemos utilizar a binária para ver o efeito da experiência para homens e para mulheres. Podemos fazer isso criando uma variável que é a multiplicação entre experiência e feminino. Vejamos:\n\nwage1$exp_f&lt;-wage1$exper*wage1$female\n\n\nwage_f_2&lt;-lm(wage ~ educ+female+ exper+exp_f+tenure, data=wage1)\nsummary(wage_f_2)\n\n\nCall:\nlm(formula = wage ~ educ + female + exper + exp_f + tenure, data = wage1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3215 -1.6447 -0.4678  1.0431 13.8889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.27347    0.75952  -2.993 0.002891 ** \neduc         0.58954    0.04938  11.938  &lt; 2e-16 ***\nfemale      -0.87766    0.41570  -2.111 0.035223 *  \nexper        0.05720    0.01589   3.601 0.000348 ***\nexp_f       -0.05635    0.01944  -2.898 0.003908 ** \ntenure       0.12810    0.02148   5.964 4.56e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.937 on 520 degrees of freedom\nMultiple R-squared:  0.3737,    Adjusted R-squared:  0.3676 \nF-statistic: 62.04 on 5 and 520 DF,  p-value: &lt; 2.2e-16\n\nmean(wage1$wage)\n\n[1] 5.896103\n\n\nNesse novo modelo temos agora a binária de mulher, female, que mostra que o fato de ser mulher diminui o salário em -US$0,88. Veja que a média de salário é de US$ 5,9. A queda representa 15% a menos em relação ao homem.\nCom relação a multiplicação pela experiência, podemos assim analisar. Como a variável é 0 para homem, o efeito da experiência para os homens é o próprio coeficiente, exper. Para cada ano a mais de experiência o salário do homem aumenta em US$ 0,057. Para as mulheres é a soma do coeficiente, exper + exp_f. Para cada ao a mais da experência da mulher o mercado paga US$0,001 . Isso está representado no gráfico abaixo.\n\n\n\nVariáveis binárias multiplicativas graficamente, fonte: University of Washington\n\n\n\n\n\n9.8.5 O uso de Dummies para categorias multiplas e Interação entre a dummy e a variável dependente\nConsidere a equação abaixo considerando o mesmo banco de dados anterior:\n\\[log(salarioh) = \\beta_0 + \\beta_1  hsolteiro  +  \\beta_2 mcasadas  + \\beta_3 msolteiras + \\beta_4educ + \\beta_5exper + \\beta_6exper^2 + u\\] Agora vamos dividir entre homens e mulheres, casadas(os) solteiras(os). Note que definimos uma dummy para cada grupo e o grupo de comparação utilizaremos o grupo de homens casados.\nAs estimativas das dummies acima irão captar para cada grupo a diferença proporcional nos salarios-horas relativamente aos homens casados.\n\n###Aplicação de RLM multiplas dummies\n\ndata(wage1, package=\"wooldridge\")\n\n\n### criando as variáveis\nlibrary(\"dplyr\")\n\nwage1 &lt;- wage1 %&gt;%\n  mutate(hcasado = ifelse(female==0 & married==1, 1, 0))\n\nwage1 &lt;- wage1 %&gt;%\n  mutate(mcasada = ifelse(female==1 & married==1, 1, 0))\n\nwage1 &lt;- wage1 %&gt;%\n  mutate(msolteira = ifelse(female==1 & married==0, 1, 0))\n\nwage1 &lt;- wage1 %&gt;%\n  mutate(hsolteiro = ifelse(female==0 & married==0, 1, 0))\n\n\nstatus_civ &lt;- lm(log(wage)~  hsolteiro + msolteira + mcasada + educ+ exper  +I(exper^2)+tenure, data=wage1)\n\nsummary(status_civ)\n\n\nCall:\nlm(formula = log(wage) ~ hsolteiro + msolteira + mcasada + educ + \n    exper + I(exper^2) + tenure, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.89832 -0.24486 -0.03093  0.23483  1.11291 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.5370094  0.1070982   5.014 7.33e-07 ***\nhsolteiro   -0.2196913  0.0555033  -3.958 8.61e-05 ***\nmsolteira   -0.3266998  0.0502974  -6.495 1.95e-10 ***\nmcasada     -0.4146530  0.0459328  -9.027  &lt; 2e-16 ***\neduc         0.0795322  0.0067169  11.841  &lt; 2e-16 ***\nexper        0.0297116  0.0051097   5.815 1.06e-08 ***\nI(exper^2)  -0.0005919  0.0001081  -5.475 6.84e-08 ***\ntenure       0.0149328  0.0028460   5.247 2.26e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3949 on 518 degrees of freedom\nMultiple R-squared:  0.4553,    Adjusted R-squared:  0.448 \nF-statistic: 61.86 on 7 and 518 DF,  p-value: &lt; 2.2e-16\n\n\nComo temos um modelos log-nível, a interpretação é dada por \\(\\% \\Delta y = (100 \\beta_1) \\Delta x\\). Com base nas estimativas, verificamos que com relação aos homens casados:\n\nHomens solteiros recebem cerca de 22% a menos.\nMulheres solteiras recebem 32,6% a menos.\nMulheres casadas recebem 41,5% a menos.\n\nMantendo fixas educaçao, experiência e tempo no emprego.\nVariáveis ao Quadrado: interpretando\nNote que temos uma nova variável, a experência, ao quadrado. A variável ao quadrado tenta capturar a seguinte ideia: se cada ano a mais de experiência sempre gera o mesmo incremento de salário, ou se recebe cada vez menos por cada ano a mais de experiência. Temos agora uma mudança de interpretação\n\\[\\frac{\\% \\Delta y }{\\Delta x}= 100\\beta_5 +200\\beta_6exper  \\]\nCom base nas estimatvas:\n\\[\\frac{\\% \\Delta y }{\\Delta x}= 100\\times0,0297 +200\\times -0,000592 \\times exper\\]\n\\[\\frac{\\% \\Delta y }{\\Delta x}= 2,97 -0,118 \\times exper  \\] Vamos pensar em uma pessoa que tem 0 anos de experiência e adquire um ano. Logo:\n\\[\\frac{\\% \\Delta y }{1}= 2,97 -0,118 \\times 1 = 2,85\\% \\] Ela ira receber 2,85% a mais de salário devido a essa experiência adquirida. Vejamos agora uma pessoa com 9 anos e adquire mais um ano, vai para 10. Logo\n\\[\\frac{\\% \\Delta y }{1}= 2,97 -0,118 \\times 10 = 1,79\\% \\] Esse ano a mais agora adicionou apenas 1,79% no seu salário. Para finalizar vejamos de 24 para 25 anos de experiência\n\\[\\frac{\\% \\Delta y }{1}= 2,97 -0,118 \\times 25 = 0,02\\% \\] Adiciona praticamente zero. E a partir desse ponto a adição será negativa, mais experiência terá menor salário. Assim o ponto de mudança será:\n\\[ 2,97 -0,118 \\times exper = 0 \\implies exper=\\frac{2,97}{0,118}=25,16 \\] O salário vai aumentando até chegar a experiência de 25 anos, depois desse ponto o mercado tende a penalizar com salários menores quem tem mais experiência.\n\n\n9.8.6 Incorporando Informações Ordinais com o uso de Variáveis Dummy.\nVamos agora entender a criação de binárias para variáveis com várias categorias. Um exemplo clássico de variável com diversas categorias é a região onde mora o indivíduo. Podemos ver isso no nosso banco anterior chamado dados:\n\ntable(dados$regiao)\n\n\nCO  N NE  S SD \n 4  7  9  3  4 \n\n\nTemos que escolher uma região de comparação, por exemplo Sudeste. Assim, pode-se criar uma binária para o CO, uma para o N, uma para o NE e uma para o S. Veja o exemplo abaixo.\n\n### criando as variáveis\nlibrary(\"dplyr\")\n\ndados &lt;- dados %&gt;%\n  mutate(CO = ifelse(regiao==\"CO\", 1, 0))\n\ndados &lt;- dados %&gt;%\n  mutate(NE = ifelse(regiao==\"NE\", 1, 0))\n\ndados &lt;- dados %&gt;%\n  mutate(N = ifelse(regiao==\"N\", 1, 0))\n\ndados &lt;- dados %&gt;%\n  mutate(S = ifelse(regiao==\"S\", 1, 0))\n\nComo esse banco é muito pequeno, vamos retomar o banco o Wooldridge wage1. Neste banco já está criada a variável de região sendo a região leste a de comparação. Vamos adicionar na regressão as regiões, sul, centro-norte e oeste.\n\nreg_salar &lt;- lm(log(wage)~  northcen + west+ south + female+ educ+ exper +tenure, data=wage1)\n\nsummary(reg_salar)\n\n\nCall:\nlm(formula = log(wage) ~ northcen + west + south + female + educ + \n    exper + tenure, data = wage1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01490 -0.25600 -0.02259  0.25361  1.30569 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.534853   0.108019   4.951 9.98e-07 ***\nnorthcen    -0.064811   0.052512  -1.234  0.21768    \nwest         0.078528   0.058211   1.349  0.17792    \nsouth       -0.061050   0.049041  -1.245  0.21374    \nfemale      -0.306808   0.037148  -8.259 1.23e-15 ***\neduc         0.086823   0.006943  12.504  &lt; 2e-16 ***\nexper        0.004808   0.001622   2.963  0.00318 ** \ntenure       0.017148   0.002968   5.777 1.31e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4137 on 518 degrees of freedom\nMultiple R-squared:  0.4022,    Adjusted R-squared:  0.3941 \nF-statistic: 49.79 on 7 and 518 DF,  p-value: &lt; 2.2e-16\n\n\nObserva-se no modelo acima que não há diferenças regionais importantes nos salários, quando controlamos para mulheres, educação, experiência e tempo no trabalho.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelos Lineares</span>"
    ]
  },
  {
    "objectID": "outrosmodelos.html",
    "href": "outrosmodelos.html",
    "title": "10  Modelos Avançados",
    "section": "",
    "text": "10.1 Modelos com Variáveis Dependentes Qualitativas",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modelos Avançados</span>"
    ]
  },
  {
    "objectID": "outrosmodelos.html#modelos-com-variáveis-dependentes-qualitativas",
    "href": "outrosmodelos.html#modelos-com-variáveis-dependentes-qualitativas",
    "title": "10  Modelos Avançados",
    "section": "",
    "text": "10.1.1 Variáveis Dependentes Binárias\nExemplos de resultados binários:\n\nUm consumidor compra ou não compra um produto.\nUm juiz acata ou não um determinado pedido.\nUm indíviduo decide ou não se vai procurar emprego.\nUm homicídio feminino é classificado como feminicídio ou não.\n\nVariável dependente binária:\nA variável de resultado será, \\(y = 1\\), se o evento de interesse ocorre, caso o contrario \\(y=0\\). Os modelos de Variáveis Dependentes Limitadas são bastante difundidos nas análises empíricas nas ciências sociais.\n\n\n10.1.2 O Modelo de Probabilidade Linear\nO modelo será especificado como:\n\\[y= \\beta_0 + \\beta_1 x_1 + ... +\\beta_kx_k +u\\]\nO modelo acima estima a probabilidade do evento de interesse ocorrer dado as variáveis explicativas. Em notação matemática pode ser representado da seguinte maneira:\n\\[p=Pr(Y=1)=F(\\mathbf{x}'\\boldsymbol{\\beta})\\]\nNo modelo de probabilidade linear:\n\\[p=Pr(Y=1)=F(\\mathbf{x}'\\boldsymbol{\\beta})= \\beta_0 + \\beta_1 x_1 + ... +\\beta_kx_k \\]\nOs coeficientes \\(\\beta_j\\) estimados indicam o impacto na probabilidade de \\(Y=1\\). No modelo MPL a estimação dos coeficientes são feitas de maneira análoga ao modelo de RLM.\nVejamos um exemplo:\n\nEstudamos a probabilidade de uma mulher estar no mercado de trabalho \\(P(inlf=1)\\)\n\n\\[P(\\text{inlf}=1)= \\beta_0 + \\beta_1 \\text{nwifeinc} + \\beta_2 \\text{edu} +\\beta_3 \\text{exp}+\\] \\[\\beta_4 \\text{exp}^2+\\beta_5 \\text{age}+\\beta_6 \\text{kidslt6}+\\beta_7 \\text{kidsge6} +\\epsilon\\]\n\nDependendo da renda familiar adicional (nwifeinc), da educação (educ), da experiência (exper e exper^2), da sua idade (age), do número de crianças menores de 6 anos (kidslt6) e do número de crianças maiores do que 6 anos (kidsge6).\n\nO Script abaixo estima um modelo de probabilidade linear usando o conjunto de dados mroz.dta, da mesma forma que fizemos anteriormente (usando o lm). Vejamos:\n\nif(!require(wooldridge)){\n    install.packages(\"wooldridge\")\n    library(wooldridge)}\n\nWarning: pacote 'wooldridge' foi compilado no R versão 4.4.3\n\nlibrary(car)\nlibrary(lmtest)\n\ndata(mroz, package=\"wooldridge\")\n\n# Estimate linear probability model\nlinprob &lt;- lm(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6 ,data=mroz)\n\nsummary(linprob)\n\n\nCall:\nlm(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + age + \n    kidslt6 + kidsge6, data = mroz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93432 -0.37526  0.08833  0.34404  0.99417 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.5855192  0.1541780   3.798 0.000158 ***\nnwifeinc    -0.0034052  0.0014485  -2.351 0.018991 *  \neduc         0.0379953  0.0073760   5.151 3.32e-07 ***\nexper        0.0394924  0.0056727   6.962 7.38e-12 ***\nI(exper^2)  -0.0005963  0.0001848  -3.227 0.001306 ** \nage         -0.0160908  0.0024847  -6.476 1.71e-10 ***\nkidslt6     -0.2618105  0.0335058  -7.814 1.89e-14 ***\nkidsge6      0.0130122  0.0131960   0.986 0.324415    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4271 on 745 degrees of freedom\nMultiple R-squared:  0.2642,    Adjusted R-squared:  0.2573 \nF-statistic: 38.22 on 7 and 745 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretação\nO coeficiente estimado da educação (educ) pode ser interpretado como: um ano adicional de escolaridade aumenta a probabilidade de uma mulher estar na força de trabalho, tudo o mais constante, em 0,038, em média.\nO Problema\nUm problema com o MPL é que \\(P(y = 1|x)\\) é especificado como uma função linear dos regressores. Por construção, existem combinações (mais ou menos realistas) de valores de regressores que produzem \\(\\hat{y} &lt; 0\\) ou \\(\\hat{y}&gt;0\\). Como são probabilidades, isso realmente não faz sentido.\nVeja que os valores previstos para duas mulheres:\n\nMULHER 1: renda familiar adicional de 50 mil ano, 7 anos de escolaridade, não tem experiência, com 25 anos e possui 2 filhos menores de 6 anos. A probabilidade de estar na força de trabalho estimada é -24%….não faz sentido.\n\n\n# Podemos calcular assim a probabilidade \n\n0.5855192 - 0.0034052*50  + 0.0379953*7  + 0.0394924*0  -0.0005963*0  - 0.0160908*25  -0.2618105*2  + 0.0130122*0\n\n[1] -0.2446647\n\n# Ou assim: \n\nxpred &lt;- list(nwifeinc=c(50),educ=c(7),exper=c(0), \"I(exper^2)\"=c(0),\n age=c(25),kidslt6=c(2),kidsge6=c(0))\n\npredict(linprob,xpred)\n\n         1 \n-0.2446632 \n\n\n\nMULHER 2: Não tem renda adicional, 17 anos de escolaridade, 30 anos de experiência, com 52 anos e não possui filhos. A probabilidade de estar na força de trabalho estimada é 104%….não faz sentido. Veja os cálculos abaixo:\n\n\n# Podemos calcular assim a probabilidade \n\n0.5855192 - 0.0034052*0  + 0.0379953*17  + 0.0394924*30  -0.0005963*900  - 0.0160908*52  -0.2618105*0  + 0.0130122*0\n\n[1] 1.04282\n\n# Ou assim\n\nxpred1 &lt;- list(nwifeinc=c(0),educ=c(17), exper=c(30), \"I(exper^2)\"=c(30),\n age=c(52),kidslt6=c(0),kidsge6=c(0))\n\npredict(linprob,xpred1)\n\n       1 \n1.042808 \n\n\n\n\n10.1.3 Modelo de Regressão Logistica: Logit\nNo modelo Logit a função linear é substituida pela função de distribuição acumulada logistica:\n\\[ Pr(Y_i=1)=F(\\mathbf{x}'\\boldsymbol{\\beta})=\\text{Logit}^{-1}(\\mathbf{x}'\\boldsymbol{\\beta})\\]\nOu\n\\[\\text{Logit}^{-1}(\\mathbf{x}'\\boldsymbol{\\beta}) = \\frac{\\exp(\\mathbf{x}'\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{x}'\\boldsymbol{\\beta})}\n\\ \\]\nAssumindo que:\n\\[log \\left( \\frac{P_t}{1-P_t}\\right)=\\mathbf{X}`\\beta\\]\nResolvendo para \\(P_t\\), tem-se que:\n\\[ Pr(Y_i=1)=F(\\mathbf{x}'\\boldsymbol{\\beta}) = \\frac{\\exp(\\mathbf{x}'\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{x}'\\boldsymbol{\\beta})}\\]\nPortanto, o resultado do modelo logístico fornece o efeito da variação de uma unidade em \\(X\\) no log da razão de chances entre as probabilidades \\(Y=1\\) e \\(Y=0\\). Infelizmente a interpretação direta não nos traz uma leitura significativa do que está acontecendo nessa população.\nDuas maneiras para analisarmos o resultados são:\nRazão de Chance\nA razão de chances ou risco relativo, \\(p/(1-p)\\), mede a probabilidade de \\(y=1\\) em relação à probabilidade de \\(y=0\\).\nUma razão de chances de 2 significa que o resultado \\(y=1\\) (de ganhar uma aposta, por exemplo) é duas vezes mais provável que o resultado de \\(y=0\\) (de perder uma aposta).\nA razão de chances é dada por:\n\\[\\frac{p}{1 - p} = \\exp(\\mathbf{x}' \\boldsymbol{\\beta})\\]\nEfeito Marginal:\nOutra maneira é encontrar o efeito marginal do estimador, que irá fornecer a seguinte interpretação, um aumento em \\(X\\) aumenta(diminui) em \\(\\beta\\) pontos percentuais a probabilidade de ser \\(Y=1\\). Vejamos os exemplos abaixo para ficar um pouco mais claro.\n\n10.1.3.1 Exemplo e interpretação\nPrimeiramente vamos estimar o modelo logit. Usamos a mesma base que indica se a mulher está na força de trabalho \\((Y=1)\\) ou se a mulher está fora da força de trabalho \\((Y=0)\\), variável inlf.\n\ndata(mroz, package=\"wooldridge\")\n\n#Modelo logit, familia da binomial = logit\nlogit &lt;- glm(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6 , family=binomial(link=\"logit\"), data=mroz)\nsummary(logit)\n\n\nCall:\nglm(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + age + \n    kidslt6 + kidsge6, family = binomial(link = \"logit\"), data = mroz)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.425452   0.860365   0.495  0.62095    \nnwifeinc    -0.021345   0.008421  -2.535  0.01126 *  \neduc         0.221170   0.043439   5.091 3.55e-07 ***\nexper        0.205870   0.032057   6.422 1.34e-10 ***\nI(exper^2)  -0.003154   0.001016  -3.104  0.00191 ** \nage         -0.088024   0.014573  -6.040 1.54e-09 ***\nkidslt6     -1.443354   0.203583  -7.090 1.34e-12 ***\nkidsge6      0.060112   0.074789   0.804  0.42154    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1029.75  on 752  degrees of freedom\nResidual deviance:  803.53  on 745  degrees of freedom\nAIC: 819.53\n\nNumber of Fisher Scoring iterations: 4\n\n\nA coluna Estimate mostra os coeficientes na forma de log da razão de chance. Quando a educação da mulher aumenta em uma unidade, a mudança esperada no log da razão de chance é de 0,22. Pode-se interpretar se os efeitos são positivos ou negativos, mas sua análise não é significativa. Aqui sabe-se que a educação tem efeito positivo na participação da mulher no mercado de trabalho, mas não sabemos de quanto!\nRazão de Chance\nVamos agora calcular a razão de chance. Utilizaremos o pacote mfx\n\n# Usando o pacote mfx\nif(!require(mfx)){\n    install.packages(\"mfx\")\n    library(mfx)}\n\n\nlogitor(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6, data=mroz)\n\nCall:\nlogitor(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + \n    age + kidslt6 + kidsge6, data = mroz)\n\nOdds Ratio:\n           OddsRatio Std. Err.       z     P&gt;|z|    \nnwifeinc   0.9788810 0.0082435 -2.5346  0.011256 *  \neduc       1.2475360 0.0541921  5.0915 3.553e-07 ***\nexper      1.2285929 0.0393847  6.4220 1.345e-10 ***\nI(exper^2) 0.9968509 0.0010129 -3.1041  0.001909 ** \nage        0.9157386 0.0133450 -6.0403 1.538e-09 ***\nkidslt6    0.2361344 0.0480729 -7.0898 1.343e-12 ***\nkidsge6    1.0619557 0.0794229  0.8038  0.421539    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# OU\n\nlogit.or=exp(coef(logit))\nlogit.or\n\n(Intercept)    nwifeinc        educ       exper  I(exper^2)         age \n  1.5302825   0.9788810   1.2475360   1.2285929   0.9968509   0.9157386 \n    kidslt6     kidsge6 \n  0.2361344   1.0619557 \n\n\nInterpretação\nQuando a educação da mulher aumenta em uma unidade, as chances de y = 1 (dela participar do mercado de trabalho) aumentam em 24% ((1,247-1)*100). Ou a probabilidade de participar do mercado de trabalho (y = 1) é 1,24 vezes maior quando a educação aumenta em uma unidade (mantendo todas as outras variáveis constante).\nEfeito Marginal\nUtilizando o mesmo pacote anterior mfx, tem-se:\n\nlogitmfx(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6, data=mroz)\n\nCall:\nlogitmfx(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + \n    age + kidslt6 + kidsge6, data = mroz)\n\nMarginal Effects:\n                 dF/dx   Std. Err.       z     P&gt;|z|    \nnwifeinc   -0.00519005  0.00204820 -2.5340  0.011278 *  \neduc        0.05377731  0.01056074  5.0922 3.539e-07 ***\nexper       0.05005693  0.00782462  6.3974 1.581e-10 ***\nI(exper^2) -0.00076692  0.00024768 -3.0965  0.001959 ** \nage        -0.02140302  0.00353973 -6.0465 1.480e-09 ***\nkidslt6    -0.35094982  0.04963897 -7.0700 1.549e-12 ***\nkidsge6     0.01461621  0.01818832  0.8036  0.421625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretação\nOs efeitos marginais mostram a mudança na probabilidade quando \\(X\\) aumenta em uma unidade. No nosso caso o aumento de 1 ano na educação da mulher aumenta em 0.053 pontos percentuais a probabilidade de a mulher participar do mercado de trabalho, \\(Pr(Y=1)\\). A probabilidade aqui é um valor entre 0 e 1.\nCalculando a Probabilidade\nVamos calcular a probabilidade de uma mulher participar do mercado de trabalho. Predizendo a probabilidade:\n\nlogit &lt;- glm(inlf~ nwifeinc + educ + exper + expersq + \n                  age+ kidslt6 + kidsge6, family=binomial(link=\"logit\"), data=mroz)\n\n# temos que fazer o inverso do modelo logit\n\ninvlogit=function (x) {1/(1+exp(-x))}\ninvlogit(coef(logit)[1]+\n           coef(logit)[2]*mean(mroz$nwifeinc)+\n           coef(logit)[3]*mean(mroz$educ)+\n           coef(logit)[4]*mean(mroz$exper)+\n           coef(logit)[5]*mean(mroz$expersq)+\n           coef(logit)[6]*mean(mroz$age)+\n           coef(logit)[7]*mean(mroz$kidslt6)+\n           coef(logit)[8]*mean(mroz$kidsge6))\n\n(Intercept) \n   0.582772 \n\n\nEm média 58% das mulheres participam do mercado de trabalho. Vejamos agora para o caso das duas mulheres que haviamos feito anteriormente:\n\nMULHER 1: renda familiar adicional de 50 mil ano, 7 anos de escolaridade, não tem experiência, com 25 anos e possui 2 filhos menores de 6 anos.\n\n\n# Mulher 1\ninvlogit=function (x) {1/(1+exp(-x))}\ninvlogit(coef(logit)[1]+\n           coef(logit)[2]*50+\n           coef(logit)[3]*7+\n           coef(logit)[4]*0+\n           coef(logit)[5]*0+\n           coef(logit)[6]*25+\n           coef(logit)[7]*2+\n           coef(logit)[8]*0)\n\n(Intercept) \n 0.01505417 \n\n\nA chance da mulher descrita acima participar do mercado de trabalho é de 1,5%.\n\nMULHER 2: Não tem renda adicional, 17 anos de escolaridade, 30 anos de experiência, com 52 anos e não possui filhos.\n\n\n# Mulher 1\ninvlogit=function (x) {1/(1+exp(-x))}\ninvlogit(coef(logit)[1]+\n           coef(logit)[2]*0+\n           coef(logit)[3]*17+\n           coef(logit)[4]*30+\n           coef(logit)[5]*900+\n           coef(logit)[6]*52+\n           coef(logit)[7]*0+\n           coef(logit)[8]*0)\n\n(Intercept) \n  0.9500491 \n\n\nA chance da Mulher 2 participar do mercado de trabalho é de 95% .\nGraficamente\nVejamos agora graficamente o efeito dos anos de estudo sobre a probabilidade de a mulher entrar no mercado de trabalho. Vamos manter as variáveis na média e vamos simular mulheres de 5 a 17 anos de estudo e estimar a probabilidde de ela participar da força de trabalho.\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)}\n\nlogit &lt;- glm(inlf~ nwifeinc + educ + exper + expersq + \n                  age+ kidslt6 + kidsge6, family=binomial(link=\"logit\"), data=mroz)\n\namp_educ&lt;-range(mroz$edu) \n\nedu_sequence &lt;- seq(from = 5, to = 17, by = .02) # 177 points along x axis\n\n\nconstanteX &lt;- with(mroz, data.frame(nwifeinc= mean(nwifeinc),\n                                  exper= mean(exper),\n                                  expersq= mean(expersq),\n                                  age= mean(age),\n                                  kidslt6= mean( kidslt6),\n                                  kidsge6= mean(kidsge6),\n                                  educ = edu_sequence \n                                  ))\n\nconstanteX$predicao &lt;- predict(object = logit, \n                                newdata = constanteX, \n                                type = \"response\")\n\np&lt;- ggplot(constanteX, aes(x = educ, y = predicao)) +\n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\np + labs(title = \"Probabilidade de Participação no Mercado de Trabalho\", x = \"Anos de Estudo\", y = \"Prob de Participação\")\n\n\n\n\n\n\n\n\n\n\n10.1.3.2 Probit\nUma alternativa ao modelo logit seria o uso do modelo probit. Que ao invés da função logística utiliza-se a função distribuição normal padrão. Vejamos os resultados:\n\ndata(mroz, package=\"wooldridge\")\n\n#Modelo logit, familia da binomial = logit\nlogit &lt;- glm(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6 , family=binomial(link=\"logit\"), data=mroz)\n\nprobit &lt;- glm(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6 , family=binomial(link=\"probit\"), data=mroz)\n\n\nlibrary(stargazer)\n\nstargazer(logit,probit, type=\"text\",keep.stat=\"n\")\n\n\n=========================================\n                 Dependent variable:     \n             ----------------------------\n                         inlf            \n                logistic       probit    \n                  (1)            (2)     \n-----------------------------------------\nnwifeinc        -0.021**      -0.012**   \n                (0.008)        (0.005)   \n                                         \neduc            0.221***      0.131***   \n                (0.043)        (0.025)   \n                                         \nexper           0.206***      0.123***   \n                (0.032)        (0.019)   \n                                         \nI(exper2)      -0.003***      -0.002***  \n                (0.001)        (0.001)   \n                                         \nage            -0.088***      -0.053***  \n                (0.015)        (0.008)   \n                                         \nkidslt6        -1.443***      -0.868***  \n                (0.204)        (0.118)   \n                                         \nkidsge6          0.060          0.036    \n                (0.075)        (0.044)   \n                                         \nConstant         0.425          0.270    \n                (0.860)        (0.508)   \n                                         \n-----------------------------------------\nObservations      753            753     \n=========================================\nNote:         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nObserve que sinal e significância estão em acordo entre os dois modelos. Vejamos agora os efeitos marginais para podermos comparar.\n\nlogitmfx(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6, data=mroz)\n\nCall:\nlogitmfx(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + \n    age + kidslt6 + kidsge6, data = mroz)\n\nMarginal Effects:\n                 dF/dx   Std. Err.       z     P&gt;|z|    \nnwifeinc   -0.00519005  0.00204820 -2.5340  0.011278 *  \neduc        0.05377731  0.01056074  5.0922 3.539e-07 ***\nexper       0.05005693  0.00782462  6.3974 1.581e-10 ***\nI(exper^2) -0.00076692  0.00024768 -3.0965  0.001959 ** \nage        -0.02140302  0.00353973 -6.0465 1.480e-09 ***\nkidslt6    -0.35094982  0.04963897 -7.0700 1.549e-12 ***\nkidsge6     0.01461621  0.01818832  0.8036  0.421625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nprobitmfx(inlf~ nwifeinc + educ + exper + I(exper^2) + \n                  age+ kidslt6 + kidsge6, data=mroz)\n\nCall:\nprobitmfx(formula = inlf ~ nwifeinc + educ + exper + I(exper^2) + \n    age + kidslt6 + kidsge6, data = mroz)\n\nMarginal Effects:\n                 dF/dx   Std. Err.       z     P&gt;|z|    \nnwifeinc   -0.00469619  0.00192965 -2.4337  0.014945 *  \neduc        0.05112843  0.00992310  5.1525 2.571e-07 ***\nexper       0.04817690  0.00734505  6.5591 5.413e-11 ***\nI(exper^2) -0.00073705  0.00023464 -3.1412  0.001683 ** \nage        -0.02064309  0.00330485 -6.2463 4.203e-10 ***\nkidslt6    -0.33914996  0.04634765 -7.3175 2.526e-13 ***\nkidsge6     0.01406306  0.01719895  0.8177  0.413546    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretando\nO aumento de 1 ano na educação da mulher aumenta em 0.053 pontos percentuais a probabilidade de a mulher participar do mercado de trabalho, \\(Pr(Y=1)\\), no modelo logit e 0.051 no modelo probit. As diferenças são pequenas entre os dois modelos.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modelos Avançados</span>"
    ]
  },
  {
    "objectID": "outrosmodelos.html#dados-de-contagem-modelo-de-regressão-de-poisson",
    "href": "outrosmodelos.html#dados-de-contagem-modelo-de-regressão-de-poisson",
    "title": "10  Modelos Avançados",
    "section": "10.2 Dados de Contagem: Modelo de Regressão de Poisson",
    "text": "10.2 Dados de Contagem: Modelo de Regressão de Poisson\nEm vez de apenas dados binários codificados em \\(0/1\\), os dados de contagem podem assumir qualquer número inteiro não negativo $0,1,2,. . $.\nSe considerarem números muito grandes (como o número de alunos numa escola, número de dias de duração de um processo), podem ser aproximados por variáveis contínuas em modelos lineares e estimados utilizando MQO. Se os números forem relativamente pequenos (como o número de filhos de uma mãe), esta aproximação pode não funcionar bem. Por exemplo, os valores previstos podem tornar-se negativos.\nO modelo de regressão de Poisson é o modelo mais básico e conveniente projetado explicitamente para dados de contagem. A probabilidade de y assumir qualquer valor \\(h \\in \\{0,1,2,..\\}\\) e o modelo pode ser escrito da seguinte maneira:\n\\[P(y = h \\mid x) = \\frac{e^{-e^{\\mathbf{x} \\boldsymbol{\\beta}}} (e^{\\mathbf{x} \\boldsymbol{\\beta}})^h}{h!}\\]\nOs parâmetros do modelo de Poisson são muito mais fáceis de interpretar do que os de um modelo probit ou logit. Neste modelo, a média condicional de y é\n\\[E(y \\mid x) = e^{\\mathbf{x} \\boldsymbol{\\beta}}\\]\nentão cada parâmetro de inclinação \\(\\beta_j\\) tem a interpretação de uma semielasticidade:\n\\[\\frac{\\partial E(y \\mid x)}{\\partial x_j} = \\beta_j \\cdot e^{\\mathbf{x} \\boldsymbol{\\beta}} = \\beta_j \\cdot E(y \\mid x)\\]\nLogo,\n\\[\\beta_j = \\frac{1}{E(y \\mid x)} \\cdot \\frac{\\partial E(y \\mid x)}{\\partial x_j}\\]\n\\[\\beta_j \\Delta{X}= \\frac{\\Delta E(y \\mid x)}{E(y \\mid x)}=E(y \\mid x)\\%\\] Interpretação\nSe \\(x_j\\) aumentar em uma unidade (tudo mais constante), \\(E(y|x)\\) aumentará aproximadamente em \\(100 \\cdot \\beta_j\\) por cento (o valor exato é novamente \\(100 \\cdot (e^{\\beta_j} - 1)\\)).\n\n10.2.1 Exemplo:\nEstimar modelos de regressão de Poisson em R é simples. Eles também pertencem à classe dos modelos lineares generalizados (GLM) e podem ser estimados usando glm. A opção para especificar um modelo de Poisson é family=poisson.\nVejamos um exemplo que analisa o número de prisões masculinas em 1986. Estamos estudando o número de vezes que um homem foi preso no ano de 1986 - narr86. As variáveis independentes são:\n\npcnv: proporção de condenações anteriores\navgsen: comprimento médio da sentença.\ntottime: tempo de prisão desde os 18 (mês)\nptime86: mos. na prisão em 1986\nqemp86: trimestres empregados, 1986\ninc86: renda legal, 1986, US$ 100\nblack: =1 se preto\nhispn: =1 se for hispânico\nborn60: =1 se nasceu em 1960\n\nPara ajudar a compreender a diferença entre a interpretação do modelo de regressão de Poisson e o modelo linerar, vamos estimar os dois abaixo:\n\ndata(crime1, package=\"wooldridge\")\n\n#Estimando o modelo de Poisson\nPoisson.res &lt;- glm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+\nblack+hispan+born60, data=crime1, family=poisson)\n\n#estimando o modelo OLS\n\nlm.res &lt;- lm(narr86~pcnv+avgsen+tottime+ptime86+qemp86+inc86+\nblack+hispan+born60, data=crime1)\n\n\nlibrary(stargazer)\n\nstargazer(Poisson.res,lm.res, type=\"text\",keep.stat=\"n\")\n\n\n=========================================\n                 Dependent variable:     \n             ----------------------------\n                        narr86           \n                Poisson          OLS     \n                  (1)            (2)     \n-----------------------------------------\npcnv           -0.402***      -0.132***  \n                (0.085)        (0.040)   \n                                         \navgsen           -0.024        -0.011    \n                (0.020)        (0.012)   \n                                         \ntottime          0.024*         0.012    \n                (0.015)        (0.009)   \n                                         \nptime86        -0.099***      -0.041***  \n                (0.021)        (0.009)   \n                                         \nqemp86           -0.038       -0.051***  \n                (0.029)        (0.014)   \n                                         \ninc86          -0.008***      -0.001***  \n                (0.001)       (0.0003)   \n                                         \nblack           0.661***      0.327***   \n                (0.074)        (0.045)   \n                                         \nhispan          0.500***      0.194***   \n                (0.074)        (0.040)   \n                                         \nborn60           -0.051        -0.022    \n                (0.064)        (0.033)   \n                                         \nConstant       -0.600***      0.577***   \n                (0.067)        (0.038)   \n                                         \n-----------------------------------------\nObservations     2,725          2,725    \n=========================================\nNote:         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nInterpretando\n{No modelo de OLS}{.underline} um aumento de 0.10 na proporção de condenação reduz o número de prisões em 0.13 unidades.\n{No modelo de Poisson}{.underline} um aumento de 0.10 na proporção de condenação reduz em \\(0,402 \\cdot 0,1 \\cdot 100=4,02\\%\\). Considerando a cor da pele, aquelas que declaram ter pele preta possuem 66% mais chance de serem presas.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modelos Avançados</span>"
    ]
  },
  {
    "objectID": "outrosmodelos.html#modelo-com-dados-em-painel-modelos-de-efeitos-fixos",
    "href": "outrosmodelos.html#modelo-com-dados-em-painel-modelos-de-efeitos-fixos",
    "title": "10  Modelos Avançados",
    "section": "10.3 Modelo com Dados em Painel: Modelos de Efeitos Fixos",
    "text": "10.3 Modelo com Dados em Painel: Modelos de Efeitos Fixos\nDados em painel são extremamente importantes na análise econométrica porque permitem o estudo de variáveis ao longo do tempo e entre diferentes indivíduos ou entidades, como empresas, países ou varas. Esse tipo de dados oferece mais informações, variabilidade e eficiência do que séries temporais ou dados cross-section isoladamente, permitindo uma melhor compreensão das dinâmicas subjacentes e das relações causais.\nO estimador de efeitos fixos é uma técnica crucial para lidar com o viés de heterogeneidade, que surge quando características não observáveis e invariantes no tempo influenciam as variáveis de interesse. Ao considerar apenas as variações dentro de cada indivíduo ao longo do tempo, o estimador de efeitos fixos controla essas características não observáveis, eliminando seu impacto sobre os estimadores dos coeficientes. Dessa forma, permite uma estimativa não enviesada dos efeitos das variáveis explicativas, proporcionando resultados mais robustos e confiáveis.\nConsidere o modelo mais simples de T períodos.\n\\[y_{it} = \\beta_0 + \\beta_1 x_{it1} + \\beta_2 x_{it2} + \\cdots + \\beta_k x_{itk} + a_i + u_{it}\\]\nNote que temos agora 3 subscritos, \\(i\\),\\(t\\) e \\(k\\). O subscrito \\(i\\) é a unidade de corte transversal (o município, a vara, o processo), \\(t\\) representa o período dessa variável, e \\(k\\) a variável que estamos obversando. O termo \\(a_i\\) é denominado efeito fixo, pois é invariante no tempo. Ele capta uma série de fatores individuais não observados que impactam \\(Y\\) mas que são invariantes no tempo.\nSe aplicarmos o método de MQO na equação acima, teremos estimadores viesados e inconsistentes. Logo, precisamos remover esse efeito fixo. Uma forma de resolver isso é subtrair a média de cada variável para cada individuo ao longo do tempo. É então aplicar o MQO. Como \\(a_i\\) é invariante sua média também é \\(a_i\\), e ao realizarmos a subtração removeremos o efeito fixo.\n\\[\\bar{y_i} = \\beta_0 + \\beta_1 \\bar{x}_{i1} + \\cdots + \\beta_k \\bar{x}_{ik} + a_i + \\bar{u}_i\\]\nO modelo transfromado é dado por:\n\\[\\ddot{y}_{it} = y_{it} - \\bar{y}_i = \\beta_1 \\ddot{x}_{it1} + \\cdots + \\beta_k \\ddot{x}_{itk} + \\ddot{u}_{it}\\] ### Exemplo:\nVamos utilizar para exemplificar um modelo simples de dois períodos. Estamos interessados em analisar o efeito do desemprego (unem) sobre a criminalidade (crmrte) em 46 cidades para os anos de 1982 e 1987. Usaremos o pacote plm\n\nif(!require(plm)){\n    install.packages(\"plm\")\n    library(plm)}\n\nCarregando pacotes exigidos: plm\n\nlibrary(plm)\ndata(crime2, package=\"wooldridge\")\ncrime2.p &lt;- pdata.frame(crime2, index=46 )\n\n##Estimando o modelo\nPainel&lt;- plm(crmrte~unem +  d87, data=crime2.p, model=\"within\")\n\nsummary(Painel)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = crmrte ~ unem + d87, data = crime2.p, model = \"within\")\n\nBalanced Panel: n = 46, T = 2, N = 92\n\nResiduals:\n   Min. 1st Qu.  Median 3rd Qu.    Max. \n-26.458  -6.384   0.000   6.384  26.458 \n\nCoefficients:\n     Estimate Std. Error t-value Pr(&gt;|t|)   \nunem  2.21800    0.87787  2.5266  0.01519 * \nd87  15.40220    4.70212  3.2756  0.00206 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    11002\nResidual Sum of Squares: 8844.8\nR-Squared:      0.19606\nAdj. R-Squared: -0.66269\nF-statistic: 5.36528 on 2 and 44 DF, p-value: 0.0082206\n\n\nObserva-se nesse modelo que o aumento de 1 na taxa de desemprego aumenta em 2.2 pontos a taxa de criminalidade. Observa-se que as características dos munícipios que são fixas no tempo estão sendo controladas.\n\n##Estimando o modelo\nPainelln&lt;- plm(lcrmrte~I(log(unem)) +  d87, data=crime2.p, model=\"within\")\n\nsummary(Painelln)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = lcrmrte ~ I(log(unem)) + d87, data = crime2.p, \n    model = \"within\")\n\nBalanced Panel: n = 46, T = 2, N = 92\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-2.3912e-01 -6.9878e-02  1.8648e-17  6.9878e-02  2.3912e-01 \n\nCoefficients:\n             Estimate Std. Error t-value Pr(&gt;|t|)  \nI(log(unem)) 0.164799   0.071263  2.3125  0.02549 *\nd87          0.121119   0.045288  2.6744  0.01047 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    0.84604\nResidual Sum of Squares: 0.72674\nR-Squared:      0.141\nAdj. R-Squared: -0.77656\nF-statistic: 3.61129 on 2 and 44 DF, p-value: 0.035303\n\n\nUsando a taxa de crime e desemprego em ln, um aumento de 1% no desemprego aumenta em 0,16% a taxa de crime nos municípios.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modelos Avançados</span>"
    ]
  }
]