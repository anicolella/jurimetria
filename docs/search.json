[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jurimetria",
    "section": "",
    "text": "1 Prefácio"
  },
  {
    "objectID": "index.html#nossa-motivação",
    "href": "index.html#nossa-motivação",
    "title": "Jurimetria",
    "section": "1.1 Nossa Motivação:",
    "text": "1.1 Nossa Motivação:\nEsse curso foi criado pensando em profissionais com formação em direito, estatística, tecnologia da informação e demais profissionais que atuam tanto no judiciário quanto em tribunais administrativos. A ideia é colocar juntas diversas visões sobre um mesmo problema para buscarmos soluções mais criativas e eficicientes para as desigualdades nacionais."
  },
  {
    "objectID": "index.html#nosso-objetivo",
    "href": "index.html#nosso-objetivo",
    "title": "Jurimetria",
    "section": "1.2 Nosso Objetivo:",
    "text": "1.2 Nosso Objetivo:\nÉ oferecer aos participantes uma abordagem probabilística do direito aplicado, tendo como foco os casos práticos e discutindo questões relativas ao direito e desigualdades. Especificamente, objetiva-se ensinar um conjunto de métodos quantitativos e oferecer ferramentas para coleta, transformação e análise de dados jurídicos disponibilizados nas páginas dos tribunais de justiça."
  },
  {
    "objectID": "index.html#nosso-problema",
    "href": "index.html#nosso-problema",
    "title": "Jurimetria",
    "section": "1.3 Nosso Problema:",
    "text": "1.3 Nosso Problema:\nTeremos como tema de fundo nesse curso os processos de homicídio feminino. Especificamente queremos compreender qual o perfil e os determinantes do feminicidio no Estado de São Paulo."
  },
  {
    "objectID": "summary.html#resumo",
    "href": "summary.html#resumo",
    "title": "3  Sumário do Curso",
    "section": "3.1 Resumo:",
    "text": "3.1 Resumo:\nA pesquisa no direito e a ciência de dados: introdução, relevância e experiências; Coleta, transformação e estruturação de dados processuais com R; Estatística descritiva; Aprendizado estatístico (regressão linear, regressão logística); Aprendizado estatístico (machine learning); Interpretação dos resultados e elaboração de relatório."
  },
  {
    "objectID": "summary.html#plano-detalhado-das-aulas",
    "href": "summary.html#plano-detalhado-das-aulas",
    "title": "3  Sumário do Curso",
    "section": "3.2 Plano detalhado das aulas:",
    "text": "3.2 Plano detalhado das aulas:\n\nA pesquisa no direito e a ciência de dados: introdução, relevância e experiências - 25/04\nColeta, transformação e estruturação de dados processuais com R - 02/05\nColeta, transformação e estruturação de dados processuais com R - 09/05\nAnálise descritiva e visualização de dados processuais - 16/05\nModelos lineares aplicados a pesquisa em direito – regressão linear - 23/05\nModelos para diferentes tipos de estrutura de dados e variáveis. - 06/06\nAprendizado estatístico (machine learning) - 13/06\nAprendizado estatístico (machine learning) - 20/06\nInterpretação dos resultados e elaboração do relatório - 27/06"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introdução ao Feminicídio",
    "section": "",
    "text": "Nessa seção pensei em falarmos um pouco no no caso referências etc…."
  },
  {
    "objectID": "lineares.html",
    "href": "lineares.html",
    "title": "7  Modelos Lineares",
    "section": "",
    "text": "8 Regressão Linear Simples - RLS\nVamos iniciar o estudo de modelos lineares começando pela Regresão Linear Simples (RLS). Mais específicamente, vamos estudar a RLS no contexto de dados de corte transversal. Tal abordagem, segmentada por tipos de dados, facilíta o entendimento das hipóteses do modelo.\nEntender como funciona o modelo RLS é a base para entender como funciona outras extensões desse tipo de modelo, tal como o modelo de regressão linear multipla (RLM), amplamente utilizado em trabalhos empíricos.\nOs conceitos estatísticos aplicados no estudo de RLS são os mesmos apresentados na seção anterior.\nO matérial desta seção é baseado na 4ed do livro de Jeffrey Wooldridge, Introdução à Econometria: Uma Abordagem Moderna, de 2013. Embora o título do livro remeta à econometria, os modelos apresentados no livro são aplicaveis à diversas Ciências Sociais, tais como o Direito, Ciência Política, Sociologia, Pscologia Empírica etc…\nA análise de Regressão é um instrumento poderoso para os centistas sociais. Grosso modo, como veremos abaixo, ela nos permite simular, ao menos parcialmente, aplicações de caráter quase experimental. Podemos verificar como determinadas variáveis importantes nas ciencias sociais interagem com outras variáveis, as vezes em uma relação de causa e efeito.\nA RLM é uma extensão natural da RLS. Entretanto, ao invés de termos apenas uma variável explicativa podemos ter \\(k\\) variáveis dependentes\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\ldots + \\beta_k x_k + u\\] A hípótese fundamental é a de que:\n\\[E(u|x_1, . . . ,x_k) = 0\\]\nQualquer problema que faça qualquer um dos \\(x_1, ...., x_k\\) ser correlacionado com \\(u\\) invalida a hipótese acima. Tal hipótese implica em não viés do MQO."
  },
  {
    "objectID": "lineares.html#introdução",
    "href": "lineares.html#introdução",
    "title": "7  Modelos Lineares",
    "section": "8.1 Introdução",
    "text": "8.1 Introdução\nConsidere o modelo abaixo\n\\[y = \\beta_0 + \\beta_1x + u \\] Nesta equação, \\(y\\) é a variavel dependente ou também denominada de variável explicativa; \\(x\\) é a variável explicativa e \\(u\\) é o termo de erro. Essa equação é uma equação de regressão linear simples.\nConsidere um exemplo mais concreto de uma equação de regressão linear simples. Sumponha que gostariamos de explicar a taxa de crimes nos bairros de uma cidade, dado os níveis de desemprego na localidade. Nosso modelo de regressão poderia ser específicado da seguinte maneira:\n\\[crime_i = \\beta_0 + \\beta_1Desemprego_i + u\\] O subscrito \\(i\\) se refere a um bairro hipotético da cidade. Note que nesse caso,\\(i\\) é o subscrito que relaciona nossas unidades de observação (bairros). Nossas unidades de observação podem variar a depender do contexto em estudo: países, cidades, bairros, estados. No caso das aplicações em Jurimetria, por exemplo, podemos ter como unidade de observação varas onde tramitam os processos.\nNote que, novamente o termo de erro, \\(u\\), está presente na equação. O termo de erro, não observado, capta tudo aquilo que afeta \\(crime\\), mas que não estamos controlando. Adiante faremos hipóteses sobre as características de \\(u\\) que irão nos auxiliar na analise da RLS.\nVoltemos a equação básica:\n\\[y = \\beta_0 + \\beta_1x + u \\]\nNa analise de RLS estamos interessados nos parametros \\(\\beta_0\\) e sobretudo \\(\\beta_1\\). A razão primordial para isso é que, tudo o mais constante, a relação acima aponta que\n\\[\\Delta y = \\beta_1 \\Delta x \\] , se \\(\\Delta u = 0\\)\nIsto é, se tudo o mais que afeta \\(y\\) permanecer inalterado, uma variáção em \\(x\\), \\(\\Delta x\\), terá um impacto de \\(\\beta_1 \\Delta x\\) em \\(y\\). No exemplo da criminalidade, teremos que:\n\\[\\Delta crime = \\beta_1 \\Delta desemprego \\]\nPor isso, quando conseguimos estimar os parêmetros \\(\\beta\\) estamos mais próximos de entender as relações entre \\(x\\) e \\(y\\) em nossas aplicações.\nCabe destacar algo bastante importante.As relações acima não encerram a questão da causalidade. Como podemos inferir um impacto causal do desemprego na criminalidade se estamos ignorando todos os demais fatores que ficaram de fora do modelo - fatores que são captados em \\(u\\) não observados.\nPara avançarmos no que diz respeito a firmações de carater causais, precisamos fazer hipoteses sobre o comportamento de \\(u\\). Com isso estaremos mais perto de inferir causalidade.\n\n8.1.1 Alguns Exemplos de Regressão Aplicados ao Contexto Jurídico:\nConsidere os seguintes exemplos de RLS aplicadas ao contexto do Direito\nPrevisão de Sentenças com Base na Gravidade do Crime: Onde, a variável dependente e a Duração da Sentença, e a variável independente é a Gravidade do Crime, \\(\\beta_0\\)é o intercepto da regressão. \\(\\beta_1\\) é o coeficiente de regressão que representa como a gravidade do crime influência a duração da sentença, \\(u\\) é o termo de erro.\n\\[Dsent = \\beta_0 + \\beta_1{\\text{Gravidade}} + u\\]\nAnálise de Fatores que Influenciam o Tempo de Julgamento: Variável Independente (\\(x\\)), Tipo de Processo (por exemplo, criminal, civil, administrativo). Variável Dependente (\\(y\\), Tempo de Duração do Processo. Objetivo: Identificar se o tipo de processo tem impacto no tempo que um caso leva para ser concluído.\n\\[Tempo = \\beta_0 + \\beta_1  Processo  + u\\]\nPrevisão de Probabilidade de Recurso com Base em Decisões Anteriores: Variável Independente (\\(x\\)): Resultado da Decisão Anterior (por exemplo, deferimento ou negação do recurso). Variável Dependente (\\(y\\)): Probabilidade de Entrada com Recurso. Objetivo: Determinar a probabilidade de um recurso ser apresentado com base no resultado de decisões passadas.\n\\[Resultado = \\beta_0 + \\beta_1 Resultado_{-1} + u \\] Análise de Relação entre Número de Testemunhas e Veredito: Variável Independente (\\(x\\)): Número de Testemunhas. Variável Dependente (\\(y\\)): Veredito (por exemplo, culpado ou inocente). Objetivo: Investigar se o número de testemunhas influencia o veredito\n\\[ Veredito = \\beta_0 + \\beta_1 Testemunhas + u \\]"
  },
  {
    "objectID": "lineares.html#hipóteses-sobre-o-comportamento-do-termo-de-erro",
    "href": "lineares.html#hipóteses-sobre-o-comportamento-do-termo-de-erro",
    "title": "7  Modelos Lineares",
    "section": "8.2 Hipóteses sobre o comportamento do Termo de Erro",
    "text": "8.2 Hipóteses sobre o comportamento do Termo de Erro\nSe especificamos o modelo com \\(\\beta_0\\), podemos assumir que \\(u\\), tem média igual a zero. Em notação de esperança matematica essa hipotese equivale a:\n\\[E(u) = 0\\] Note que, \\(x\\) é \\(u\\) são variaveis aleatórias, então então podemos definir a distribuição condicional de \\(u\\) dado qualquer valor de \\(x\\). Especialmente, para qualquer valor de \\(x\\) podemos obter o valor médio de \\(u\\) para uma parte da populção com uma dado valor de \\(x\\).\nCom base nisso, podemos definir uma segunda hipotese sobre \\(u\\). Uma hipótese bastante forte, é a de que \\(u\\) e \\(x\\) são independentes. Tal hípótese é a hipótese crucial do modelo de RLS:\n\\[E(u|x) = 0\\]\nEssa hipótese é denominada de hipótese de independência da média de \\(x\\).\nJustas as hipóteses de \\(E(u) = 0\\) e \\(E(u|x) = 0\\) são denominadas de hipotese de média condicional zero.\nAo adcionarmos essas hipoteses em nosso arcabouço de estudo RLS, temos uma nova interpretação do parametro \\(\\beta_1\\) (focaremos mais neste parametro, visto que $ é em geral menos importante na maioria das aplicações de análise de regressão)\nConsidere o modelo baixo padrão\n\\[y = \\beta_0 + \\beta_1x + u\\] Aplicando o operador \\(E( \\cdot |x )\\), obetmos\n\\[ E( y |x ) = \\beta_0 + \\beta_1x \\] Assim, na equação acima, temos um aumento de uma unidade em \\(x\\), implica em um aumento no valor esperado (ou em média) de \\(y\\) na magnitude de \\(\\beta_1\\).\nPara todo \\(x\\) a distribuição dos valores de \\(y\\) estarão centratas ao redor da média condicional de \\(y\\) dado \\(x\\), $ E( y |x )$. Lembre que a média é uma medida de tendencia central. Nesse sentido, a regressão linear simples calcula uma média condicional.\nA equação acima, é caracterizada como função de regressão populacional. Essa função nos fornece a elação entre os diferentes níveis de\\(x\\) é o nível médio de \\(y\\), isto é \\(E( y |x )\\).\n\n\n\nFunção de Regressão Populacional\n\n\nAgora, podemos voltar a nossa equação base e verificarmos o progresso feito no entendimento do modelo:\n\\[y = \\beta_0 + \\beta_1x + u \\] \\[y = E( y |x ) + u \\] Na equação acima, \\(E( y |x )\\) é chamada de parte sistematica de \\(y\\). Isto é, a parte sistematicamente explicada por \\(x\\). Já o termo de erro não observado, \\(u\\) é a parte não sistematica de \\(y\\), não explicada por \\(x\\)."
  },
  {
    "objectID": "lineares.html#estimação-dos-parâmetros-da-rls",
    "href": "lineares.html#estimação-dos-parâmetros-da-rls",
    "title": "7  Modelos Lineares",
    "section": "8.3 Estimação dos Parâmetros da RLS",
    "text": "8.3 Estimação dos Parâmetros da RLS\nNão conhecemos \\(\\beta_0\\) e \\(\\beta_1\\) queremos estimadores desses parametros: \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\).\nSuponha que tenhamos uma amostra aleatória da população: \\(\\{(x_i, y_i): i = 1, ..., n\\}\\)\nEm nosso modelo base:\n\\[y_i = \\beta_0 + \\beta_1x_i + u_i \\]\nonde \\(u_i\\) é o erro aleatório da \\(i\\)-ésima observação.\nComo estimamos \\(\\beta_0\\) e \\(\\beta_1\\)?\nUtilizando as hipoteses \\(E(u) = 0\\) e \\(E(u|x) = 0\\), podemos utilizar aplicar um estimador de metodos dos momentos.\nComo \\(x\\) e \\(u\\) são não correlacionados (são independentes), a covariancia entre eles é 0, \\(Cov(x,u) = 0\\). Pode-se demostrar que\n\\[Cov(x,u) = E(ux) - E(u)E(x)\\]\nMas como, por hipotese, \\(E(u) = 0\\), a equação anterior se reduz à\n\\[Cov(x,u) = E(ux)\\] Assim, como \\(Cov(x,u)\\) é \\(0\\), temos também que\n\\[E(ux) = 0\\] Logo, temos que \\[E(u) = 0\\] e\n\\[E(xu)=0\\] Essas duas ultimas igualdades, são denominadas condições de momentos populacionais. Elas são hipóteses feitas na população.\nPara aplicarmos o Método dos Momentos, precisamos igual as condições de momento populacionais as condições de momentos amostrais.\nAntes, vamos reescrever as condições populacionais.\nComo\n\\[y = \\beta_0 + \\beta_1x + u \\]\nPodemos isolar \\(u\\) e obtermos\n\\[ u  =  y - \\beta_0  - \\beta_1x \\] Com isso podemos reescrever as hipoteses \\(E(u) = 0\\) e \\(E(ux) = 0\\) reespectivamente como\n\\[E( y - \\beta_0  - \\beta_1x) = 0\\]\n\\[E(x( y - \\beta_0  - \\beta_1x)) = 0\\] Os equivalentes amostrais serão dados por:\n\\[n^{-1} \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 = 0\\] e\n\\[ n^{-1} \\sum_{i=1}^{n} x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 = 0\\]\nAs duas equações acima formam um sistema de 2 equações com 2 incognitas, \\(\\beta_0\\) e \\(\\beta_1\\). Com algumas manipilações algébricas é possível demostrar que, os valores de dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) serão dados por:\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\ne\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\nOnde, {x} e {y} são as médias amostrais de \\(x\\) e \\(y\\) respectivamente.\nOs estimadores \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) são chamados de estimadores de Mínimos Quadrados Ordinários (MQO). Note que, entretanto, método de estimação para encontrarmos \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\). Na verdade, pode-se mostrar que as estimativas do modelo de RLS serão equivalentes pelos dois métodos, dadas as hipoteses enunciadas anteriormente.\nA ideia é que os parametros que estimamos, \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\), são os parametros que minimizam a soma dos quadrados das diferenças entre nossos \\(y_i\\) obversados e seus valores preditos definidos como\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\]\nO raciocinio é o seguinte. Vamos definir a distancia entre \\(y_i\\) é \\(\\hat{y}_i\\), denominada de resíduo da equação como\n\\[\\hat{u_i} = \\hat{y}- \\hat{y}_i = y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1x\\]\nNote que em uma amostra de tamanho \\(n\\), teremos \\(n\\) desses resíduos. Esconlhendo os parametros \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) que minimizam a soma dos quadrados desses resíduos:\n\\[\\sum_{i=1}^{n} \\hat{u}_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\\]\nFormalmente a soma dos quadrados dos resíduos é uma função de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\). Chamemos essa função de \\(Q(\\hat{\\beta_0},\\hat{\\beta_1} )\\):\n\\[Q(\\hat{\\beta_0},\\hat{\\beta_1} ) = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\\] Utilizando tecnicas de minimização do cálculo (foras do escopo deste curso), obteremos os valores de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\):\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\ne\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\] Tal qual obtidos anteriormente.\nIndependentemente do método escolido, conseguimos obter a Função de Regressão Amostral (FRA), que é a versão estimada da Função de Regressão Populacional (FRP)\nA FRP é fixa e desconhecida, a FRA varia de amostra para amostra.\nFormalmente, a FRA é dada por\n\\[\\hat{y}= \\hat{\\beta}_0 + \\hat{\\beta}_1x\\]\nComo mencionado anteriormente, nosso interresse primordial reside em \\(\\hat{\\beta}_1\\)\n\n\n\nValores Ajustados e Resíduos de MQO\n\n\nNote que conforme já comentado:\n\\[\\Delta \\hat{y} = \\hat{\\beta}_1 \\Delta x \\]\nIsolando \\(\\hat{\\beta_1}\\)\n\\[ \\hat{\\beta}_1  = \\Delta \\hat{y} /\\Delta x \\]\nAssim, uma variáção, \\(\\Delta\\), em \\(x\\), nos permimite calcular o impacto em \\(y\\), medido dado por \\(\\hat{\\beta}_1\\).\nAntes de passarmos para o proxímo tópico de RLS, cabe mencionar uma nota sobre terminologia: quando estimamos equações de RLS do tipo \\(y = \\beta_0 + \\beta_1 x + u\\), dizemos que “rodamos a regressão de \\(y\\) sobre \\(x\\) !”\n\n8.3.1 Aplicação - Estimando uma Regressão Linear Simples\n\n### Aplicação da RLS - Método dos Mínimos Quadrados Ordinários"
  },
  {
    "objectID": "lineares.html#caracteristicas-do-método-dos-mínimos-quadrados-ordinários-em-determinadas-amostras-de-dados",
    "href": "lineares.html#caracteristicas-do-método-dos-mínimos-quadrados-ordinários-em-determinadas-amostras-de-dados",
    "title": "7  Modelos Lineares",
    "section": "8.4 Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados",
    "text": "8.4 Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados\n\n8.4.1 Valores estimados e Resíduos\nUma vez estimados \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) temos os valores ajustados ou também denominados de valores preditos ou fitted values.\nPara uma observação qualquer \\(i\\), seu valor estimado é:\n\\[\\hat{y}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\]\nDe maneira geral teremos a reta de regressão, dada por:\n\\[\\hat{y}= \\hat{\\beta}_0 + \\hat{\\beta}_1x\\]\nTodos os valores \\(\\hat{y}\\) estarão sobre a reta de regressão.\nO resíduo, tal qual definido anteriormente, será a diferença entre o valor ajustado \\(\\hat{y}\\) e o verdadeiro \\(y\\) em nosso banco de dados:\n\\[\\hat{u}_i = y_i - \\hat{y}_i\\]\nSe \\(\\hat{u_i} &gt; 0\\) a regressaõ subestima \\(y_i\\). Se \\(\\hat{u_i} &lt; 0\\) a reta superestima \\(y_i\\). O cenário ideal é quando \\(\\hat{u}_i= 0\\), algo que quase nunca acontece.\n\n\n8.4.2 Propriedades Algébricas do MQO\n\nA Soma dos Resíduos é zero:\n\n\\[\\sum_{i=1}^{n} \\hat{u}_i = 0\\]\n\nA covariancia amostral entre a variavel explicativa e os resíduos é zero:\n\n\\[\\sum_{i=1}^{n} x_i \\hat{u}_i = 0\\]\n\nO ponto \\((\\bar{x},\\bar{y})\\) sempre estará sob a reta de regressão.\nA média dos valores estimados, \\(\\bar{\\hat{y}}\\) é igual a média dos valores observados \\(\\bar{y}\\).\nNote que as estimatvas de MQO decompõe \\(y\\) em 2 partes: 1) os valores ajustados \\(\\hat{y}\\) e os resíduos \\(\\hat{u}\\).\nOs valores de \\(\\hat{y}\\) e \\(\\hat{u}\\) são não correlacionados na amostra!\n\n\n\n8.4.3 Qualidade do Ajuste\nNesta seção vamos responder a seguinte questão: “Quão bem \\(x\\) explica \\(y\\) ?”\nConsidere as seguintes definições\nSoma dos Quadrado Totais (SQT):\n\\[\\text{SQT} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nSoma dos Quadrados Explicados (SQE):\n\\[\\text{SQE} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nSoma dos Quadrados dos Resíduos (SQR):\n\\[\\text{SQR} = \\sum_{i=1}^{n} \\hat{u}_i^2\\] Temos alguns resultados bem importantes. Note que, ao dividirmos \\(SQT\\) por \\(n-1\\), teremos a variancia amostral de \\(y\\). Por sua vez, ao dividirmos \\(SQE\\) por \\(n-1\\), termeos a variancia populacional de \\(\\hat{y}\\).\nEntretanto, o resultado mais importante é o seguinte:\n\\[SQT =  SQE + SQR\\] É apartir dessa iguadade que podemos mostrar algo sobre o ajuste dos MQO.\nDívidindo ambos os lados por \\(SQT\\) teremos\n\\[1 = \\frac{\\text{SQE}}{\\text{SQT}}  + \\frac{\\text{SQR}}{\\text{SQT}}\\] Rearranjando os termos\n\\[R^2 = \\frac{\\text{SQE}}{\\text{SQT}} = 1 - \\frac{\\text{SQR}}{\\text{SQT}}\\]\nO \\(R^2\\) é a porcentagem da variação de y que é explicada por \\(x\\). O valor de \\(R^2\\) sempre estará na RLS entre \\(0\\) e \\(1\\).\nO nome \\(R^2\\) advém do fato de que, pode ser mostrado que tal médida é equivalente ao quadrado do coeficiente de correlação entre \\(\\hat{y}\\) e \\(y\\)."
  },
  {
    "objectID": "lineares.html#ausência-de-viés-e-variância-dos-estimadores-de-mqo",
    "href": "lineares.html#ausência-de-viés-e-variância-dos-estimadores-de-mqo",
    "title": "7  Modelos Lineares",
    "section": "8.5 Ausência de Viés e Variância dos Estimadores de MQO",
    "text": "8.5 Ausência de Viés e Variância dos Estimadores de MQO\n\n8.5.1 Ausencia de Viés em MQO\nUm estimador não viésado (ou não tendêncioso) por definição, um estimador que em média “acerta” o verdadeiro valor do parâmetro estimado.\nConsidere um estimador arbitrario \\(\\hat{\\theta}\\). Quando dezemos que \\(\\hat{\\theta}\\) é não viesado, queremos dizer que\n\\[E(\\hat{\\theta}) = \\theta\\]\nSerá que os \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) estimados pelo método do MQO são não viésados?\nPara responder a essa pergunta precisamos, mais uma vez, recorrer a novas hipoteses sobre os dados. Esse novo conjunto de hipoteses irão nos permitir verificarmos algumas propriedades estatísticas do método de MQO. A depender das hipóteses, umas mais fracas que outras, as propriedades podem mudar.\nA garantia da ausência de viés, em dados de corte transversal, requer quatro hipóteses:\n1) Linearidade dos Parametros: No modelo Populacional a variavel dependente \\(y\\) está relacionada a variável dependente \\(x\\) e ao termo de erro \\(u\\) como:\n\\[y = \\beta_0 + \\beta_1x + u \\] Por exemplo no modelo \\(y= \\beta_0 + e^{\\beta_1} + u\\), não é linear no parametro \\(\\beta_1\\).\n2) Amostragem Aleatória: Usamos uma amostra aleatória de tamanho \\(n\\), \\(\\{(x_i, y_i): i = 1, 2, ..., n\\}\\) proveniente de um modelo populacional.\n3) Existe Variação Amostral em \\(x\\): Isto é, os valores de nossa variável explicativa não são todos iguais.\n4) Independencia da Média: O termo de erro \\(u\\) tem um valor esperado de zero, dado qualquer valor da variável explicativa.\\[E(u|x) = 0\\]\nDadas as hipóteses 1, 2, 3 e 4:\n\\[E(\\hat{\\beta_1}) = \\beta_1\\]\ne\n\\[E(\\hat{\\beta_0}) = \\beta_0\\]\nPortantos nossoa estimadores de MQO, são não viésados.\n\n\n8.5.2 Variância dos estimadores de MQO\nQuão distantes em média \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) estão dos verdadeiros parâmetros? Saber disso nos permite escolher o melhor estimador entre todos os etimadores não viesados - ou ao menos entre uma ampla classe deles.\nPara obetermos expressões tratáveis das variâncias do MQO recorremos a mais uma hipótese.\n5) Hipótese de Homoscedasticidade: O erro \\(u\\) tem a mesma variâcia, dado qualquer valor da variável explicativa. Isto é,\n\\[Var(u|x)=\\sigma^2\\]\nA variância de \\(u\\) pode ser reescrita em termos de esperança:\n\\[Var(u|x)= \\sigma^2 = E(u^2|x) - [E(u|x)]^2\\]\nMas como temos por hipótese que \\([E(u|x)]=0\\), a hipótese de homoscedasticidade as vezes é representada como\n\\[ E(u^2|x) = \\sigma^2\\]\nNote também, que como \\(E(u)=0\\), a variancia do erro, não condicional a \\(x\\), será dada por:\n\\[\\sigma^2=E(u^2)=Var(u)\\] Por isso chamamos \\(\\sigma^2\\) de variância do erro.\nPodemos “vizualizar” a hipótese de homoscedasticidade na figura abaixo:\n\n\n\nModelo de Regressão Sob Homoscedasticidade\n\n\nApartir das hiopóteses 1,2,3,4 e 5 podemos progredir no entendimento dos MQO. Sob as hipoteses 1 - 5, as variancias de \\(\\hat{\\beta_1}\\) e \\(\\hat{\\beta_0}\\) são dadas por:\n\\[\\text{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}=\\frac{\\sigma^2}{\\text{SST}_x}\\] e\n\\[\\text{Var}(\\hat{\\beta}_0) = \\frac{\\sigma^2n^{-1}\\sum_{i=1}^{n} x_i^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\nUma extenção da variância dos estimadores de MQO são seus desvios padrão definidos simplismente pela raiz quadrada da variância:\n\\[ep(\\hat{\\beta}_0) = \\sqrt{Var(\\hat{\\beta_0})}\\]\ne\n\\[ep(\\hat{\\beta}_1) = \\sqrt{Var(\\hat{\\beta_1})} = \\frac{\\sigma}{\\sqrt{SQT_x}}\\]\n\n\n8.5.3 Estimação da Variância do erro\nNote que, nas formulas das variâncias e dos desvios-padrão, o que aparece explicitamente é o parâmetro populacional, \\(\\sigma^2\\) e \\(\\sigma\\). Raramente conhecemos esses valores, logo as formulas derivadas acima terão pouca utilidade. Não obstante, caso encontremos um estimador para \\(\\sigma^2\\) e \\(\\sigma\\) podemos usar esse estimador.\nPodemos utilizar os resíduos para construir um estimador da variância dos erros. Lembre que o resíduos da \\(i\\)-ésima observação do modelo é dado por:\n\\[\\hat{u_i} = y_i - \\hat{y_i}\\] Pode-se demostrar que, um estimador não viesado de \\(\\sigma^2\\) será dado por:\n\\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^{n} \\hat{u}_i^2}{n-2} = \\frac{\\text{SQR}}{n-2}\\]\nCom \\(\\hat{\\sigma}^2\\) em mãos, podemos reescrever os erros padrão dos estimadores de MQO:\n\\[ep(\\hat{\\beta}_1) = \\sqrt{Var(\\hat{\\beta_1})} = \\frac{\\hat{\\sigma}^2}{\\sqrt{SQT_x}}\\]\nOs erros padrão são utilizados para calcularmos as estatísticas de teste e Intervalos de Confiança para as Estimativas. Mais a frente iremos nos concentar nessas questões."
  },
  {
    "objectID": "lineares.html#aplicação-de-rls---foco-no-erro-padrão-do-parametro-b1",
    "href": "lineares.html#aplicação-de-rls---foco-no-erro-padrão-do-parametro-b1",
    "title": "7  Modelos Lineares",
    "section": "8.6 Aplicação de RLS - Foco no erro padrão do parametro b1",
    "text": "8.6 Aplicação de RLS - Foco no erro padrão do parametro b1\n\n### Aplicação do de RLS - Foco no erro padrão"
  },
  {
    "objectID": "lineares.html#exemplos-de-regressão-multipla-no-contexto-jurídico",
    "href": "lineares.html#exemplos-de-regressão-multipla-no-contexto-jurídico",
    "title": "7  Modelos Lineares",
    "section": "9.1 Exemplos de Regressão Multipla no Contexto Jurídico",
    "text": "9.1 Exemplos de Regressão Multipla no Contexto Jurídico\nDeterminação de Fatores que Afetam o Valor de Indenizações: Variáveis Independentes: Idade da Vítima, Gravidade do Dano, Jurisdição. Variável Dependente: Valor da Indenização.Objetivo: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Gravidade + \\beta_2 Jurisdisção + u\\]\nAnálise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais: Variáveis Independentes: Idade do Réu, Tipo de Crime, Local do Julgamento. Variável Dependente: Taxa de Condenação.Objetivo: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Local  + \\beta_3  crime + u\\]\nnálise de Fatores que Influenciam a Taxa de Feminicídio: Variáveis Independentes (X): Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher. Variável Dependente (Y): Taxa de Feminicídio por 100.000 mulheres. Objetivo: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.\n\\[Tx.feminicidio = \\beta_0 + \\beta_1 desemprego + \\beta_2 educ + \\beta_3 política + u \\]"
  },
  {
    "objectID": "lineares.html#a-interpretação-da-equação-de-regressão-de-mqo",
    "href": "lineares.html#a-interpretação-da-equação-de-regressão-de-mqo",
    "title": "7  Modelos Lineares",
    "section": "9.2 A Interpretação da Equação de Regressão de MQO",
    "text": "9.2 A Interpretação da Equação de Regressão de MQO\nTal qual no modelo de RLS, temos que:\n\\[\\Delta y = \\beta_1 \\Delta x_1 + \\beta_2 \\Delta x_2 + ...+\\beta_k \\Delta x_k\\]\nO coefiente \\(\\beta_j\\), com \\(j=1,...k\\), mede o efeito do incremento de uma unidade de \\(x_j\\) em \\(y\\).\nSuponha que estejamos interessados no impacto de \\(x_1\\). Podemos fazer o seguinte exercício: Tudo o mais constante qual o impacto de \\(x_1\\) em \\(y\\):\n\\[\\frac{\\Delta y}{\\Delta x} = \\beta_1  \\]\nManter outros fatores fixos permite o cientista social, “mimetizar” um experimento, tal qual nas Ciências Naturasi. Obviamente, isso não tão simples assim. Entretanto, mater outros fatores fixos, e supondo que \\[E(u|x_1, . . . ,x_k) = 0\\] , nos aproxíma de afirmações de cunho causal."
  },
  {
    "objectID": "lineares.html#estimação-dos-k-parametros-na-rlm",
    "href": "lineares.html#estimação-dos-k-parametros-na-rlm",
    "title": "7  Modelos Lineares",
    "section": "9.3 Estimação dos k parametros na RLM",
    "text": "9.3 Estimação dos k parametros na RLM\n\n9.3.1 Estimativas de MQO\nA mecânica para conseguirmos estimativas de \\(\\beta_j\\), fica uma pouco mais complicada. Felizmente, os softwares, tais como o R, fornecem essas estimativas com muita facilidade. Não obstante, convêm apresentar uma abordagem para o calculo desses parâmetros. Utilizaremos álgebra de matrizes para mostrar um “algoritmo” para calcular esses \\(\\beta_j\\). Em verdade, esse é o algoritmo utilizado pela função lm() do R no computo dos estimadores.\nConsidere o modelo para a \\(i\\)-ésima observação\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_{i2} x_2 + \\beta_{i3} x_3 + \\ldots + \\beta_k x_{ik} + u\\]\ncomo temos \\(n\\) observações (tamanho de nossa amostra), podemos organizar esses valores em vetores e matrizes.\nPara os valores de \\(y\\) teremos\n\\[\n\\mathbf{y} = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\]\nOs valores das variáves independentes \\(X\\) podem ser organizados da seguinte maneira:\n\\[\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & \\cdots & x_{1k} \\\\\n1 & x_{21} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & \\cdots & x_{nk}\n\\end{bmatrix}\\]\nOs \\(\\beta_j\\) são organizados da seguinte maneira:\n\\[\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\\]\nOs erros \\(u_i\\) serão armazenados da em um vetor coluna tambêm:\n\\[\\mathbf{u} = \\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\vdots \\\\\nu_n\n\\end{bmatrix}\\]\nApartir disso representamos o modelo de RLM da seguinte maneira:\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{u}\\]\nQueremos encontrar um estimador de \\(\\boldsymbol{\\beta}\\), que chamaremos de \\(\\boldsymbol{\\hat{\\beta}}\\). Esse estimador sera dado por:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\]\nOnde \\(\\mathbf{X}^T\\) é a matriz transposta de \\(\\mathbf{X}\\), e \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\) é a matriz inversa da seguinte multiplicação de matrizes, \\((\\mathbf{X}^T \\mathbf{X})\\)\nEsse vetor de estimativas irá nos fornecer os valores para os \\(\\beta_j\\),com \\(j=1,...,k\\). Assim temos que:\n\\[ \\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n\\hat{\\beta}_0 \\\\\n\\hat{\\beta}_1 \\\\\n\\hat{\\beta}_2 \\\\\n\\vdots \\\\\n\\hat{\\beta}_k\n\\end{bmatrix}\\]\nAntes, de prosseguirmos cabe destacar que, embora não precisamos saber as formulas explicitas para dos \\(\\hat{\\beta_j}\\), as vezes isso nos ajuda a entender o funcionamento da regressão multipla e os efeitos parciais que os \\(\\beta_j\\) exercem sobre \\(y\\).\nPor exemplo, \\(\\hat{\\beta_1}\\) pode ser representado da seguinte maneira no contexto da RLM:\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n \\hat{r_{i1}} y_i}{\\sum_{i=1}^n \\hat{r_{i1}}^2}\\]\nOnde \\(r_{i1}\\) são os resíduos da regressão de \\(x_1\\) sobre todas as demais variáveis do modelo \\(x_2,...,x_k\\). Nesse caso, podemos entender que $_1 $ mede o efeito de \\(x_1\\) sobre \\(y\\) após terem sido parcializados ou descontados os impactos de \\(x_2,...,x_k\\).\nPara um \\(\\beta_j\\) qualquer a interpretação é análoga.\n\\[\\hat{\\beta}_j = \\frac{\\sum_{i=1}^n \\hat{r_{ij}} y_i}{\\sum_{i=1}^n \\hat{r_{ij}}^2}\\]"
  },
  {
    "objectID": "lineares.html#qualidade-do-ajuste-na-regressão-linear-multipla",
    "href": "lineares.html#qualidade-do-ajuste-na-regressão-linear-multipla",
    "title": "7  Modelos Lineares",
    "section": "9.4 Qualidade do Ajuste na Regressão Linear Multipla",
    "text": "9.4 Qualidade do Ajuste na Regressão Linear Multipla\nTal qual na regressão linear simples, são válidas as seguintes relações:\nSoma dos Quadrado Totais (SQT):\n\\[\\text{SQT} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nSoma dos Quadrados Explicados (SQE):\n\\[\\text{SQE} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nSoma dos Quadrados dos Resíduos (SQR):\n\\[\\text{SQR} = \\sum_{i=1}^{n} \\hat{u}_i^2\\] Novamente podemos definir a qualidade do ajuste, medido pelo \\(R^2\\), como sendo igual à\n\\[R^2 = \\frac{\\text{SQE}}{\\text{SQT}} = 1 - \\frac{\\text{SQR}}{\\text{SQT}}\\]\nExplicitamente podemos escrever o \\(R^2\\) tal como:\n\\[R^2 =  \\frac{[\\sum_{i=1}^n (y_i - \\bar{y}) ( \\hat{y}_i - \\bar{\\hat{y}} )]^2}{[\\sum_{i=1}^n (y_i - \\bar{y})^2][\\sum_{i=1}^n(\\hat{y}-\\bar{\\hat{y}})^2]}\\]\nUm fato importante sobre \\(R^2\\) é que ele nunca diminui ao incluirmos variáveis no modelo. Isso decorre de propriedades algébricas desse indicador.Por isso, os softwares geralmente reportam uma outra estatística de ajuste o R-quadrado ajustado,\\(\\bar{R}^2\\):\n\\[\\bar{R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\] O \\(\\bar{R}^2\\), tal como expresso acima conta com a presença do \\(R^2\\) original, mas note que, no denominador estamos dividindo por \\(n-k-1\\), onde \\(n\\) é o tamanho da amostra em mãos, e \\(k-1\\) se referem ao fato de termos \\(k\\) variáveis explicativas e uma constante. Portanto, o \\(\\bar{R}^2\\), impões uma penalidade à inclusão de variáveis independentes em um modelo de regressão.\nCabe destacar que, diferente do \\(R^2\\), \\(\\bar{R}^2\\) pode ser negativo, indicando um ajuste bastante pobre do modelo.\nNão obstante, cabe destacar que um \\(R^2\\) baixo não é definitivamente uma evidencia definitiva contra nosso modelo. Em ciências sociais em geral é bastante comum verificarmos \\(R^2\\) relatvamente pequenos. Isso significa que, embora coletovamente as variáveis explicativas não expliquem muito das variações de \\(y\\), é possível que as estimativas de MQO sejam estimativas confiaveis dos efeitos parciais - tudo o mais constante - de cada \\(x_j\\) sobre \\(y\\)."
  },
  {
    "objectID": "lineares.html#valor-espererado-dos-estimadores-de-mqo",
    "href": "lineares.html#valor-espererado-dos-estimadores-de-mqo",
    "title": "7  Modelos Lineares",
    "section": "9.5 Valor Espererado dos Estimadores de MQO",
    "text": "9.5 Valor Espererado dos Estimadores de MQO\nQuando analisamos a ausencia de viés nos estimadores do modelo de RLS, um invocamos um conjunto de hipóteses. Felizmente, na nalises de RLM tais hipoteses se mantém, bastando apenas algumas modificações para o contexto de mais variáveis explicativas. Abaixo vamos enunciar essas hipoteses que garantem o não viés dos \\(\\beta_j\\) estimados.\n1) Linearidade dos Parametros: No modelo Populacional a variavel dependente \\(y\\) está relacionada as variáveis independentes e ao termo de erro \\(u\\) como:\n\\[y = \\beta_0 + \\beta_1x_1 + ...+\\beta_kx_k+u \\]\n2) Amostragem Aleatória: Usamos uma amostra aleatória de tamanho \\(n\\), \\(\\{(x_{i1},...,x_{ik}, y_i): i = 1, 2, ..., n\\}\\) proveniente de um modelo populacional.\n3) Colinearidade não Perfeita: Na amostra, e portanto na população, nehuma das variáveis explicativas são constantes e não há relações lineares extas entre as variáveis.Isto é, tais variáveis não são perfeitamente correlacionadas.\n***4) Independencia da Média:** O termo de erro \\(u\\) tem um valor esperado de zero, dado qualquer valor da variável explicativa.\n\\[E(u|x_1,...,x_k) = 0\\]\nEssa é a propriedade mais importante. Tal hipótese é violada quando a forma funciinal das variáveis explicativas e da variável dependente está incorreta ou mal especificada.\nQundo a hipótese se mantém, dizemos que as variáveis explicativas são exógenas.\nDadas as hipóteses 1, 2, 3 e 4:\n\\[E(\\hat{\\beta_j}) = \\beta_j\\] Para \\(j = 1,2,...,k\\). Isto é, \\(\\hat{\\beta_j}\\) são não viesados"
  },
  {
    "objectID": "lineares.html#variância-dos-estimadores-de-mqo-1",
    "href": "lineares.html#variância-dos-estimadores-de-mqo-1",
    "title": "7  Modelos Lineares",
    "section": "9.6 Variância dos Estimadores de MQO",
    "text": "9.6 Variância dos Estimadores de MQO\nAnalogamente ao que foi feito no caso da regressão simples, para obtermos a variância dos \\(\\hat{\\beta_j}\\) de MQO presisamos recorrer à mais uma hipótese. Tal hipótese é a de homoscedasticidade. Isto é, de que condicionadas as variáveis explicativas, a variância do termo de erro é constante para todas as observações.\n5) Hipótese de Homoscedasticidade: O erro \\(u\\) tem a mesma variâcia, dado qualquer valor das variáveis explicativas:\n\\[Var(u|x_1,...,x_k)=\\sigma^2\\]\nSob as hipóteses 1,2,3,4 e 5, a varincias de \\(\\hat{\\beta_j}\\) e dada por:\n\\[\\text{Var}(\\hat{\\beta}_j) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})^2(1-R^2_j)}=\\frac{\\hat\\sigma^2}{\\text{SQT}_j(1-R^2_j)}\\] Onde \\(SQT_j\\) é a a variação amostral de \\(x_j\\)e \\(R^2_j\\) é o R-quadrado da regressão de \\(x_j\\) sobre as outras variáveis explicativas.O termo \\(\\hat{\\sigma}^2\\) é o estimador da variâcia do termo de erro \\(u\\), dado por:\n\\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^{n} \\hat{u}_i^2}{n-k-1} = \\frac{\\text{SQR}}{n-k-1}\\] Onde \\(n-k-1\\) são os graus de liberdade do problema de MQO: \\(k\\) variáveis e uma constantante.\n\n9.6.1 Eficiencia de MQO: O Teorema de Gauss-Markov\nTeorema de Gauss-Markov:Sob as hipóteses 1,2,3,4 e 5 os estimadores \\(\\hat{\\beta}_0,\\hat{\\beta}_1,...,\\hat{\\beta}_k\\) são os melhores estimadores lineares não viesados de \\(\\beta_1, \\beta_2,...,\\beta_k\\).\nSe alguma das hipóteses falhar, o teorema não é mais válido.\nÉ o teorema de Gauss-Markov que justifica o uso do MQO para estimar modelos de Regressão Linear Múltipla."
  },
  {
    "objectID": "lineares.html#uso-de-variáveis-qualitativas-na-análise-de-rlm",
    "href": "lineares.html#uso-de-variáveis-qualitativas-na-análise-de-rlm",
    "title": "7  Modelos Lineares",
    "section": "9.7 Uso de Variáveis Qualitativas na Análise de RLM",
    "text": "9.7 Uso de Variáveis Qualitativas na Análise de RLM\nConsidere os exemplos mencionados anteriormente.\nDeterminação de Fatores que Afetam o Valor de Indenizações: Variáveis Independentes: Idade da Vítima, Gravidade do Dano, Jurisdição. Variável Dependente: Valor da Indenização.Objetivo: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Gravidade + \\beta_2 Jurisdisção + u\\]\nAnálise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais: Variáveis Independentes: Idade do Réu, Tipo de Crime, Local do Julgamento. Variável Dependente: Taxa de Condenação.Objetivo: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Local  + \\beta_3  crime + u\\]\nnálise de Fatores que Influenciam a Taxa de Feminicídio: Variáveis Independentes (X): Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher. Variável Dependente (Y): Taxa de Feminicídio por 100.000 mulheres. Objetivo: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.\n\\[Tx.feminicidio = \\beta_0 + \\beta_1 desemprego + \\beta_2 educ + \\beta_3 política + u \\]\nEm todos esses exemplos temos o denominamos de variáveis qualitativas. No primeiro exemplo, gravidade e jurisdição não são quantificaveis. No segundo exemplo, o tipo de crime, e a localidade também não. Por fim, no último exemplo, a variável que indica ou não a presença de uma política pública de proteção a mulher também é qualitativa.\nFelizmente, isso não traz problemas adcionais a estimação de MQO. Pelo menos quando tais variáveis aparecem como variáveis explicativas. Mais a frente veremos o que acontece quando a variável dependente é do tipo qualitativa.\nDuas coisas mudam quando temos as variáveis qualitativas como regressores. Em primeiro lugar temos que organizar essas informações no banco de dados de modo coerente. Em sugundo lugar, a introdução de variáveis qualitativas muda a interpretação dos resultados.\nFatores qualitativos geralmente aparecem na forma de informações binários, também denominadas,dummy.\nVariável Binária ou Dummy: É uma variável que assume o valor 1 se a “condição occorre” e 0 caso o contrario.\nExemplos: O crime de feminicio ocorreu na região central? se sim, ela recebe o valor 1, se não recebe o valor 0. Podemos ter uma coluna em nosso banco de dados denominada centro, nas quais a linha recebe o valor 1 caso o crime tenha ocorrido, por exemplo, no centro e 0 caso o contrário.\nO crime foi considerado grave? Se sim, recebe o valor 1 se não recebe o valor 0.\nSuponha que queiramos investigar descriminação de gênero no mercado de trabalho. Se tivermos uma base de dados com informçoes sobre diversos individuos como anos de estudo, experiencia no mercado de trabalho. Podemos ter também uma variavel binaria que assume o valor 1 caso o individuo na amostra for do sexo feminino ou 0 caso o contrario.\nO mesmo pode acontecer com etnia. Podemos ter uma dummy que assuma o valor 1, caso o individuo seja autodeclarado não-branco e 0, caso o contrario.\nAs observações que tiveram 1 como valor serão comparadas com aquelas observações que ficaram com o valor 0. Essas observações seriam nosso “grupo de comparação/grupo base”.\nTrabalhar com variáveis qualitativas é muito comum. Na maioria das vezes cabe ao pesquisador o julgamento de trabalhar com uma variável desse tipo.\nUtilizae 1 ou 0 é, de fato, um criterio arbitrario. Todavia, essa escolha reside justamente na vantagem de se capturar informações importantes que tornam a interpretação dos resultados mais simples.\n\n9.7.1 Uma única Variável Dummy Independente\nA forma mais simples de incorporar uma variável qualitativa e simplismente adcionarmos ela na equação de regressão.\nConsidere o exemplo prático. Queremos relacionar anos de estudo e salários. Obviamente, o exemplo é bastante simples, mas será útil para avaliarmos a questão.\n\\[salario = \\beta_0 + \\delta_0 feminino + \\beta_1 educ + \\beta_3 experiencia + u \\] Nesse exemplo, estamos controlando educação e experiência. A variável feminino assume o valor 1, se o individuo for do sexo feminino e 0, caso o contrário, isto é, o indivíduo é um homem. Na equação \\(\\delta_0\\) mede a diferença no salario entre homems e mulheres, dado os mesmos anos de estudos e de experiencia, e evidentemente o mesmo \\(u\\).\nAssim, um indicio de discriminação de genero no mercado de trabalho, seria se encontracemos uma relação do tipo \\(\\delta_0 &lt; 0\\).\nIsso deveria ser interpretado como a diferença da média condicional aos níveis de educação e experiencia, entre homens e mulheres.\nO uso de variáveis dummy pode ser interessante no contexto da análise de políticas públicas.\nConsidere que tenhamos uma base de dados municipal com uma serie dados sobre crimes contra as mulheres. Suponha hipoteticamente que alguns bairros dessa cidade hipotética tenham alguma iniciativa ou política pública local de combate e prevenção desses crimes. Em um primeiro momento suponha que queremos avaliar se nesses bairros a taxa de crimes contra as mulheres é menor. Considere que temos dados sobre o nível educacional médio da população do bairro, a taxa de desemprego nesses bairros, e uma variável que assume o valor 1 se no bairro em questão existe alguma política pública como a mencionada.\nConsidere a equação:\n\\[Tx.feminicidio = \\beta_0 + \\beta_1 desemprego + \\beta_2 educ + \\beta_3 política + u \\] Nesse exemplo, se estimarmos um \\(\\beta_3 &lt; 0\\) teriamos evidências de que em bairros onde há a política pública em questão, a taxa de crimes contra as mulheres é menor. Claramente, estamos supondo que todas as hipoteses mencinadas anteriormente são validadas, de modo que temos estimadores não viesados. Se existir alguma variável omitida em \\(u\\) ou correlação entre \\(u\\) e as variáveis explicativas, sobretudo entre \\(u\\) e nossa dummy de interesse, teremos um estimador viésado de \\(\\beta_3\\) e provavelmente dos demais coeficientes.\n\n\n9.7.2 O uso de Dummies para categorias multiplas.\nConsidere a equação abaixo\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Local  + \\beta_3  crime + u\\] Suponha que nossas unidades de observação sejam indivíduos na cidade de São Paulo. Suponha que o local, seja o local do crime e represente as cinco zonas da cidade de São Paulo. Da forma como esta na regressão teriamos de escolher apenas uma Zona para avaliar (assumindo o valor 1), e as demais seriam as categorias bases. Pode ser interessante, caso nosso interresse resida na região em questão. Não obstante, podemos avaliar mais categorias.\nTemos 5 zonas na cidade de São Paulo. Nesse caso, precisamos utilizar uma delas como grupo base. Supoha que tal zona seja o Centro. Nosso modelo fica da seguinte maneira\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Leste+  \\beta_3 Sul +  \\beta_4 Oeste   + \\beta_5 Norte + \\beta_6  crime + u\\]\nO mesmo deve ocorrer no tipo de crime. Suponha que por algum motivo razoavel classificamos os crimes em 3 categorias 1, 2 e 3, e queremos utilizar a categoria 2 como grupo base. Nesse caso, apenas rodamos a regressão com os crimes do tipo 1 e 2 :\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Leste+  \\beta_3 Sul +  \\beta_3 Oeste   + \\beta_5 Norte + \\beta_6  crime1 + \\beta_7  crime3 u\\]\nO coeficiente estimado de \\(\\beta_3\\), por exemplo, captaria a diferença média da taxa de condenação entre crimes que ocorrem na região Sul e o Centro da cidade de São Paulo, controlado para o mesmo tipo de crime e idade do indivíduo. Ja o coeficiente \\(\\beta_6\\) mediria a diferença de media entre a taxa de condenação para crimes do 1 e crimes do tipo 2, controlados pela localidade e idade do indivíduo.\n\n\n9.7.3 Incorporando Informações Ordinais com o uso de Variáveis Dummy.\nDeterminação de Fatores que Afetam o Valor de Indenizações: Variáveis Independentes: Idade da Vítima e Gravidade do Dano . Variável Dependente: Valor da Indenização.Objetivo: Analisar como a idade da vítima, a gravidade do dano influenciam o valor das indenizações concedidas.\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Gravidade  + u\\] Suponha que a gravidade varie em uma escala de 0 a 4. Uma posibilidade seria estimar a equação acima, tal qual específicada.\nInfelizmente, seria muito dificil interpretar um aumento de uma unidade na gravidade. Uma vez que a gravidade de um crime é medida de maneira ordinal. Por exemplo uma Gravidade de um crime classificada como 4 é maior que a Gravidade de um crime classificada como 3.\nUtilizando o que já aprendemos, podemos utilizar a categoria de gravidade=0 (não grave) como base de comparação. Nesse caso estimariamos a seguinte regressão:\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Grav1  ++ \\beta_3 Grav2  ++ \\beta_4 Grav3  ++ \\beta_5 Grav4 + u\\] onde \\(\\beta_2\\) por exemplo, capta a diferença na indenização de uma crime de gravidade 1 em comparação de um crime com gravidade igual a 0."
  },
  {
    "objectID": "lineares.html#alguns-problemas-que-podem-surgir-na-análise-de-regressão",
    "href": "lineares.html#alguns-problemas-que-podem-surgir-na-análise-de-regressão",
    "title": "7  Modelos Lineares",
    "section": "9.8 Alguns Problemas que podem surgir na análise de regressão",
    "text": "9.8 Alguns Problemas que podem surgir na análise de regressão\n\n9.8.1 Viés de Variável Omitida\n\n\n9.8.2 Multicolinearidade\n\n\n9.8.3 Heteroscedasticidade"
  },
  {
    "objectID": "lineares.html#hipótese-de-normalidade",
    "href": "lineares.html#hipótese-de-normalidade",
    "title": "7  Modelos Lineares",
    "section": "10.1 Hipótese de Normalidade",
    "text": "10.1 Hipótese de Normalidade"
  },
  {
    "objectID": "lineares.html#teste-de-hipotese-sob-um-unico-parametro",
    "href": "lineares.html#teste-de-hipotese-sob-um-unico-parametro",
    "title": "7  Modelos Lineares",
    "section": "10.2 Teste de Hipotese sob um unico Parametro",
    "text": "10.2 Teste de Hipotese sob um unico Parametro\n\n10.2.1 Teste contra Hipoteses Alternativas Unilaterais\n\n\n10.2.2 Teste contra Hipoteses Alternativas Bilaterais\n\n\n10.2.3 Cálculos dos p-valores dos Testes t\n\n\n10.2.4 Teste de Restrições de Lineares Múltiplas: O teste F"
  },
  {
    "objectID": "lineares.html#teste-de-hipótese-sob-um-único-parametro",
    "href": "lineares.html#teste-de-hipótese-sob-um-único-parametro",
    "title": "7  Modelos Lineares",
    "section": "10.2 Teste de Hipótese sob um Único Parametro",
    "text": "10.2 Teste de Hipótese sob um Único Parametro\n\n10.2.1 Teste contra Hipoteses Alternativas Unilaterais\n\n\n10.2.2 Teste contra Hipoteses Alternativas Bilaterais\n\n\n10.2.3 Cálculos dos p-valores dos Testes t\n\n\n10.2.4 Teste de Restrições de Lineares Múltiplas: O teste F"
  },
  {
    "objectID": "lineares.html#multicolinearidade-pefeita",
    "href": "lineares.html#multicolinearidade-pefeita",
    "title": "7  Modelos Lineares",
    "section": "11.1 5.1 - Multicolinearidade Pefeita",
    "text": "11.1 5.1 - Multicolinearidade Pefeita\nA multicolinearidade é uma violação da hipótese 3. Nesse caso as variáveis ou alguma das variáveis podem ser escritas como uma combinação linear das demais. Esse é um caso extremo e que, no limite, não aparece com frequência nas aplicaçoes empíricas. Não obstante, uma multicolinearidade alta pode aparecer. Na presença de Multicolinearidade, nossos estimadores continuam sendo não viésados. Entretanto, a variância de \\(\\hat{\\beta_j}\\) pode ficar muito elevada, prejudicando os procedimentos de inferência.\nPodemos ver isso ao avaliarmos a formula da variância de uma dado \\(\\hat{\\beta_j}\\) estimado:\n\\[\\text{Var}(\\hat{\\beta}_j) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})^2(1-R^2_j)}=\\frac{\\hat\\sigma^2}{\\text{SQT}_j(1-R^2_j)}\\]\nNote que, se a relação linear entre os \\(x_1,...,x_k\\) forem elevadas, o \\(R^2_j\\) será alto. No limite, um \\(R^2_j\\) proximo de faz o denominador da divisão acima ficar muito pequeno. Com um denominador pequeno a \\(\\text{Var}(\\hat{\\beta}_j)\\) fica muito alta. No caso em que há multicolinearidade perfeita, o \\(R^2_j =1\\). logo, a \\(Var(\\hat{\\beta_j})\\) não estará nem mesmo definida (haverá um divisão por zero!).\nNa prática o problema da multicolinearidade não é uma unanimidade entre os autores. Muitos livros textos dedicam capítulos interios para tratar do problema, ao passo que outros autores a enxergam como uma mal menor.\nA razão de se enchergar a multicolinearidade como uma mal menor é o fato de que ao olharmos novamente para o denominador da variância de \\(\\beta_j\\), veremos que a variância de \\(\\hat{\\beta_j}\\) pode diminuir ao aumentarmos a Soma dos Quadrados dos Totais de \\(x_j\\), \\(SQT_j\\). Isso pode ser feito aumentando o tamanho da amostra, e por conseguinte a variabilidade dos valores de \\(x_j\\). Por isso alguns autores se referem de maneira satírica ao problema da multicolinearidade como o problema da micronumerosidade. A razão disso é que, muitas vezes, nossa variância é alta porque nossa amostra é muito pequena."
  },
  {
    "objectID": "lineares.html#heteroscedasticidade",
    "href": "lineares.html#heteroscedasticidade",
    "title": "7  Modelos Lineares",
    "section": "11.2 Heteroscedasticidade",
    "text": "11.2 Heteroscedasticidade\nA heteroscedasticidade é a violação da hipótese de homoscedasticidade. Nesse caso, a variância do termo de erro \\(u\\) deixa de ser constante. Logo cada observação passa a ter sua própria variância.\nNão entraremos nos detalhes técnicos de como resolver o problema. Felizmente, a heterocesdasticidade pode ser detectada e corrigida sem grandes problemas.\nA heteroscedasticidade, embora não interfira no viés das estimativas de MQO, interfere na variância. Quando ela esta presente nossos estimadores de MQO não serão mais eficientes. Isto é, MQO deixa de ter variância mínima. Com isso os erros padrão reportados pelo software estátistico serão problemáticos e todo nosso procedimento de inferência poderá estar comprometido.\nUma saída é computar erros-padrão robustos para nossos estimadores. Tais erros padrão robustos à heteroscedasticidade são maiores que os erros padrão convencionais. Não obstante, podemos nos resguardar do problema da heteroscedasticidade.\nVamos verificar isso com um exemplo:\n\n### Aplicação: Erros Padrão Robustos à Heteroscedasticidade\n\nPara detectar formalmente a heteroscedasticidade, podemos utilizar os seguintes testes: teste de Breusch-Pagan (BP) e o Teste de White.\n\n####Teste de Breusch-Pagan para Heteroscedasticidade\n\n\n\n#####Teste de White para a detecção de Heteroscedasticidade"
  },
  {
    "objectID": "lineares.html#viés-de-variável-omitida-e-a-correlação-entre-as-variáveis-explicativas-e-o-termo-de-erro.",
    "href": "lineares.html#viés-de-variável-omitida-e-a-correlação-entre-as-variáveis-explicativas-e-o-termo-de-erro.",
    "title": "7  Modelos Lineares",
    "section": "11.3 Viés de Variável Omitida e a Correlação entre as variáveis explicativas e o termo de Erro.",
    "text": "11.3 Viés de Variável Omitida e a Correlação entre as variáveis explicativas e o termo de Erro."
  },
  {
    "objectID": "descritiva.html",
    "href": "descritiva.html",
    "title": "6  Análise Descritiva",
    "section": "",
    "text": "7 Conceitos Básicos de Probabilidade e Estatística"
  },
  {
    "objectID": "descritiva.html#probabilidade-conceitos-básicos",
    "href": "descritiva.html#probabilidade-conceitos-básicos",
    "title": "6  Análise Descritiva",
    "section": "7.1 Probabilidade Conceitos Básicos",
    "text": "7.1 Probabilidade Conceitos Básicos"
  },
  {
    "objectID": "descritiva.html#medidas-de-tendência-central",
    "href": "descritiva.html#medidas-de-tendência-central",
    "title": "6  Análise Descritiva",
    "section": "7.2 Medidas de Tendência Central",
    "text": "7.2 Medidas de Tendência Central"
  },
  {
    "objectID": "descritiva.html#variáveis-aleatórias-e-esperança-matématica",
    "href": "descritiva.html#variáveis-aleatórias-e-esperança-matématica",
    "title": "6  Análise Descritiva",
    "section": "7.3 Variáveis Aleatórias e Esperança Matématica",
    "text": "7.3 Variáveis Aleatórias e Esperança Matématica\n\n7.3.1 Uma Breve Introdução as Variáveis Aleatórias Conjuntamente Distribuídas"
  },
  {
    "objectID": "descritiva.html#conceitos-básicos-inferência-estatística",
    "href": "descritiva.html#conceitos-básicos-inferência-estatística",
    "title": "6  Análise Descritiva",
    "section": "7.4 Conceitos Básicos Inferência Estatística",
    "text": "7.4 Conceitos Básicos Inferência Estatística\n\n7.4.1 População, Amostra, Parâmetros e Estimadores"
  },
  {
    "objectID": "manipulacao.html#criando-um-projeto",
    "href": "manipulacao.html#criando-um-projeto",
    "title": "5  Manipulação de Dados",
    "section": "5.1 Criando um projeto",
    "text": "5.1 Criando um projeto\nPrimeiramente precisamos criar um projeto. Clique no icone +R crie um projeto chamado feminicídio em uma nova pasta"
  },
  {
    "objectID": "manipulacao.html#instalando-os-packages-necessários",
    "href": "manipulacao.html#instalando-os-packages-necessários",
    "title": "5  Manipulação de Dados",
    "section": "5.2 Instalando os Packages Necessários",
    "text": "5.2 Instalando os Packages Necessários"
  }
]