[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jurimetria",
    "section": "",
    "text": "1 Prefácio"
  },
  {
    "objectID": "index.html#nossa-motivação",
    "href": "index.html#nossa-motivação",
    "title": "Jurimetria",
    "section": "1.1 Nossa Motivação:",
    "text": "1.1 Nossa Motivação:\nEsse curso foi criado pensando em profissionais com formação em direito, estatística, tecnologia da informação e demais profissionais que atuam tanto no judiciário quanto em tribunais administrativos. A ideia é colocar juntas diversas visões sobre um mesmo problema para buscarmos soluções mais criativas e eficicientes para as desigualdades nacionais."
  },
  {
    "objectID": "index.html#nosso-objetivo",
    "href": "index.html#nosso-objetivo",
    "title": "Jurimetria",
    "section": "1.2 Nosso Objetivo:",
    "text": "1.2 Nosso Objetivo:\nÉ oferecer aos participantes uma abordagem probabilística do direito aplicado, tendo como foco os casos práticos e discutindo questões relativas ao direito e desigualdades. Especificamente, objetiva-se ensinar um conjunto de métodos quantitativos e oferecer ferramentas para coleta, transformação e análise de dados jurídicos disponibilizados nas páginas dos tribunais de justiça."
  },
  {
    "objectID": "index.html#nosso-problema",
    "href": "index.html#nosso-problema",
    "title": "Jurimetria",
    "section": "1.3 Nosso Problema:",
    "text": "1.3 Nosso Problema:\nTeremos como tema de fundo nesse curso os processos de homicídio feminino. Especificamente queremos compreender qual o perfil e os determinantes do feminicidio no Estado de São Paulo."
  },
  {
    "objectID": "summary.html#resumo",
    "href": "summary.html#resumo",
    "title": "3  Sumário do Curso",
    "section": "3.1 Resumo:",
    "text": "3.1 Resumo:\nA pesquisa no direito e a ciência de dados: introdução, relevância e experiências; Coleta, transformação e estruturação de dados processuais com R; Estatística descritiva; Aprendizado estatístico (regressão linear, regressão logística); Aprendizado estatístico (machine learning); Interpretação dos resultados e elaboração de relatório."
  },
  {
    "objectID": "summary.html#plano-detalhado-das-aulas",
    "href": "summary.html#plano-detalhado-das-aulas",
    "title": "3  Sumário do Curso",
    "section": "3.2 Plano detalhado das aulas:",
    "text": "3.2 Plano detalhado das aulas:\n\nA pesquisa no direito e a ciência de dados: introdução, relevância e experiências - 25/04\nColeta, transformação e estruturação de dados processuais com R - 02/05\nColeta, transformação e estruturação de dados processuais com R - 09/05\nAnálise descritiva e visualização de dados processuais - 16/05\nModelos lineares aplicados a pesquisa em direito – regressão linear - 23/05\nModelos para diferentes tipos de estrutura de dados e variáveis. - 06/06\nAprendizado estatístico (machine learning) - 13/06\nAprendizado estatístico (machine learning) - 20/06\nInterpretação dos resultados e elaboração do relatório - 27/06"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introdução ao Feminicídio",
    "section": "",
    "text": "Nessa seção pensei em falarmos um pouco no no caso referências etc…."
  },
  {
    "objectID": "lineares.html",
    "href": "lineares.html",
    "title": "7  Modelos Lineares",
    "section": "",
    "text": "8 Regressão Linear Simples - RLS\nA RLM é uma extensão natural da RLS. Entretanto, ao invés de termos apenas uma variável explicativa podemos ter \\(k\\) variáveis dependentes\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\ldots + \\beta_k x_k + u\\] A hípótese fundamental é a de que:\n\\[E(u|x_1, . . . ,x_k) = 0\\]\nQualquer problema que faça qualquer um dos \\(x_1, ...., x_k\\) ser correlacionado com \\(u\\) invalida a hipótese acima. Tal hipótese implica em não viés do MQO."
  },
  {
    "objectID": "lineares.html#introdução",
    "href": "lineares.html#introdução",
    "title": "7  Modelos Lineares",
    "section": "8.1 Introdução",
    "text": "8.1 Introdução\nConsidere o modelo abaixo\n\\[y = \\beta_0 + \\beta_1x + u \\] Nesta equação, \\(y\\) é a variavel dependente ou também denominada de variável explicativa; \\(x\\) é a variável explicativa e \\(u\\) é o termo de erro. Essa equação é uma equação de regressão linear simples.\nConsidere um exemplo mais concreto de uma equação de regressão linear simples. Sumponha que gostariamos de explicar a taxa de crimes nos bairros de uma cidade, dado os níveis de desemprego na localidade. Nosso modelo de regressão poderia ser específicado da seguinte maneira:\n\\[crime_i = \\beta_0 + \\beta_1Desemprego_i + u\\] O subscrito \\(i\\) se refere a um bairro hipotético da cidade. Note que nesse caso,\\(i\\) é o subscrito que relaciona nossas unidades de observação (bairros). Nossas unidades de observação podem variar a depender do contexto em estudo: países, cidades, bairros, estados. No caso das aplicações em Jurimetria, por exemplo, podemos ter como unidade de observação varas onde tramitam os processos.\nNote que, novamente o termo de erro, \\(u\\), está presente na equação. O termo de erro, não observado, capta tudo aquilo que afeta \\(crime\\), mas que não estamos controlando. Adiante faremos hipóteses sobre as características de \\(u\\) que irão nos auxiliar na analise da RLS.\nVoltemos a equação básica:\n\\[y = \\beta_0 + \\beta_1x + u \\]\nNa analise de RLS estamos interessados nos parametros \\(\\beta_0\\) e sobretudo \\(\\beta_1\\). A razão primordial para isso é que, tudo o mais constante, a relação acima aponta que\n\\[\\Delta y = \\beta_1 \\Delta x \\] , se \\(\\Delta u = 0\\)\nIsto é, se tudo o mais que afeta \\(y\\) permanecer inalterado, uma variáção em \\(x\\), \\(\\Delta x\\), terá um impacto de \\(\\beta_1 \\Delta x\\) em \\(y\\). No exemplo da criminalidade, teremos que:\n\\[\\Delta crime = \\beta_1 \\Delta desemprego \\]\nPor isso, quando conseguimos estimar os parêmetros \\(\\beta\\) estamos mais próximos de entender as relações entre \\(x\\) e \\(y\\) em nossas aplicações.\nCabe destacar algo bastante importante.As relações acima não encerram a questão da causalidade. Como podemos inferir um impacto causal do desemprego na criminalidade se estamos ignorando todos os demais fatores que ficaram de fora do modelo - fatores que são captados em \\(u\\) não observados.\nPara avançarmos no que diz respeito a firmações de carater causais, precisamos fazer hipoteses sobre o comportamento de \\(u\\). Com isso estaremos mais perto de inferir causalidade.\n\n8.1.1 Alguns Exemplos de Regressão Aplicados ao Contexto Jurídico:\nConsidere os seguintes exemplos de RLS aplicadas ao contexto do Direito\nPrevisão de Sentenças com Base na Gravidade do Crime: Onde, a variável dependente e a Duração da Sentença, e a variável independente é a Gravidade do Crime, \\(\\beta_0\\)é o intercepto da regressão. \\(\\beta_1\\) é o coeficiente de regressão que representa como a gravidade do crime influência a duração da sentença, \\(u\\) é o termo de erro.\n\\[Dsent = \\beta_0 + \\beta_1{\\text{Gravidade}} + u\\]\nAnálise de Fatores que Influenciam o Tempo de Julgamento: Variável Independente (\\(x\\)), Tipo de Processo (por exemplo, criminal, civil, administrativo). Variável Dependente (\\(y\\), Tempo de Duração do Processo. Objetivo: Identificar se o tipo de processo tem impacto no tempo que um caso leva para ser concluído.\n\\[Tempo = \\beta_0 + \\beta_1  Processo  + u\\]\nPrevisão de Probabilidade de Recurso com Base em Decisões Anteriores: Variável Independente (\\(x\\)): Resultado da Decisão Anterior (por exemplo, deferimento ou negação do recurso). Variável Dependente (\\(y\\)): Probabilidade de Entrada com Recurso. Objetivo: Determinar a probabilidade de um recurso ser apresentado com base no resultado de decisões passadas.\n\\[Resultado = \\beta_0 + \\beta_1 Resultado_{-1} + u \\] Análise de Relação entre Número de Testemunhas e Veredito: Variável Independente (\\(x\\)): Número de Testemunhas. Variável Dependente (\\(y\\)): Veredito (por exemplo, culpado ou inocente). Objetivo: Investigar se o número de testemunhas influencia o veredito\n\\[ Veredito = \\beta_0 + \\beta_1 Testemunhas + u \\]"
  },
  {
    "objectID": "lineares.html#hipóteses-sobre-o-comportamento-do-termo-de-erro",
    "href": "lineares.html#hipóteses-sobre-o-comportamento-do-termo-de-erro",
    "title": "7  Modelos Lineares",
    "section": "8.2 Hipóteses sobre o comportamento do Termo de Erro",
    "text": "8.2 Hipóteses sobre o comportamento do Termo de Erro\nSe especificamos o modelo com \\(\\beta_0\\), podemos assumir que \\(u\\), tem média igual a zero. Em notação de esperança matematica essa hipotese equivale a:\n\\[E(u) = 0\\] Note que, \\(x\\) é \\(u\\) são variaveis aleatórias, então então podemos definir a distribuição condicional de \\(u\\) dado qualquer valor de \\(x\\). Especialmente, para qualquer valor de \\(x\\) podemos obter o valor médio de \\(u\\) para uma parte da populção com uma dado valor de \\(x\\).\nCom base nisso, podemos definir uma segunda hipotese sobre \\(u\\). Uma hipótese bastante forte, é a de que \\(u\\) e \\(x\\) são independentes. Tal hípótese é a hipótese crucial do modelo de RLS:\n\\[E(u|x) = 0\\]\nEssa hipótese é denominada de hipótese de independência da média de \\(x\\).\nJustas as hipóteses de \\(E(u) = 0\\) e \\(E(u|x) = 0\\) são denominadas de hipotese de média condicional zero.\nAo adcionarmos essas hipoteses em nosso arcabouço de estudo RLS, temos uma nova interpretação do parametro \\(\\beta_1\\) (focaremos mais neste parametro, visto que $ é em geral menos importante na maioria das aplicações de análise de regressão)\nConsidere o modelo baixo padrão\n\\[y = \\beta_0 + \\beta_1x + u\\] Aplicando o operador \\(E( \\cdot |x )\\), obetmos\n\\[ E( y |x ) = \\beta_0 + \\beta_1x \\] Assim, na equação acima, temos um aumento de uma unidade em \\(x\\), implica em um aumento no valor esperado (ou em média) de \\(y\\) na magnitude de \\(\\beta_1\\).\nPara todo \\(x\\) a distribuição dos valores de \\(y\\) estarão centratas ao redor da média condicional de \\(y\\) dado \\(x\\), $ E( y |x )$. Lembre que a média é uma medida de tendencia central. Nesse sentido, a regressão linear simples calcula uma média condicional.\nA equação acima, é caracterizada como função de regressão populacional. Essa função nos fornece a elação entre os diferentes níveis de\\(x\\) é o nível médio de \\(y\\), isto é \\(E( y |x )\\).\n\n\n\nFunção de Regressão Populacional\n\n\nAgora, podemos voltar a nossa equação base e verificarmos o progresso feito no entendimento do modelo:\n\\[y = \\beta_0 + \\beta_1x + u \\] \\[y = E( y |x ) + u \\] Na equação acima, \\(E( y |x )\\) é chamada de parte sistematica de \\(y\\). Isto é, a parte sistematicamente explicada por \\(x\\). Já o termo de erro não observado, \\(u\\) é a parte não sistematica de \\(y\\), não explicada por \\(x\\)."
  },
  {
    "objectID": "lineares.html#estimação-dos-parâmetros-da-rls",
    "href": "lineares.html#estimação-dos-parâmetros-da-rls",
    "title": "7  Modelos Lineares",
    "section": "8.3 Estimação dos Parâmetros da RLS",
    "text": "8.3 Estimação dos Parâmetros da RLS\nNão conhecemos \\(\\beta_0\\) e \\(\\beta_1\\) queremos estimadores desses parametros: \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\).\nSuponha que tenhamos uma amostra aleatória da população: \\(\\{(x_i, y_i): i = 1, ..., n\\}\\)\nEm nosso modelo base:\n\\[y_i = \\beta_0 + \\beta_1x_i + u_i \\]\nonde \\(u_i\\) é o erro aleatório da \\(i\\)-ésima observação.\nComo estimamos \\(\\beta_0\\) e \\(\\beta_1\\)?\nUtilizando as hipoteses \\(E(u) = 0\\) e \\(E(u|x) = 0\\), podemos utilizar aplicar um estimador de metodos dos momentos.\nComo \\(x\\) e \\(u\\) são não correlacionados (são independentes), a covariancia entre eles é 0, \\(Cov(x,u) = 0\\). Pode-se demostrar que\n\\[Cov(x,u) = E(ux) - E(u)E(x)\\]\nMas como, por hipotese, \\(E(u) = 0\\), a equação anterior se reduz à\n\\[Cov(x,u) = E(ux)\\] Assim, como \\(Cov(x,u)\\) é \\(0\\), temos também que\n\\[E(ux) = 0\\] Logo, temos que \\[E(u) = 0\\] e\n\\[E(xu)=0\\] Essas duas ultimas igualdades, são denominadas condições de momentos populacionais. Elas são hipóteses feitas na população.\nPara aplicarmos o Método dos Momentos, precisamos igual as condições de momento populacionais as condições de momentos amostrais.\nAntes, vamos reescrever as condições populacionais.\nComo\n\\[y = \\beta_0 + \\beta_1x + u \\]\nPodemos isolar \\(u\\) e obtermos\n\\[ u  =  y - \\beta_0  - \\beta_1x \\] Com isso podemos reescrever as hipoteses \\(E(u) = 0\\) e \\(E(ux) = 0\\) reespectivamente como\n\\[E( y - \\beta_0  - \\beta_1x) = 0\\]\n\\[E(x( y - \\beta_0  - \\beta_1x)) = 0\\] Os equivalentes amostrais serão dados por:\n\\[n^{-1} \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 = 0\\] e\n\\[ n^{-1} \\sum_{i=1}^{n} x_i (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 = 0\\]\nAs duas equações acima formam um sistema de 2 equações com 2 incognitas, \\(\\beta_0\\) e \\(\\beta_1\\). Com algumas manipilações algébricas é possível demostrar que, os valores de dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) serão dados por:\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\ne\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\nOnde, {x} e {y} são as médias amostrais de \\(x\\) e \\(y\\) respectivamente.\nOs estimadores \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) são chamados de estimadores de Mínimos Quadrados Ordinários (MQO). Note que, entretanto, método de estimação para encontrarmos \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\). Na verdade, pode-se mostrar que as estimativas do modelo de RLS serão equivalentes pelos dois métodos, dadas as hipoteses enunciadas anteriormente.\nA ideia é que os parametros que estimamos, \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\), são os parametros que minimizam a soma dos quadrados das diferenças entre nossos \\(y_i\\) obversados e seus valores preditos definidos como\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\]\nO raciocinio é o seguinte. Vamos definir a distancia entre \\(y_i\\) é \\(\\hat{y}_i\\), denominada de resíduo da equação como\n\\[\\hat{u_i} = \\hat{y}- \\hat{y}_i = y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1x\\]\nNote que em uma amostra de tamanho \\(n\\), teremos \\(n\\) desses resíduos. Esconlhendo os parametros \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) que minimizam a soma dos quadrados desses resíduos:\n\\[\\sum_{i=1}^{n} \\hat{u}_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\\]\nFormalmente a soma dos quadrados dos resíduos é uma função de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\). Chamemos essa função de \\(Q(\\hat{\\beta_0},\\hat{\\beta_1} )\\):\n\\[Q(\\hat{\\beta_0},\\hat{\\beta_1} ) = \\sum_{i=1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\\] Utilizando tecnicas de minimização do cálculo (foras do escopo deste curso), obteremos os valores de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\):\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\ne\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\] Tal qual obtidos anteriormente.\nIndependentemente do método escolido, conseguimos obter a Função de Regressão Amostral (FRA), que é a versão estimada da Função de Regressão Populacional (FRP)\nA FRP é fixa e desconhecida, a FRA varia de amostra para amostra.\nFormalmente, a FRA é dada por\n\\[\\hat{y}= \\hat{\\beta}_0 + \\hat{\\beta}_1x\\]\nComo mencionado anteriormente, nosso interresse primordial reside em \\(\\hat{\\beta}_1\\)\n\n\n\nValores Ajustados e Resíduos de MQO\n\n\nNote que conforme já comentado:\n\\[\\Delta \\hat{y} = \\hat{\\beta}_1 \\Delta x \\]\nIsolando \\(\\hat{\\beta_1}\\)\n\\[ \\hat{\\beta}_1  = \\Delta \\hat{y} /\\Delta x \\]\nAssim, uma variáção, \\(\\Delta\\), em \\(x\\), nos permimite calcular o impacto em \\(y\\), medido dado por \\(\\hat{\\beta}_1\\).\nAntes de passarmos para o proxímo tópico de RLS, cabe mencionar uma nota sobre terminologia: quando estimamos equações de RLS do tipo \\(y = \\beta_0 + \\beta_1 x + u\\), dizemos que “rodamos a regressão de \\(y\\) sobre \\(x\\) !”\n\n8.3.1 Aplicação - Estimando uma Regressão Linear Simples\n\n### Aplicação da RLS - Método dos Mínimos Quadrados Ordinários"
  },
  {
    "objectID": "lineares.html#caracteristicas-do-método-dos-mínimos-quadrados-ordinários-em-determinadas-amostras-de-dados",
    "href": "lineares.html#caracteristicas-do-método-dos-mínimos-quadrados-ordinários-em-determinadas-amostras-de-dados",
    "title": "7  Modelos Lineares",
    "section": "8.4 Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados",
    "text": "8.4 Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados\n\n8.4.1 Valores estimados e Resíduos\nUma vez estimados \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) temos os valores ajustados ou também denominados de valores preditos ou fitted values.\nPara uma observação qualquer \\(i\\), seu valor estimado é:\n\\[\\hat{y}_i= \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\]\nDe maneira geral teremos a reta de regressão, dada por:\n\\[\\hat{y}= \\hat{\\beta}_0 + \\hat{\\beta}_1x\\]\nTodos os valores \\(\\hat{y}\\) estarão sobre a reta de regressão.\nO resíduo, tal qual definido anteriormente, será a diferença entre o valor ajustado \\(\\hat{y}\\) e o verdadeiro \\(y\\) em nosso banco de dados:\n\\[\\hat{u}_i = y_i - \\hat{y}_i\\]\nSe \\(\\hat{u_i} &gt; 0\\) a regressaõ subestima \\(y_i\\). Se \\(\\hat{u_i} &lt; 0\\) a reta superestima \\(y_i\\). O cenário ideal é quando \\(\\hat{u}_i= 0\\), algo que quase nunca acontece.\n\n\n8.4.2 Propriedades Algébricas do MQO\n\nA Soma dos Resíduos é zero:\n\n\\[\\sum_{i=1}^{n} \\hat{u}_i = 0\\]\n\nA covariancia amostral entre a variavel explicativa e os resíduos é zero:\n\n\\[\\sum_{i=1}^{n} x_i \\hat{u}_i = 0\\]\n\nO ponto \\((\\bar{x},\\bar{y})\\) sempre estará sob a reta de regressão.\nA média dos valores estimados, \\(\\bar{\\hat{y}}\\) é igual a média dos valores observados \\(\\bar{y}\\).\nNote que as estimatvas de MQO decompõe \\(y\\) em 2 partes: 1) os valores ajustados \\(\\hat{y}\\) e os resíduos \\(\\hat{u}\\).\nOs valores de \\(\\hat{y}\\) e \\(\\hat{u}\\) são não correlacionados na amostra!\n\n\n\n8.4.3 Qualidade do Ajuste\nNesta seção vamos responder a seguinte questão: “Quão bem \\(x\\) explica \\(y\\) ?”\nConsidere as seguintes definições\nSoma dos Quadrado Totais (SQT):\n\\[\\text{SQT} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nSoma dos Quadrados Explicados (SQE):\n\\[\\text{SQE} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nSoma dos Quadrados dos Resíduos (SQR):\n\\[\\text{SQR} = \\sum_{i=1}^{n} \\hat{u}_i^2\\] Temos alguns resultados bem importantes. Note que, ao dividirmos \\(SQT\\) por \\(n-1\\), teremos a variancia amostral de \\(y\\). Por sua vez, ao dividirmos \\(SQE\\) por \\(n-1\\), termeos a variancia populacional de \\(\\hat{y}\\).\nEntretanto, o resultado mais importante é o seguinte:\n\\[SQT =  SQE + SQR\\] É apartir dessa iguadade que podemos mostrar algo sobre o ajuste dos MQO.\nDívidindo ambos os lados por \\(SQT\\) teremos\n\\[1 = \\frac{\\text{SQE}}{\\text{SQT}}  + \\frac{\\text{SQR}}{\\text{SQT}}\\] Rearranjando os termos\n\\[R^2 = \\frac{\\text{SQE}}{\\text{SQT}} = 1 - \\frac{\\text{SQR}}{\\text{SQT}}\\]\nO \\(R^2\\) é a porcentagem da variação de y que é explicada por \\(x\\). O valor de \\(R^2\\) sempre estará na RLS entre \\(0\\) e \\(1\\).\nO nome \\(R^2\\) advém do fato de que, pode ser mostrado que tal médida é equivalente ao quadrado do coeficiente de correlação entre \\(\\hat{y}\\) e \\(y\\)."
  },
  {
    "objectID": "lineares.html#ausência-de-viés-e-variância-dos-estimadores-de-mqo",
    "href": "lineares.html#ausência-de-viés-e-variância-dos-estimadores-de-mqo",
    "title": "7  Modelos Lineares",
    "section": "8.5 Ausência de Viés e Variância dos Estimadores de MQO",
    "text": "8.5 Ausência de Viés e Variância dos Estimadores de MQO\n\n8.5.1 Ausencia de Viés em MQO\nUm estimador não viésado (ou não tendêncioso) por definição, um estimador que em média “acerta” o verdadeiro valor do parâmetro estimado.\nConsidere um estimador arbitrario \\(\\hat{\\theta}\\). Quando dezemos que \\(\\hat{\\theta}\\) é não viesado, queremos dizer que\n\\[E(\\hat{\\theta}) = \\theta\\]\nSerá que os \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) estimados pelo método do MQO são não viésados?\nPara responder a essa pergunta precisamos, mais uma vez, recorrer a novas hipoteses sobre os dados. Esse novo conjunto de hipoteses irão nos permitir verificarmos algumas propriedades estatísticas do método de MQO. A depender das hipóteses, umas mais fracas que outras, as propriedades podem mudar.\nA garantia da ausência de viés, em dados de corte transversal, requer quatro hipóteses:\n1) Linearidade dos Parametros: No modelo Populacional a variavel dependente \\(y\\) está relacionada a variável dependente \\(x\\) e ao termo de erro \\(u\\) como:\n\\[y = \\beta_0 + \\beta_1x + u \\] Por exemplo no modelo \\(y= \\beta_0 + e^{\\beta_1} + u\\), não é linear no parametro \\(\\beta_1\\).\n2) Amostragem Aleatória: Usamos uma amostra aleatória de tamanho \\(n\\), \\(\\{(x_i, y_i): i = 1, 2, ..., n\\}\\) proveniente de um modelo populacional.\n3) Existe Variação Amostral em \\(x\\): Isto é, os valores de nossa variável explicativa não são todos iguais.\n4) Independencia da Média: O termo de erro \\(u\\) tem um valor esperado de zero, dado qualquer valor da variável explicativa.\\[E(u|x) = 0\\]\nDadas as hipóteses 1, 2, 3 e 4:\n\\[E(\\hat{\\beta_1}) = \\beta_1\\]\ne\n\\[E(\\hat{\\beta_0}) = \\beta_0\\]\nPortantos nossoa estimadores de MQO, são não viésados.\n\n\n8.5.2 Variância dos estimadores de MQO\nQuão distantes em média \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) estão dos verdadeiros parâmetros? Saber disso nos permite escolher o melhor estimador entre todos os etimadores não viesados - ou ao menos entre uma ampla classe deles.\nPara obetermos expressões tratáveis das variâncias do MQO recorremos a mais uma hipótese.\n5) Hipótese de Homoscedasticidade: O erro \\(u\\) tem a mesma variâcia, dado qualquer valor da variável explicativa. Isto é,\n\\[Var(u|x)=\\sigma^2\\]\nA variância de \\(u\\) pode ser reescrita em termos de esperança:\n\\[Var(u|x)= \\sigma^2 = E(u^2|x) - [E(u|x)]^2\\]\nMas como temos por hipótese que \\([E(u|x)]=0\\), a hipótese de homoscedasticidade as vezes é representada como\n\\[ E(u^2|x) = \\sigma^2\\]\nNote também, que como \\(E(u)=0\\), a variancia do erro, não condicional a \\(x\\), será dada por:\n\\[\\sigma^2=E(u^2)=Var(u)\\] Por isso chamamos \\(\\sigma^2\\) de variância do erro.\nPodemos “vizualizar” a hipótese de homoscedasticidade na figura abaixo:\n\n\n\nModelo de Regressão Sob Homoscedasticidade\n\n\nApartir das hiopóteses 1,2,3,4 e 5 podemos progredir no entendimento dos MQO. Sob as hipoteses 1 - 5, as variancias de \\(\\hat{\\beta_1}\\) e \\(\\hat{\\beta_0}\\) são dadas por:\n\\[\\text{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}=\\frac{\\sigma^2}{\\text{SST}_x}\\] e\n\\[\\text{Var}(\\hat{\\beta}_0) = \\frac{\\sigma^2n^{-1}\\sum_{i=1}^{n} x_i^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\nUma extenção da variância dos estimadores de MQO são seus desvios padrão definidos simplismente pela raiz quadrada da variância:\n\\[ep(\\hat{\\beta}_0) = \\sqrt{Var(\\hat{\\beta_0})}\\]\ne\n\\[ep(\\hat{\\beta}_1) = \\sqrt{Var(\\hat{\\beta_1})} = \\frac{\\sigma}{\\sqrt{SQT_x}}\\]\n\n\n8.5.3 Estimação da Variância do erro\nNote que, nas formulas das variâncias e dos desvios-padrão, o que aparece explicitamente é o parâmetro populacional, \\(\\sigma^2\\) e \\(\\sigma\\). Raramente conhecemos esses valores, logo as formulas derivadas acima terão pouca utilidade. Não obstante, caso encontremos um estimador para \\(\\sigma^2\\) e \\(\\sigma\\) podemos usar esse estimador.\nPodemos utilizar os resíduos para construir um estimador da variância dos erros. Lembre que o resíduos da \\(i\\)-ésima observação do modelo é dado por:\n\\[\\hat{u_i} = y_i - \\hat{y_i}\\] Pode-se demostrar que, um estimador não viesado de \\(\\sigma^2\\) será dado por:\n\\[\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^{n} \\hat{u}_i^2}{n-2} = \\frac{\\text{SQR}}{n-2}\\]\nCom \\(\\hat{\\sigma}^2\\) em mãos, podemos reescrever os erros padrão dos estimadores de MQO:\n\\[ep(\\hat{\\beta}_1) = \\sqrt{Var(\\hat{\\beta_1})} = \\frac{\\hat{\\sigma}^2}{\\sqrt{SQT_x}}\\]\nOs erros padrão são utilizados para calcularmos as estatísticas de teste e Intervalos de Confiança para as Estimativas. Mais a frente iremos nos concentar nessas questões."
  },
  {
    "objectID": "lineares.html#aplicação-de-rls---foco-no-erro-padrão-do-parametro-b1",
    "href": "lineares.html#aplicação-de-rls---foco-no-erro-padrão-do-parametro-b1",
    "title": "7  Modelos Lineares",
    "section": "8.6 Aplicação de RLS - Foco no erro padrão do parametro b1",
    "text": "8.6 Aplicação de RLS - Foco no erro padrão do parametro b1\n\n### Aplicação do de RLS - Foco no erro padrão"
  },
  {
    "objectID": "lineares.html#exemplos-de-regressão-multipla-no-contexto-jurídico",
    "href": "lineares.html#exemplos-de-regressão-multipla-no-contexto-jurídico",
    "title": "7  Modelos Lineares",
    "section": "9.1 Exemplos de Regressão Multipla no Contexto Jurídico",
    "text": "9.1 Exemplos de Regressão Multipla no Contexto Jurídico\nDeterminação de Fatores que Afetam o Valor de Indenizações: Variáveis Independentes: Idade da Vítima, Gravidade do Dano, Jurisdição. Variável Dependente: Valor da Indenização.Objetivo: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas\n\\[Indenização = \\beta_0 + \\beta_1 idade+ \\beta_2 Gravidade + \\beta_2 Jurisdisção + u\\]\nAnálise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais: Variáveis Independentes: Idade do Réu, Tipo de Crime, Local do Julgamento. Variável Dependente: Taxa de Condenação.Objetivo: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.\n\\[Tx.Condenação = \\beta_0 + \\beta_1  idade  +  \\beta_2 Local  + \\beta_3  crime + u\\]\nAnálise de Fatores que Influenciam a Taxa de Feminicídio: Variáveis Independentes (X): Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher. Variável Dependente (Y): Taxa de Feminicídio por 100.000 mulheres. Objetivo: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.\n\\[Tx.feminicidio = \\beta_0 + \\beta_1 desemprego + \\beta_2 educ + \\beta_3 política + u \\]"
  },
  {
    "objectID": "lineares.html#a-interpretação-da-equação-de-regressão-de-mqo",
    "href": "lineares.html#a-interpretação-da-equação-de-regressão-de-mqo",
    "title": "7  Modelos Lineares",
    "section": "9.2 A Interpretação da Equação de Regressão de MQO",
    "text": "9.2 A Interpretação da Equação de Regressão de MQO\nTal qual no modelo de RLS, temos que:\n\\[\\Delta y = \\beta_1 \\Delta x_1 + \\beta_2 \\Delta x_2 + ...+\\beta_k \\Delta x_k\\]\nO coefiente \\(\\beta_j\\), com \\(j=1,...k\\), mede o efeito do incremento de uma unidade de \\(x_j\\) em \\(y\\).\nSuponha que estejamos interessados no impacto de \\(x_1\\). Podemos fazer o seguinte exercício: Tudo o mais constante qual o impacto de \\(x_1\\) em \\(y\\):\n\\[\\frac{\\Delta y}{\\Delta x} = \\beta_1  \\]\nManter outros fatores fixos permite o cientista social, “mimetizar” um experimento, tal qual nas Ciências Naturasi. Obviamente, isso não tão simples assim. Entretanto, mater outros fatores fixos, e supondo que \\[E(u|x_1, . . . ,x_k) = 0\\] , nos aproxíma de afirmações de cunho causal."
  },
  {
    "objectID": "lineares.html#estimação-dos-k-parametros-na-rlm",
    "href": "lineares.html#estimação-dos-k-parametros-na-rlm",
    "title": "7  Modelos Lineares",
    "section": "9.3 Estimação dos k parametros na RLM",
    "text": "9.3 Estimação dos k parametros na RLM\n\n9.3.1 Estimativas de MQO\nA mecânica para conseguirmos estimativas de \\(\\beta_j\\), fica uma pouco mais complicada. Felizmente, os softwares, tais como o R, fornecem essas estimativas com muita facilidade. Não obstante, convêm apresentar uma abordagem para o calculo desses parâmetros. Utilizaremos álgebra de matrizes para mostrar um “algoritmo” para calcular esses \\(\\beta_j\\). Em verdade, esse é o algoritmo utilizado pela função lm() do R no computo dos estimadores.\nConsidere o modelo para a \\(i\\)-ésima observação\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_{i2} x_2 + \\beta_{i3} x_3 + \\ldots + \\beta_k x_{ik} + u\\]\ncomo temos \\(n\\) observações (tamanho de nossa amostra), podemos organizar esses valores em vetores e matrizes.\nPara os valores de \\(y\\) teremos\n\\[\n\\mathbf{y} = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n\\]\nOs valores das variáves independentes \\(X\\) podem ser organizados da seguinte maneira:\n\\[\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & \\cdots & x_{1k} \\\\\n1 & x_{21} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & \\cdots & x_{nk}\n\\end{bmatrix}\\]\nOs \\(\\beta_j\\) são organizados da seguinte maneira:\n\\[\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}\\]\nOs erros \\(u_i\\) serão armazenados da em um vetor coluna tambêm:\n\\[\\mathbf{u} = \\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\vdots \\\\\nu_n\n\\end{bmatrix}\\]\nApartir disso representamos o modelo de RLM da seguinte maneira:\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{u}\\]\nQueremos encontrar um estimador de \\(\\boldsymbol{\\beta}\\), que chamaremos de \\(\\boldsymbol{\\hat{\\beta}}\\). Esse estimador sera dado por:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\]\nOnde \\(\\mathbf{X}^T\\) é a matriz transposta de \\(\\mathbf{X}\\), e \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\) é a matriz inversa da seguinte multiplicação de matrizes, \\((\\mathbf{X}^T \\mathbf{X})\\)\nEsse vetor de estimativas irá nos fornecer os valores para os \\(\\beta_j\\),com \\(j=1,...,k\\). Assim temos que:\n\\[ \\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix}\n\\hat{\\beta}_0 \\\\\n\\hat{\\beta}_1 \\\\\n\\hat{\\beta}_2 \\\\\n\\vdots \\\\\n\\hat{\\beta}_k\n\\end{bmatrix}\\]\nAntes, de prosseguirmos cabe destacar que, embora não precisamos saber as formulas explicitas para dos \\(\\hat{\\beta_j}\\), as vezes isso nos ajuda a entender o funcionamento da regressão multipla e os efeitos parciais que os \\(\\beta_j\\) exercem sobre \\(y\\).\nPor exemplo, \\(\\hat{\\beta_1}\\) pode ser representado da seguinte maneira no contexto da RLM:\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n \\hat{r_{i1}} y_i}{\\sum_{i=1}^n \\hat{r_{i1}}^2}\\]\nOnde \\(r_{i1}\\) são os resíduos da regressão de \\(x_1\\) sobre todas as demais variáveis do modelo \\(x_2,...,x_k\\). Nesse caso, podemos entender que $_1 $ mede o efeito de \\(x_1\\) sobre \\(y\\) após terem sido parcializados ou descontados os impactos de \\(x_2,...,x_k\\).\nPara um \\(\\beta_j\\) qualquer a interpretação é análoga.\n\\[\\hat{\\beta}_j = \\frac{\\sum_{i=1}^n \\hat{r_{ij}} y_i}{\\sum_{i=1}^n \\hat{r_{ij}}^2}\\]"
  },
  {
    "objectID": "lineares.html#qualidade-do-ajuste-na-regressão-linear-multipla",
    "href": "lineares.html#qualidade-do-ajuste-na-regressão-linear-multipla",
    "title": "7  Modelos Lineares",
    "section": "9.4 Qualidade do Ajuste na Regressão Linear Multipla",
    "text": "9.4 Qualidade do Ajuste na Regressão Linear Multipla\nTal qual na regressão linear simples, são válidas as seguintes relações:\nSoma dos Quadrado Totais (SQT):\n\\[\\text{SQT} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nSoma dos Quadrados Explicados (SQE):\n\\[\\text{SQE} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nSoma dos Quadrados dos Resíduos (SQR):\n\\[\\text{SQR} = \\sum_{i=1}^{n} \\hat{u}_i^2\\] Novamente podemos definir a qualidade do ajuste, medido pelo \\(R^2\\), como sendo igual à\n\\[R^2 = \\frac{\\text{SQE}}{\\text{SQT}} = 1 - \\frac{\\text{SQR}}{\\text{SQT}}\\]\nExplicitamente podemos escrever o \\(R^2\\) tal como:\n\\[R^2 =  \\frac{[\\sum_{i=1}^n (y_i - \\bar{y}) ( \\hat{y}_i - \\bar{\\hat{y}} )]^2}{[\\sum_{i=1}^n (y_i - \\bar{y})^2][\\sum_{i=1}^n(\\hat{y}-\\bar{\\hat{y}})^2]}\\]\nUm fato importante sobre \\(R^2\\) é que ele nunca diminui ao incluirmos variáveis no modelo. Isso decorre de propriedades algébricas desse indicador.Por isso, os softwares geralmente reportam uma outra estatística de ajuste o R-quadrado ajustado,\\(\\bar{R}^2\\):\n\\[\\bar{R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\] O \\(\\bar{R}^2\\), tal como expresso acima conta com a presença do \\(R^2\\) original, mas note que, no denominador estamos dividindo por \\(n-k-1\\), onde \\(n\\) é o tamanho da amostra em mãos, e \\(k-1\\) se referem ao fato de termos \\(k\\) variáveis explicativas e uma constante. Portanto, o \\(\\bar{R}^2\\), impões uma penalidade à inclusão de variáveis independentes em um modelo de regressão.\nCabe destacar que, diferente do \\(R^2\\), \\(\\bar{R}^2\\) pode ser negativo, indicando um ajuste bastante pobre do modelo.\nNão obstante, cabe destacar que um \\(R^2\\) baixo não é definitivamente uma evidencia definitiva contra nosso modelo. Em ciências sociais em geral é bastante comum verificarmos \\(R^2\\) relatvamente pequenos. Isso significa que, embora coletovamente as variáveis explicativas não expliquem muito das variações de \\(y\\), é possível que as estimativas de MQO sejam estimativas confiaveis dos efeitos parciais - tudo o mais constante - de cada \\(x_j\\) sobre \\(y\\)."
  },
  {
    "objectID": "lineares.html#valor-espererado-dos-estimadores-de-mqo",
    "href": "lineares.html#valor-espererado-dos-estimadores-de-mqo",
    "title": "7  Modelos Lineares",
    "section": "9.5 Valor Espererado dos Estimadores de MQO",
    "text": "9.5 Valor Espererado dos Estimadores de MQO"
  },
  {
    "objectID": "lineares.html#variância-dos-estimadores-de-mqo-1",
    "href": "lineares.html#variância-dos-estimadores-de-mqo-1",
    "title": "7  Modelos Lineares",
    "section": "9.6 Variância dos Estimadores de MQO",
    "text": "9.6 Variância dos Estimadores de MQO"
  },
  {
    "objectID": "lineares.html#uso-de-variáveis-qualitativas-na-análise-de-rlm",
    "href": "lineares.html#uso-de-variáveis-qualitativas-na-análise-de-rlm",
    "title": "7  Modelos Lineares",
    "section": "9.7 Uso de Variáveis Qualitativas na Análise de RLM",
    "text": "9.7 Uso de Variáveis Qualitativas na Análise de RLM"
  },
  {
    "objectID": "lineares.html#alguns-problemas-que-podem-surgir-na-análise-de-regressão",
    "href": "lineares.html#alguns-problemas-que-podem-surgir-na-análise-de-regressão",
    "title": "7  Modelos Lineares",
    "section": "9.8 Alguns Problemas que podem surgir na análise de regressão",
    "text": "9.8 Alguns Problemas que podem surgir na análise de regressão\n\n9.8.1 Viés de Variável Omitida\n\n\n9.8.2 Multicolinearidade\n\n\n9.8.3 Heteroscedasticidade"
  },
  {
    "objectID": "lineares.html#hipótese-de-normalidade",
    "href": "lineares.html#hipótese-de-normalidade",
    "title": "7  Modelos Lineares",
    "section": "10.1 Hipótese de Normalidade",
    "text": "10.1 Hipótese de Normalidade"
  },
  {
    "objectID": "lineares.html#teste-de-hipotese-sob-um-unico-parametro",
    "href": "lineares.html#teste-de-hipotese-sob-um-unico-parametro",
    "title": "7  Modelos Lineares",
    "section": "10.2 Teste de Hipotese sob um unico Parametro",
    "text": "10.2 Teste de Hipotese sob um unico Parametro\n\n10.2.1 Teste contra Hipoteses Alternativas Unilaterais\n\n\n10.2.2 Teste contra Hipoteses Alternativas Bilaterais\n\n\n10.2.3 Cálculos dos p-valores dos Testes t\n\n\n10.2.4 Teste de Restrições de Lineares Múltiplas: O teste F"
  }
]