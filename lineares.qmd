---
title: "Modelos Lineares"
subtitle: "Aplicação a Pesquisa em Direito"
author:
- Alexandre Nicolella e Felipe Bauer
date: "2024"
format: html
---

Vamos iniciar o estudo de modelos lineares começando pela Regresão Linear Simples (RLS). Mais específicamente, vamos estudar a RLS no contexto de dados de corte transversal. Tal abordagem, segmentada por tipos de dados, facilíta o entendimento das hipóteses do modelo.

Os conceitos estatísticos aplicados no estudo de RLS são os mesmos apresentados na seção anterior.

O matérial desta seção é baseado na 4ed do livro de Jeffrey Wooldridge, Introdução à Econometria: Uma Abordagem Moderna, de 2013. Embora o título do livro remeta à econometria, os modelos apresentados no livro são aplicaveis à diversas Ciências Sociais, tais como o Direito, Ciência Política, Sociologia, Psicologia Empírica etc...

A análise de Regressão é um instrumento poderoso para os centistas sociais. Podemos verificar como determinadas variáveis importantes nas ciencias sociais interagem com outras, as vezes em uma relação de causa e efeito.

Aqui também teremos que ter uma questão ou problema. Anteriormente gostariamos de entender o que estava acontecendo, qual a Renda pc das mulheres, qual a taxa de feminicídio... Agora queremos ir mais a fundo, porque determinados bairros tem mais crime do que outros? O que determina a renda pc de uma mulher?

## Introdução

Começamos com o modelo mais geral da população:

$$y = \beta_0 + \beta_1x + u $$

Nesta equação, $y$ é a variavel dependente ou também denominada de variável explicativa; $x$ é a variável explicativa e $u$ é o termo de erro. Essa equação é uma equação de **regressão linear simples**.

**Vejamos um exemlo**

Gostariamos de explicar a taxa de crimes nos bairros de uma cidade e consideramos que os níveis de desemprego nas localidades são importantes. Nosso modelo de regressão poderia ser específicado da seguinte maneira:

$$Crime_i = \beta_0 + \beta_1Desemprego_i + u$$

O subscrito $i$ se refere a um bairro hipotético da cidade. Note que nesse caso,$i$ é o subscrito que relaciona nossas unidades de observação (bairros). Nossas unidades de observação podem variar a depender do contexto em estudo: países, cidades, bairros, estados, pessoas, processos, varas, tribunais....

Note que, novamente o termo de erro, $u$, está presente na equação. O termo de erro, não observado, capta tudo aquilo que afeta $Crime$, mas que não estamos controlando, ou seja, que não é explicado pelo $Desemprego$.

Voltemos a equação básica:

$$y = \beta_0 + \beta_1x + u $$

Na analise de RLS estamos interessados nos parâmetros $\beta_0$ e sobretudo $\beta_1$. A razão primordial para isso é que, tudo o mais constante, a relação acima aponta que

$$\Delta y = \beta_1 \Delta x $$ , se $$\Delta u = 0$$

Isto é, se tudo o mais que afeta $y$ permanecer inalterado, uma variáção em $x$, $\Delta x$, terá um impacto de $\beta_1 \Delta x$ em $y$. No exemplo da criminalidade, teremos que:

$$\Delta crime = \beta_1 \Delta desemprego $$

Assim, o aumento o de 1 ponto no desemprego irá ter o efeito de $\beta_1$ unidades de crimes em média. Por isso, quando conseguimos estimar os parêmetros $\beta$ estamos mais próximos de entender as relações entre $x$ e $y$ em nossas aplicações.

Cabe destacar algo bastante importante.As relações acima não encerram a questão da causalidade. Como podemos inferir um impacto causal do desemprego na criminalidade se estamos ignorando todos os demais fatores que ficaram de fora do modelo - fatores que são captados em $u$ não observados.

### Alguns Exemplos de Regressão Aplicados ao Contexto Jurídico:

Considere os seguintes exemplos de RLS aplicadas ao contexto do Direito

**Previsão de Sentenças com Base na Gravidade do Crime:** Onde, a variável dependente e tamanho da pena, e a variável independente é a Gravidade do Crime, $\beta_0$é o intercepto da regressão. $\beta_1$ é o coeficiente de regressão que representa como a gravidade do crime influência o tamanho da pena, $u$ é o termo de erro.

$$Dpena = \beta_0 + \beta_1{\text{Gravidade}} + u$$

**Análise de Fatores que Influenciam o Tempo de Julgamento:** Variável Independente ($x$), Tipo de Processo (por exemplo, criminal, civil, administrativo). Variável Dependente ($y$), Tempo de Duração do Processo. Objetivo: Identificar se o tipo de processo tem impacto no tempo que um caso leva para ser concluído.

$$Tempo = \beta_0 + \beta_1  Processo  + u$$

**Previsão de Probabilidade de Recurso com Base em Decisões Anteriores:** Variável Independente ($x$): Resultado da Decisão Anterior (por exemplo, deferimento ou negação do recurso). Variável Dependente ($y$): Probabilidade de Entrada com Recurso. Objetivo: Determinar a probabilidade de um recurso ser apresentado com base no resultado de decisões passadas.

$$Resultado = \beta_0 + \beta_1 Resultado_{-1} + u $$

**Análise de Relação entre Número de Testemunhas e Veredito:** Variável Independente ($x$): Número de Testemunhas. Variável Dependente ($y$): Veredito (por exemplo, culpado ou inocente). Objetivo: Investigar se o número de testemunhas influencia o veredito

$$ Veredito = \beta_0 + \beta_1 Testemunhas + u $$

## Hipóteses sobre o comportamento do Termo de Erro

Se especificamos o modelo com $\beta_0$, podemos assumir que $u$, tem média igual a zero. Em notação de esperança matematica essa hipotese equivale a:

$$E(u) = 0$$

Podemos definir uma segunda hipotese sobre $u$. Uma hipótese forte, é a de que $u$ e $x$ são independentes. Tal hípótese é a hipótese crucial do modelo de RLS:

$$E(u|x) = 0$$

Justas as hipóteses de $E(u) = 0$ e $E(u|x) = 0$ são denominadas de hipotese de média condicional zero.

Dessa forma, no nosso modelo inicial:

$$y = \beta_0 + \beta_1x + u$$

Aplicando o operador $E( \cdot |x )$, obetmos

$$ E( y |x ) = \beta_0 + \beta_1x $$

Assim, na equação acima, temos um aumento de uma unidade em $x$, implica em um aumento no valor esperado (ou em média) de $y$ na magnitude de $\beta_1$.

A equação acima, é caracterizada como **função de regressão populacional**. Essa função nos fornece a elação entre os diferentes níveis de$x$ é o nível médio de $y$, isto é $E( y |x )$.

![Função de Regressão Populacional, fonte: Anderson David R., Sweeney Dennis J., Williams Thomas A. (2019), Statistics for Business & Economics, Cengage Learning; 14th edition](figuras\modelpop.png)

Agora, podemos voltar a nossa equação base e verificarmos o progresso feito no entendimento do modelo:

$$y = \beta_0 + \beta_1x + u $$

$$y = E( y |x ) + u $$

Na equação acima, $E( y |x )$ é chamada de parte sistematica de $y$. Isto é, a parte sistematicamente explicada por $x$. Já o termo de erro não observado, $u$ é a parte não sistematica de $y$, não explicada por $x$.

## Estimação dos Parâmetros da RLS

Não conhecemos $\beta_0$ e $\beta_1$ queremos estimadores desses parametros: $\hat{\beta_0}$ e $\hat{\beta_1}$.

Suponha que tenhamos uma amostra aleatória da população: $\{(x_i, y_i): i = 1, ..., n\}$. Poderia ser uma amostra de crimes e taxa de desemprego por bairros.

Em nosso modelo base:

$$y_i = \beta_0 + \beta_1x_i + u_i $$

onde $u_i$ é o erro aleatório da $i$-ésima observação.

[**Como estimamos** $\beta_0$ e $\beta_1$?]{.underline}

Após algum esforço algébrico, tem-se:

$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

e

$$\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$$

Onde, $\bar{x}$ e $\bar{y}$ são as médias amostrais de $x$ e $y$ respectivamente.

Os estimadores $\hat{\beta}_0$ e $\hat{\beta}_1$ são chamados de estimadores de Mínimos Quadrados Ordinários (MQO).

A ideia é que os parâmetros que estimamos, $\hat{\beta}_0$ e $\hat{\beta}_1$, são os parâmetros que minimizam a soma dos quadrados das diferenças entre nossos $y_i$ obversados e seus **valores preditos** definidos como

$$y_i- \hat{y}_i =y_i -  \hat{\beta}_0 + \hat{\beta}_1x_i=u_i$$ Queremos minimizar o quadrado dessa diferença. Vejamos graficamente:

![Valores Ajustados e Resíduos de MQO](figuras\fittedvalue.png)

Antes de passarmos para o proxímo tópico de RLS, cabe mencionar uma nota sobre terminologia: quando estimamos equações de RLS do tipo $y = \beta_0 + \beta_1 x + u$, dizemos que "*rodamos a regressão de* $y$ sobre $x$ !"

### Aplicação - Estimando uma Regressão Linear Simples

Vamos utilizar aqui o nosso banco de dados anterior sobre feminicídio. É um banco com poucas obserações e possui fim didátco, apesar de serem dados reais.

Uma questão incial é entender quem será $Y$ e quem será $X$. Estamos querendo entender a taxa de feminicídio. A questão é, quais são seus determinantes? Aqui precisamos de alguma teoria...mas or hora, vamos ter como hipótese inicial que quanto maior a taxa de homícídio espera-se que em média a taxa de feminicídio aumente. :

$$Feminc_{tx} = \beta_0 + \beta_1 Homici_{tx} + u$$

Apesar de parecer "muito complicado" na teoria, na prática o `r` estima uma RLS em segundos. A função para estimar é a`lm`, ou linear model.

```{r}
### Aplicação da RLS - Método dos Mínimos Quadrados Ordinários

#carregando o pacote para ler arquivos em excel
load("C:/Users/Alexandre_Nicolella/Aulas/FEA-RP/Jurimetria/jurimetria/final_fem_22.Rdata")

#Vamos chamar a partir de agora nosso banco de dados
dados<- final_fem_22 

##Função para rodar a regressão
modelo <- lm(data = dados, feminic_tx ~ homic_tx)


#Essa função resume a regressão, ja testar a hipótese sobre o coeficiente e da outras estatisticas que abordaremos a seguir.
summary(modelo)

```

#### Entendendo os Resultados

**1. Resíduos** Residuals: apresenta a  distribuição dos resíduos, observa-se que amediana dos resíduos está próxima a zero.

**2.Coeficientes** Em seguida, abaixo de *Coefficients* temos 5 colunas.

*Coluna 1*: Nome da variável

*Coluna 2*: Coeficiente estimado $\hat{\beta_0}$ o intercepto e $\hat{\beta_1}$ do termo da variável homicídio

*Coluna 3*: É o desvio padrão da estimativas de $\beta$

*Coluna 4*: É a estatistica t utilizada para fazer o teste de hipótese e neste caso, tem-se: $$H_0:\beta_0=0$$ e $$H_0:\beta_1=0$$

*Coluna 5*: É o p-valor, a probabilidade de encontrarmos valores mais extremos da estatística t. Os asteriscos indicam \*\*\* signicante a 0,1% \*\* a 1% \* a 5% e $.$ a 10%.

**[Interpretando o Coeficiente]{.underline}**

O coeficiente do intercepto nos diz que quando a taxa de homicídio for 0, ainda existira uma taxa de feminicídio de 1,4 pontos, $\beta_0$, significante a 0,1%. E cada 1 ponto de aumento na taxa de homicídio, aumenta em 0,10 $\beta_1$ pontos a taxa de feminicídio, significante a 10%.

**3. $R^2$**: Em seguida temos o $R^2$ e $R^2$ ajustado. Essas estatísticas calculam quanto da variância da taxa de feminicídio pode ser explicada pela variância da taxa de homicídio. Quando estão próximos a 1 explicam muito, quanto estão próximos a 0 explicam pouco. No caso o nosso $R^2$ ficou baixo e portanto boa parte da variabilidade do feminicídio, não foi explicada pela taxa de homicídio. 

**4. Estaística F**: E por fim a estatística F mostra o grau de ajustamento do modelo. Se ela for significativa diz que o modelo é bem ajustados aos dados. As variáveis explicativas incluídas são importantes para a explicação da taxa de feminicídio.


**VISUALMENTE**

Abaixo tem-se os dados utilizados e a nossa reta de regressão estimada acima. Veja como ela se ajusta a nuve de pontos. 

```{r}

# Ajustando a reta de regessão.
 plot(dados$homic_tx, dados$feminic_tx,
     main = "Taxa de Feminicídio vs.Tx de homicidio",
     xlab = "Taxa de Homicidio",
     ylab = "Taxa de Feminicídio",
     col = "steelblue",          # Cor dos pontos
     pch = 16,              # Forma dos pontos (círculos sólidos)
     cex = 1.0,         # Tamanho dos pontos
     abline(modelo, col = "lightsalmon3", lwd = 3))

```

## Caracteristicas do Método dos Mínimos Quadrados Ordinários em Determinadas Amostras de Dados

### Valores estimados e Resíduos

Uma vez estimados $\hat{\beta}_0$ e $\hat{\beta}_1$ temos os valores ajustados ou também denominados de valores preditos ou *fitted values*.

Para uma observação qualquer $i$, seu valor estimado é:

$$\hat{y}_i= \hat{\beta}_0 + \hat{\beta}_1x_i$$

Todos os valores $\hat{y}$ estarão sobre a reta de regressão.

O resíduo, tal qual definido anteriormente, será a diferença entre o valor ajustado $\hat{y}$ e o verdadeiro $y$ em nosso banco de dados:

$$\hat{u}_i = y_i - \hat{y}_i$$

Se $\hat{u_i} > 0$ a regressaõ subestima $y_i$. Se $\hat{u_i} < 0$ a reta superestima $y_i$. O cenário ideal é quando $\hat{u}_i= 0$, algo que quase nunca acontece.

```{r}
##Obtendo os residuos da regressão
resid <- residuals(modelo)
 k <- density(resid)
  plot(k, xlab="Erro",main="Densidade de Kernel para o erro")
  polygon(k, col="burlywood3", border="burlywood4")
```

### Propriedades Algébricas do MQO

(1) A Soma dos Resíduos é zero:

$$\sum_{i=1}^{n} \hat{u}_i = 0$$

```{r}

###Soma dos resíduos
sum(resid)
#note que é um número praticamente igual a zero

```

(2) A covariancia amostral entre a variavel explicativa e os resíduos é zero:

$$\sum_{i=1}^{n} x_i \hat{u}_i = 0$$

```{r}
#obtendo a variável x
x <- dados$homic_tx

#somando com os residuos
sum(x*resid)


```

(3) O ponto $(\bar{x},\bar{y})$ sempre estará sob a reta de regressão.

(4) A média dos valores estimados, $\bar{\hat{y}}$ é igual a média dos valores observados $\bar{y}$.

```{r}

#y dos dados 
y <- dados$feminic_tx
mean(y)

#y estimado
y_hat <- fitted.values(modelo)
mean(y_hat)

mean(y)==mean(y_hat)
```

(5) Note que as estimatvas de MQO decompõe $y$ em 2 partes: 1) os valores ajustados $\hat{y}$ e os resíduos $\hat{u}$.

(6) Os valores de $\hat{y}$ e $\hat{u}$ são não correlacionados na amostra!



### Qualidade do Ajuste

Nesta seção vamos responder a seguinte questão: "*Quão bem* $x$ explica $y$ ?"

Considere as seguintes definições

Soma dos Quadrado Totais (SQT):

$$\text{SQT} = \sum_{i=1}^{n} (y_i - \bar{y})^2$$

```{r}
# Calcular a média da variável dependente (feminic_tx)
media_feminic_tx <- mean(dados$feminic_tx)

# Calcular a Soma dos Quadrados Totais (SQT)
sqt <- sum((dados$feminic_tx - media_feminic_tx)^2)
sqt
```

Soma dos Quadrados Explicados (SQE):

$$\text{SQE} = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2$$

```{r}

# Calcular a Soma dos Quadrados Explicados (SQE)
sqe <- sum((y_hat - media_feminic_tx)^2)
sqe
```

Soma dos Quadrados dos Resíduos (SQR):

$$\text{SQR} = \sum_{i=1}^{n} \hat{u}_i^2$$

```{r}
residuos_quadrados <- residuals(modelo)^2
sqr <- sum(residuos_quadrados)
sqr
```


O resultado mais importante é o seguinte:

$$SQT =  SQE + SQR$$ 

É apartir dessa iguadade que podemos mostrar algo sobre o ajuste dos MQO.

Dívidindo ambos os lados por $SQT$ teremos

$$1 = \frac{\text{SQE}}{\text{SQT}}  + \frac{\text{SQR}}{\text{SQT}}$$ 

Rearranjando os termos

$$R^2 = \frac{\text{SQE}}{\text{SQT}} = 1 - \frac{\text{SQR}}{\text{SQT}}$$

```{r}

#r-quadrado do modelo
r_quadrado <- 1 - (sqr/sqt)
r_quadrado


```

Novamente, o $R^2$ é a porcentagem da variação de y que é explicada por $x$. O valor de $R^2$ sempre estará na RLS entre $0$ e $1$.




## Unidade de Medida e Forma Funcional

### Unidade de Medida

Vejamos o efeito de unidade de medidas na nossa regressão. 

  1. Se a variável dependente ($Y$) é multiplicada por uma constante $c$, então as estimativas do intercepto e da inclinação também serão multiplicadas por $c$. Vamos multiplicar por 10 e ver o que acontece:

```{r warning=FALSE}
library(dplyr)
dados <- dados |> 
  mutate(feminic_tx10 =feminic_tx*10 )

modelo1 <- lm(data = dados, feminic_tx10 ~ homic_tx)

summary(modelo)$coefficients

summary(modelo1)$coefficients

```

  2. Se a variável explicativa ($X$) é multiplicada por uma contante $c$, então o coeficiente de inclinação será dívido por $c$. Nada acontece com o intercepto. Vamos multiplicar agora a taxa de homicídio por 10

```{r}
dados <- dados |> 
  mutate(homic_tx10 =homic_tx*10 )

modelo2 <- lm(data = dados, feminic_tx ~ homic_tx10)

summary(modelo)$coefficients

summary(modelo2)$coefficients

```



  3. Se a variável explicativa ($X$) é dividida por uma constante $c$, então o coeficiente de inclinação é multiplicado por $c$. Nada acontece com a constante.


```{r}
dados <- dados |> 
  mutate(homic_tx_10 =homic_tx/10 )

modelo3 <- lm(data = dados, feminic_tx ~ homic_tx_10)

summary(modelo)$coefficients

summary(modelo3)$coefficients

```

::: callout-note
## Importante

O $R^2$ e os testes de hipótese não depende das unidades de nossas variáveis.
:::




## Incorporando não linearidades (nas variáveis!)

Vamos agora incorporar a não linearidade nas variáveis e ver como fica a sua interpretação. Aabaixo tem-se uma tabela para interpretar os coeficientes e será explicada nos exemplos abaxo. 



| Modelo | Dependente |  Explicativa | Interpretação de $\beta_1$     |
|--------------------|--------------|--------------|------------------|
| Nivel-nível | $y$     | $x$      | $\beta_1 \Delta x$     |
| Nível-Log   | $y$     | $ln(x)$  | $(\beta_1/100)\%$ $\Delta x$      |
| Log-Nível   | $ln(y)$ | $x$ | $\% \Delta y = (100 \beta_1) \Delta x$ |
| Log_log     | $ln(y)$ | $ln(x)$ | $\% \Delta y = \beta_1\% \Delta x$  |



### Modelo Nível-Nível

Esse é o modelo que fizemos anteriormente e nos diz que uma variação absoluta em $X$ terá um efeito de $\beta$ em Y. 


```{r}
modelo_nn<- lm(data=dados, dados$feminic_tx ~ dados$homic_tx)
summary(modelo_nn)$coefficients
```
O aumento de uma unidade na taxa de homicídio aumenta em 0,1058 a taxa de feminicídio. 



### Modelo Nível-log

Primeiramente vamos criar as variáveis em ln:

```{r}
dados$Lnfeminic_tx <- log(dados$feminic_tx)
dados$Lnhomic_tx <- log(dados$homic_tx)

```

O modelo Nível-Log considerá $Y$ no nível e $X$ no log. As variações relativas de $X$ implicam em variações absolutas constantes em $Y$.


```{r}
modelo_nl<- lm(data=dados, dados$feminic_tx ~ dados$Lnhomic_tx)
summary(modelo_nl)$coefficients
```

A variação relativa de 1\% na taxa de homicídio ($X$), aumenta a taxa de feminicídio em 0,00623 $(\beta_1/100)\%$ 



### Modelo Log-Nível

Nesse modelo $Y$ está em log e $X$ em nível. Variações absolutas em $X$ implicam em variações exponenciais de $Y$.

```{r}

modelo_ln<- lm(data=dados, dados$Lnfeminic_tx~ dados$homic_tx)
summary(modelo_ln)$coefficients

```



Para cada incremento aicional da taxa de homicídio, tem-se um incremento de 4,5\% na taxa de feminicídio $\% \Delta y = (100 \beta_1) \Delta x$. 



### Modelo Log-Log

Nesse modelo tanto $X$ como $Y$ estão em log. Variaões relativas em $X$ implicam em varições relativas em $Y$



```{r}
modelo_ll <- lm(data=dados, dados$Lnfeminic_tx~ dados$Lnhomic_tx)
summary(modelo_ll)$coefficients

```


Um aumento de 1\% percentual nos homicídios aumenta em 0,29\% a taxa de feminicídio nos estados brasileiros.


##  Regressão Linear Múltipla (RLM)

A RLM é uma extensão natural da RLS. Entretanto, ao invés de termos apenas uma variável explicativa podemos ter $k$ variáveis dependentes

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \ldots + \beta_k x_k + u$$ A hípótese fundamental é a de que:

$$E(u|x_1, . . . ,x_k) = 0$$

Problemas que façam com que um dos $x_1, ...., x_k$ ser correlacionado com $u$,  invalida a hipótese acima. Tal hipótese implica em não viés do MQO.

###  Exemplos de Regressão Multipla no Contexto Jurídico

**Determinação de Fatores que Afetam o Valor de Indenizações:** 

Variáveis Independentes: Idade da Vítima, Gravidade do Dano, Jurisdição. Variável Dependente: Valor da Indenização.Objetivo: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas

$$Indenização = \beta_0 + \beta_1 idade+ \beta_2 Gravidade + \beta_2 Jurisdisção + u$$

**Análise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais:** 

Variáveis Independentes: Idade do Réu, Tipo de Crime, Local do Julgamento. Variável Dependente: Taxa de Condenação.Objetivo: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.

$$Tx.Condenação = \beta_0 + \beta_1  idade  +  \beta_2 Local  + \beta_3  crime + u$$

**Análise de Fatores que Influenciam a Taxa de Feminicídio:** 

Variáveis Independentes (X): Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher. Variável Dependente (Y): Taxa de Feminicídio por 100.000 mulheres. Objetivo: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.

$$Tx.feminicidio = \beta_0 + \beta_1 desemprego + \beta_2 educ + \beta_3 política + u $$

### A Interpretação da Equação de Regressão de MQO

Tal qual no modelo de RLS, temos que:

$$\Delta y = \beta_1 \Delta x_1 + \beta_2 \Delta x_2 + ...+\beta_k \Delta x_k$$

O coefiente $\beta_j$, com $j=1,...k$, mede o efeito do incremento de uma unidade de $x_j$ em $y$. Isso continua com a mesma interpretação para modelos que usam o log. 

Suponha que estejamos interessados no impacto de $x_1$. Podemos fazer o seguinte exercício: Tudo o mais constante qual o impacto de $x_1$ em $y$:

$$\frac{\Delta y}{\Delta x} = \beta_1 $$

Manter outros fatores fixos permite o cientista social, "mimetizar" um experimento, tal qual nas Ciências Naturais. Obviamente, isso não é tão simples assim. Entretanto, manter outros fatores fixos, e supondo que $$E(u|x_1, . . . ,x_k) = 0$$ , nos aproxíma de afirmações de cunho causal.

## Estimação dos k parametros na RLM

###  Estimadores de MQO

A mecânica para conseguirmos estimativas de $\beta_j$, fica uma pouco mais complicada. Felizmente, os *softwares*, tais como o *R*, fornecem essas estimativas com muita facilidade. Não obstante, convêm apresentar uma abordagem para o calculo desses parâmetros. Utilizaremos álgebra de matrizes para mostrar um "algoritmo" para calcular esses $\beta_j$. Em verdade, esse é o algoritmo utilizado pela função `lm()` do *R* no computo dos estimadores.

Considere o modelo para a $i$-ésima observação

$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_{2} x_{i2} + \beta_{3} x_{i3} + \ldots + \beta_k x_{ik} + u$$

como temos $n$ observações (tamanho de nossa amostra), podemos organizar esses valores em vetores e matrizes.


Assim, representamos o modelo de RLM da seguinte maneira:

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}$$

Queremos encontrar um estimador de $\boldsymbol{\beta}$, que chamaremos de $\boldsymbol{\hat{\beta}}$. Esse estimador sera dado por:

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

Onde $\mathbf{X}^T$ é a matriz transposta de $\mathbf{X}$, e $(\mathbf{X}^T \mathbf{X})^{-1}$ é a matriz inversa da seguinte multiplicação de matrizes, $(\mathbf{X}^T \mathbf{X})$

Esse vetor de estimativas irá nos fornecer os valores para os $\beta_j$,com $j=1,...,k$. 


###  Estimativas de MQO

Vamos analisar o efeito da taxa de homicídio e da renda per capita sobre a taxa de feminicídio. Ou seja:

$$Feminic_i = \beta_0 + \beta_1 Homic_{i} + \beta_{2} Rendapc_i  + u$$

```{r}
### Estimando uma regressão Linear Multipla

rlm <- lm(data = dados, feminic_tx ~ homic_tx + rendapc)

summary(rlm)

```


A leitura dos resultados é análoga ao que já fizemos anteriormente. O aumento de uma unidade na taxa de homicídio aumenta em 0,11 a taxa de feminicídio e é significante a 10\%. Entretanto, não há evidências de que a renda pc tem efeito sobre a taxa de feminicídio. Assim, estados mais violentos tendem a ter mais feminicídios. Entretanto, estados mais pobres e mais ricos possuem taxas similares de feminicídio. 



### Qualidade do Ajuste na Regressão Linear Multipla


Um fato importante sobre $R^2$ é que ele nunca diminui ao incluirmos variáveis no modelo. Isso decorre de propriedades algébricas desse indicador. Em nosso exemplo:

 * **Modelo somente com Homicídio**: $R^2=0,12$ 
 * **Modelo com Homicídio e Renda**: $R^2=0,14$
 
 Veja que o $R^2$ aumentou ao incluir a renda pc.  
 
Por isso, os softwares geralmente reportam uma outra estatística de ajuste o R-quadrado ajustado,$\bar{R}^2$:



$$\bar{R}^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - k - 1}$$

O $\bar{R}^2$, tal como expresso acima conta com a presença do $R^2$ original, mas note que, no denominador estamos dividindo por $n-k-1$, onde $n$ é o tamanho da amostra em mãos, e $k-1$ se referem ao fato de termos $k$ variáveis explicativas e uma constante. Portanto, o $\bar{R}^2$, impões uma penalidade à inclusão de variáveis independentes em um modelo de regressão.No nosso modelo:


 * **Modelo somente com Homicídio**: $R^2=0,12$ e $R^2_{ajust}= 0,094$
 * **Modelo com Homicídio e Renda**: $R^2=0,14$ e $R^2_{ajust}= 0,069$
 

Podemos notar que a inclusão da renda pc contribui muito pouco para a explicação do modelo mas mas prejudicou as estimativas (menor grau de liberdade. dessa forma observa-se que o $R^2_{ajust}$ ficou menor. 

Não obstante, cabe destacar que um $R^2$ baixo não é uma evidencia definitiva contra nosso modelo. Em ciências sociais é comum verificarmos $R^2$ relatvamente pequenos. Isso significa que, embora coletivamente as variáveis explicativas não expliquem muito das variações de $y$, é possível que as estimativas de MQO sejam efeitos parciais confiáveis- tudo o mais constante - de cada $x_j$ sobre $y$.




### Uso de Variáveis Qualitativas na Análise de RLM 

O uso de variáveis binárias é a alternativa para utilizarmos variáveis qualitativas (discretas ou categóricas) no modelo de regressão. Dessa forma, variáveis que possuem categorias (homem/mulher; regiões, típo de homicídio). Considere os exemplos mencionados anteriormente.

* **Determinação de Fatores que Afetam o Valor de Indenizações:** 
  + *Variáveis Independentes*: Idade da Vítima, Gravidade do Dano, Jurisdição. 
  + *Variável Dependente*: Valor da Indenização.
  + *Objetivo*: Analisar como a idade da vítima, a gravidade do dano e a jurisdição influenciam o valor das indenizações concedidas

$$Indenização = \beta_0 + \beta_1 idade+ \beta_2 Gravidade + \beta_2 Jurisdisção + u$$

* **Análise de Variáveis que Influenciam Taxas de Condenação em Casos Criminais:**
  + *Variáveis Independentes*: Idade do Réu, Tipo de Crime, Local do Julgamento. 
  + *Variável Dependente*: Taxa de Condenação.
  + *Objetivo*: Investigar como a idade do réu, o tipo de crime e o local do julgamento afetam a probabilidade de condenação.

$$Tx.Condenação = \beta_0 + \beta_1  idade  +  \beta_2 Local  + \beta_3  crime + u$$

  * **Análise de Fatores que Influenciam a Taxa de Feminicídio:** 
    + *Variáveis Independentes* $(X)$: Taxa de Desemprego, Índice de Educação, Presença de Políticas de Proteção à Mulher. 
    + *Variável Dependente* $(Y)$: Taxa de Feminicídio por 100.000 mulheres. 
    + *Objetivo*: Investigar como o desemprego, o nível de educação e a existência de políticas de proteção à mulher estão relacionados à taxa de feminicídio em diferentes regiões.

$$Tx.feminicidio = \beta_0 + \beta_1 desemprego + \beta_2 educ + \beta_3 política + u $$

Em todos esses exemplos temos o que denominamos de **variáveis qualitativas**. No primeiro exemplo, gravidade e jurisdição não são quantificaveis. No segundo exemplo, o tipo de crime, e a localidade. Por fim, no último exemplo, a variável que indica ou não a presença de uma política pública de proteção a mulher também é qualitativa. 

Felizmente, isso não traz problemas adcionais a estimação de MQO. Pelo menos quando tais variáveis aparecem como variáveis explicativas. Mais a frente veremos o que acontece quando a variável dependente é do tipo qualitativa.

Duas coisas mudam quando temos as variáveis qualitativas como regressores. Em primeiro lugar temos que organizar essas informações no banco de dados de modo coerente. Em segundo lugar, a introdução de variáveis qualitativas muda a interpretação dos resultados.

Fatores qualitativos geralmente aparecem na forma de informações binárias, também denominadas,*dummy*.


::: callout-note
## Variável Binária ou Dummy:

É uma variável que assume o valor 1 se a "condição occorre" e 0 caso o contrário.
:::
 



[**Exemplos:**]{.underline}

**O crime de feminicídio ocorreu na região central?** 

  * se sim, ela recebe o valor 1, 
  * se não ocorreu a variável recebe o valor 0. 
  
  Nesse caso teremos uma coluna em nosso banco de dados denominada centro, nas quais a linha recebe o valor 1 caso o crime tenha ocorrido no centro e 0 caso o contrário.

**O crime foi considerado grave?** 
  
  * Se sim, recebe o valor 1, 
  * caso contráriorecebe o valor 0.




**Outras binárias....**

  * 1 se mulher e 0 se for homem.
  * 1 se for autodeclarado não-branco e 0 se for branco
  * 1 se trabalha e 0 se não trabalha
  * 1 se feminicídio e 0 se não for feminicídio. 


Utilizar 1 ou 0 é, de fato, um criterio arbitrario, mas essa escolha reside justamente na vantagem de se capturar informações importantes que tornam a interpretação dos resultados mais simples.

As observações que tiveram 1 como valor serão comparadas com aquelas observações que ficaram com o valor 0. Essas observações seriam nosso "grupo de comparação/grupo base". 





####  Uma única Variável Dummy Independente

A forma mais simples de incorporar uma variável qualitativa e simplismente adcionarmos ela na equação de regressão. 

Considere o exemplo prático retirado do livro do Wooldridge. Queremos relacionar anos de estudo e salários. Obviamente, o exemplo é bastante simples, mas será útil para avaliarmos a questão.


$$salarioh = \beta_0 + \delta_0 feminino + \beta_1 educ + \beta_3 exper + \beta_4perm + u $$
Nesse exemplo, estamos tentando entender os determinantes do salário. O salário pode ser explicado pela educação, experência, tempo no cargo e pelo fato de ser mulher. A variável feminino assume o valor 1, se o individuo for do sexo feminino e 0, caso o contrário. Na equação $\delta_0$ mede a diferença no salário entre homems e mulheres, dado os mesmos anos de estudos, experiência e tempo no cargo, e evidentemente o mesmo $u$.

```{r}

data(wage1, package= "wooldridge")

wage_f<-lm(wage ~ female+educ+exper+tenure, data=wage1)
summary(wage_f)

```

O coeficiente estimado para a binária que identifica mulheres foi de -1,81 e significativa a 1\%. Isso indica que em média, uma mulher ganha menos US$ 1,81 por hora do que um homem com a mesma educação, experiência e tempo no cargo. Veja a figura abaixo ela mostra o efeito da binária feminina. Note que para um mesmo níve de escolaridade e o salário do homem é maior do que o salário da mulher. A dummy "desloca" o intercepto 


![Variáveis binárias graficamente, fonte: SEmantic Scholar](figuras\dummy1.png)



Podemos utilizar a binária para ver o efeito da experiência para homens e para mulheres. Podemos fazer isso criando uma variável que é a multiplicação entre experiência e feminino. Vejamos: 

```{r}

wage1$exp_f<-wage1$exper*wage1$female


wage_f_2<-lm(wage ~ educ+female+ exper+exp_f+tenure, data=wage1)
summary(wage_f_2)
mean(wage1$wage)
```

Nesse novo modelo temos agora a binária de mulher, female, que mostra que o fato de ser mulher diminui o salário em -US\$0,88. Veja que a média de salário é de US\$ 5,9. A queda representa 15\% a menos em relação ao homem. 

Com relação a multiplicação pela experiência, podemos assim analisar. Como a variável é 0 para homem, o efeito da experiência para os homens é o próprio coeficiente,  exper. Para cada ano a mais de experiência o salário do homem aumenta em US\$ 0,057. Para as mulheres é a soma do coeficiente, exper + exp_f. Para cada ao a mais da experência da mulher o mercado paga US$0,001 . Isso está representado no gráfico abaixo.


![Variáveis binárias multiplicativas graficamente, fonte: University of Washington](figuras\dummy2.png)


### O uso de Dummies para categorias multiplas  e Interação entre a dummy e a variável dependente

Considere a equação abaixo

$$log(salarioh) = \beta_0 + \beta_1  hcasado  +  \beta_2 mcasadas  + \beta_3 msolteiras + educ + exper + (exper)^2 u$$
Note que, definimos uma dummy para cada grupo e o o grupo de comparação é o conunto de homens solteiros.As estimativas das dummies acima irão captar a diferença proporcional nos salarios-horas relativamente aos homens solteiros.

```{r}
###Aplicação de RLM multiplas dummies

data(wage1, package="wooldridge")
ols.dummy <- lm(log(wage)~ married*female+
     educ+ exper  +I(exper^2)+tenure+I(tenure^2),
   data=wage1)

summary(ols.dummy)



```
Com base nas estimativas, verificamos que os homens casados ganham cerca de 21,3\% mais que os homens solteiros, mantendo fixas educaçao, experiência e permanência. Uma mulher casada, no entanto, ganharia, em média, cerca de 30\% menos que um homem solteiro controlando por todas as demais características do banco de dados.




### Incorporando Informações Ordinais com o uso de Variáveis Dummy.

```{r}
#Efeito da Classificação das faculdades de direito sobre salarios iniciais

data(lawsch85, package="wooldridge")

# Define os pontos de corte para rankear as faculdades
cutpts <- c(0,10,25,40,60,100,175)

# criando uma varaiavel factor para ranquear as escolas
 lawsch85$rankcat <- cut(lawsch85$rank, cutpts)
 
# Escolhendo a categoria de referencia
lawsch85$rankcat <- relevel(lawsch85$rankcat,"(100,175]")


#rodando a regressão
res <- lm(log(salary)~rankcat+LSAT+GPA+log(libvol)+log(cost), data=lawsch85)
summary(res)

```
Vemos imediatamente que todas as variáveis fictícias que definem as diferentes classificações são estatisticamente significativas. A estimativa de classificação (60.100] significa que, mantendo LSAT, GPA, libvol (volume na biblioteca) e custo fixos, o salário médio em uma faculdade de direito classificada entre 61 e 100 é cerca de 13,2% maior do que em uma faculdade de direito classificada abaixo de 100. A diferença entre uma escola top 10 e uma escola abaixo de 100 é bastante grande.


# Teste de Hipóteses no RLM
## Hipótese de Normalidade
Para realizarmos procedimentos de inferência sobre os parâmetros estimados do modelo adotamos a hipótese de normalidade.

6) **Hipótese de Normalidade do erro:** O termo de erro $u$ é independente das variáveis explicativas com média zero e variância dada por $\sigma^2$.

Com as hipoteses anteriores mais a hipótese de normalidade, pode-se demostrar que os parâmetros estimados $\hat{\beta_j}$ são normalmente distibuídos com média dada por $\beta_j$ e variância dada por $Var(\hat{\beta_j})$.

Note que, os parâmetros são variáveis aleatórias normalmente distribuídas. Com isso, podemos usar a distribuição normal padrão para calcular as estatísticas de teste, bem como a distribuição $t$.

Isto é

$$\frac{(\hat{\beta_j} - \beta_j)}{ep(\hat{\beta_j})} \sim N(0,1) $$

ou


$$\frac{(\hat{\beta_j} - \beta_j)}{ep(\hat{\beta_j})} \sim t_{n-k-1} $$

## Teste de Hipótese sob um Único Parametro
 
Na maior parte do tempo testamos hipóteses do tipo 

$$H_0: \hat{\beta_j}=0$$
Nesse caso a estatística usada no teste será do tipo

$$t_{\hat{\beta_j}} \equiv \frac{\hat{\beta_j}}{ep(\hat{\beta_j})}$$


###  Teste contra Hipoteses Alternativas Unilaterais

Podemos testar a hipótese de que um dado parâmetro $j$ seja zero, contra uma alternativa unilateral de que $\beta_j > 0$

Formalmente, a hípotese alternativa é 

$$H_1:  \beta_j > 0$$
Dado um nível de segnificancia adequado rejeitamos a hipotese nula se 

$$t_{\hat{\beta_j}}  > t_c$$

Onde $t_c$ é o valor crítico do teste.

![Teste Unicaldal](thunical.png)
Oviamente, podemos ter o caso onde

$$H_1 = \beta < 0$$
e a regra de rejeição será 

$$t_{\hat{\beta_j}} < - t_c$$
![Teste Unicaldal](thunical2.png)


###  Teste contra Hipoteses Alternativas Bilaterais
Pode ser o caso onde testa-se $H_0$ contra

$$H_1: \beta_j \neq 0$$
Nesse caso a região de regeição é dada por:

$$|t_{\hat{\beta_j}}|  > t_c$$
![Teste Unicaldal](thbicaldal.png)
###  Cálculos dos p-valores dos Testes t

Dado o valor observado de $t_c$ qual é o menor nível de significancia ao qual a hipótese nula é rejeitada? esse nível é denominado de **p-valor** do teste de hipótese.

###  Teste de Restrições de Lineares Múltiplas: O teste F

Após o uso da função `summary` nota-se que o r também reporta uma estatística denominada de F. Essa estatistica tal qual reportada pelo software testa a seguinte hipótese.

$$H_0:\beta_1=...=\beta_k=0$$
contra 

$$H_1: H_0\text{ é falsa}$$
Essa estatistica de teste é dada por:

$$F = \frac{R^2/k}{(1-R^2)/n-k-1}$$
Pode ser mostrado que $F \sim F_{q,n-k-1}$.

